{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n",
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 481\n",
      "Train/Dev split: 931/1000\n",
      "I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n",
      "name: GRID K520\n",
      "major: 3 minor: 0 memoryClockRate (GHz) 0.797\n",
      "pciBusID 0000:00:03.0\n",
      "Total memory: 4.00GiB\n",
      "Free memory: 3.95GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\n",
      "Writing to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912\n",
      "\n",
      "2016-08-12T18:59:09.490372: step 1, loss 1.71668, acc 0.59375\n",
      "2016-08-12T18:59:09.559540: step 2, loss 2.45227, acc 0.515625\n",
      "2016-08-12T18:59:09.629437: step 3, loss 2.28296, acc 0.5\n",
      "2016-08-12T18:59:09.700097: step 4, loss 1.91829, acc 0.546875\n",
      "2016-08-12T18:59:09.770401: step 5, loss 1.68534, acc 0.5625\n",
      "2016-08-12T18:59:09.842599: step 6, loss 3.47074, acc 0.46875\n",
      "2016-08-12T18:59:09.910893: step 7, loss 2.66461, acc 0.5625\n",
      "2016-08-12T18:59:09.979886: step 8, loss 2.0715, acc 0.5\n",
      "2016-08-12T18:59:10.050695: step 9, loss 2.19848, acc 0.46875\n",
      "2016-08-12T18:59:10.120893: step 10, loss 1.88943, acc 0.515625\n",
      "2016-08-12T18:59:10.192596: step 11, loss 2.05949, acc 0.515625\n",
      "2016-08-12T18:59:10.262591: step 12, loss 2.15925, acc 0.5625\n",
      "2016-08-12T18:59:10.333631: step 13, loss 1.87399, acc 0.515625\n",
      "2016-08-12T18:59:10.404637: step 14, loss 2.05634, acc 0.484375\n",
      "2016-08-12T18:59:30.867123: step 15, loss 1.51167, acc 0.6\n",
      "2016-08-12T18:59:30.939939: step 16, loss 1.36479, acc 0.59375\n",
      "2016-08-12T18:59:31.011456: step 17, loss 1.57787, acc 0.625\n",
      "2016-08-12T18:59:31.083600: step 18, loss 1.83428, acc 0.59375\n",
      "2016-08-12T18:59:31.153685: step 19, loss 1.71755, acc 0.65625\n",
      "2016-08-12T18:59:31.224506: step 20, loss 1.14295, acc 0.625\n",
      "2016-08-12T18:59:31.296048: step 21, loss 1.40105, acc 0.703125\n",
      "2016-08-12T18:59:31.365732: step 22, loss 1.50868, acc 0.625\n",
      "2016-08-12T18:59:31.438196: step 23, loss 1.66902, acc 0.578125\n",
      "2016-08-12T18:59:31.509014: step 24, loss 1.36991, acc 0.609375\n",
      "2016-08-12T18:59:31.577526: step 25, loss 2.1041, acc 0.484375\n",
      "2016-08-12T18:59:31.648933: step 26, loss 2.0856, acc 0.546875\n",
      "2016-08-12T18:59:31.719461: step 27, loss 1.73071, acc 0.5625\n",
      "2016-08-12T18:59:31.790541: step 28, loss 1.49115, acc 0.578125\n",
      "2016-08-12T18:59:31.860993: step 29, loss 0.996933, acc 0.625\n",
      "2016-08-12T18:59:31.910845: step 30, loss 1.78565, acc 0.485714\n",
      "2016-08-12T18:59:31.982242: step 31, loss 1.33152, acc 0.578125\n",
      "2016-08-12T18:59:32.056122: step 32, loss 1.69046, acc 0.546875\n",
      "2016-08-12T18:59:32.127562: step 33, loss 1.55765, acc 0.609375\n",
      "2016-08-12T18:59:32.197415: step 34, loss 1.45248, acc 0.65625\n",
      "2016-08-12T18:59:32.267213: step 35, loss 1.31775, acc 0.625\n",
      "2016-08-12T18:59:32.339179: step 36, loss 1.3526, acc 0.546875\n",
      "2016-08-12T18:59:32.412217: step 37, loss 1.38927, acc 0.578125\n",
      "2016-08-12T18:59:32.484069: step 38, loss 0.99628, acc 0.578125\n",
      "2016-08-12T18:59:32.554257: step 39, loss 1.05621, acc 0.703125\n",
      "2016-08-12T18:59:32.625064: step 40, loss 1.36105, acc 0.578125\n",
      "2016-08-12T18:59:32.696977: step 41, loss 1.17765, acc 0.671875\n",
      "2016-08-12T18:59:32.769557: step 42, loss 1.28897, acc 0.515625\n",
      "2016-08-12T18:59:32.841341: step 43, loss 0.957034, acc 0.609375\n",
      "2016-08-12T18:59:32.913475: step 44, loss 1.20766, acc 0.625\n",
      "2016-08-12T18:59:32.962955: step 45, loss 1.08271, acc 0.657143\n",
      "2016-08-12T18:59:33.033399: step 46, loss 0.566604, acc 0.796875\n",
      "2016-08-12T18:59:33.103800: step 47, loss 1.10461, acc 0.640625\n",
      "2016-08-12T18:59:33.173648: step 48, loss 1.09698, acc 0.671875\n",
      "2016-08-12T18:59:33.243911: step 49, loss 0.588222, acc 0.75\n",
      "2016-08-12T18:59:33.316383: step 50, loss 1.0087, acc 0.609375\n",
      "2016-08-12T18:59:33.388604: step 51, loss 0.995495, acc 0.703125\n",
      "2016-08-12T18:59:33.458432: step 52, loss 1.10183, acc 0.625\n",
      "2016-08-12T18:59:33.531495: step 53, loss 0.883193, acc 0.703125\n",
      "2016-08-12T18:59:33.603737: step 54, loss 0.943516, acc 0.75\n",
      "2016-08-12T18:59:33.675313: step 55, loss 1.2205, acc 0.5\n",
      "2016-08-12T18:59:33.746612: step 56, loss 0.978444, acc 0.640625\n",
      "2016-08-12T18:59:33.818133: step 57, loss 1.54209, acc 0.5\n",
      "2016-08-12T18:59:33.889672: step 58, loss 1.26287, acc 0.5\n",
      "2016-08-12T18:59:33.961688: step 59, loss 1.03683, acc 0.609375\n",
      "2016-08-12T18:59:34.010634: step 60, loss 0.901261, acc 0.6\n",
      "2016-08-12T18:59:34.083889: step 61, loss 1.14687, acc 0.59375\n",
      "2016-08-12T18:59:34.155098: step 62, loss 0.878294, acc 0.6875\n",
      "2016-08-12T18:59:34.224112: step 63, loss 0.861303, acc 0.65625\n",
      "2016-08-12T18:59:34.293909: step 64, loss 0.664302, acc 0.765625\n",
      "2016-08-12T18:59:34.364314: step 65, loss 0.817665, acc 0.765625\n",
      "2016-08-12T18:59:34.436384: step 66, loss 0.717122, acc 0.6875\n",
      "2016-08-12T18:59:34.509488: step 67, loss 0.706816, acc 0.71875\n",
      "2016-08-12T18:59:34.581432: step 68, loss 0.862126, acc 0.703125\n",
      "2016-08-12T18:59:34.652271: step 69, loss 0.875735, acc 0.671875\n",
      "2016-08-12T18:59:34.724196: step 70, loss 0.421087, acc 0.859375\n",
      "2016-08-12T18:59:34.794138: step 71, loss 0.950528, acc 0.640625\n",
      "2016-08-12T18:59:34.866154: step 72, loss 0.731156, acc 0.71875\n",
      "2016-08-12T18:59:34.936463: step 73, loss 1.26911, acc 0.5625\n",
      "2016-08-12T18:59:35.008452: step 74, loss 0.719051, acc 0.6875\n",
      "2016-08-12T18:59:35.057811: step 75, loss 1.38984, acc 0.6\n",
      "2016-08-12T18:59:35.130337: step 76, loss 0.942581, acc 0.703125\n",
      "2016-08-12T18:59:35.200885: step 77, loss 0.698032, acc 0.65625\n",
      "2016-08-12T18:59:35.273084: step 78, loss 1.34599, acc 0.546875\n",
      "2016-08-12T18:59:35.344519: step 79, loss 0.947117, acc 0.640625\n",
      "2016-08-12T18:59:35.414577: step 80, loss 0.487517, acc 0.828125\n",
      "2016-08-12T18:59:35.486835: step 81, loss 0.731018, acc 0.6875\n",
      "2016-08-12T18:59:35.557383: step 82, loss 0.567868, acc 0.734375\n",
      "2016-08-12T18:59:35.627994: step 83, loss 0.541251, acc 0.78125\n",
      "2016-08-12T18:59:35.696566: step 84, loss 0.852018, acc 0.71875\n",
      "2016-08-12T18:59:35.769256: step 85, loss 0.711413, acc 0.71875\n",
      "2016-08-12T18:59:35.841483: step 86, loss 0.945943, acc 0.671875\n",
      "2016-08-12T18:59:35.913033: step 87, loss 0.693256, acc 0.734375\n",
      "2016-08-12T18:59:35.987078: step 88, loss 0.585791, acc 0.75\n",
      "2016-08-12T18:59:36.057881: step 89, loss 1.21762, acc 0.578125\n",
      "2016-08-12T18:59:36.108671: step 90, loss 0.650631, acc 0.742857\n",
      "2016-08-12T18:59:36.180956: step 91, loss 0.853956, acc 0.6875\n",
      "2016-08-12T18:59:36.253207: step 92, loss 0.624041, acc 0.78125\n",
      "2016-08-12T18:59:36.325822: step 93, loss 0.696311, acc 0.703125\n",
      "2016-08-12T18:59:36.395887: step 94, loss 0.766504, acc 0.703125\n",
      "2016-08-12T18:59:36.466653: step 95, loss 0.582261, acc 0.765625\n",
      "2016-08-12T18:59:36.538004: step 96, loss 0.591635, acc 0.703125\n",
      "2016-08-12T18:59:36.612013: step 97, loss 0.665663, acc 0.703125\n",
      "2016-08-12T18:59:36.681893: step 98, loss 0.599432, acc 0.78125\n",
      "2016-08-12T18:59:36.753429: step 99, loss 0.602456, acc 0.75\n",
      "2016-08-12T18:59:36.823864: step 100, loss 0.864268, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T18:59:37.909954: step 100, loss 0.615799, acc 0.715\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-100\n",
      "\n",
      "2016-08-12T18:59:38.594703: step 101, loss 0.600961, acc 0.765625\n",
      "2016-08-12T18:59:38.664696: step 102, loss 0.861077, acc 0.65625\n",
      "2016-08-12T18:59:38.733990: step 103, loss 0.575115, acc 0.75\n",
      "2016-08-12T18:59:38.806861: step 104, loss 0.586652, acc 0.734375\n",
      "2016-08-12T18:59:38.854496: step 105, loss 0.853417, acc 0.742857\n",
      "2016-08-12T18:59:38.925405: step 106, loss 0.531828, acc 0.796875\n",
      "2016-08-12T18:59:38.995827: step 107, loss 0.781329, acc 0.640625\n",
      "2016-08-12T18:59:39.064876: step 108, loss 0.587506, acc 0.765625\n",
      "2016-08-12T18:59:39.133903: step 109, loss 0.623573, acc 0.78125\n",
      "2016-08-12T18:59:39.206019: step 110, loss 0.382585, acc 0.84375\n",
      "2016-08-12T18:59:39.276470: step 111, loss 0.648666, acc 0.6875\n",
      "2016-08-12T18:59:39.348235: step 112, loss 0.580597, acc 0.765625\n",
      "2016-08-12T18:59:39.417238: step 113, loss 0.721711, acc 0.78125\n",
      "2016-08-12T18:59:39.489237: step 114, loss 0.654243, acc 0.671875\n",
      "2016-08-12T18:59:39.559553: step 115, loss 0.867416, acc 0.671875\n",
      "2016-08-12T18:59:39.631367: step 116, loss 0.498311, acc 0.765625\n",
      "2016-08-12T18:59:39.704094: step 117, loss 0.56891, acc 0.8125\n",
      "2016-08-12T18:59:39.773449: step 118, loss 0.632299, acc 0.765625\n",
      "2016-08-12T18:59:39.844593: step 119, loss 0.466317, acc 0.828125\n",
      "2016-08-12T18:59:39.899317: step 120, loss 0.552295, acc 0.771429\n",
      "2016-08-12T18:59:39.971432: step 121, loss 0.493168, acc 0.765625\n",
      "2016-08-12T18:59:40.042966: step 122, loss 0.501547, acc 0.765625\n",
      "2016-08-12T18:59:40.113957: step 123, loss 0.476296, acc 0.75\n",
      "2016-08-12T18:59:40.186089: step 124, loss 0.381574, acc 0.828125\n",
      "2016-08-12T18:59:40.259010: step 125, loss 1.04592, acc 0.6875\n",
      "2016-08-12T18:59:40.329117: step 126, loss 0.377594, acc 0.84375\n",
      "2016-08-12T18:59:40.401694: step 127, loss 0.770279, acc 0.6875\n",
      "2016-08-12T18:59:40.472535: step 128, loss 0.60698, acc 0.734375\n",
      "2016-08-12T18:59:40.544136: step 129, loss 0.707469, acc 0.796875\n",
      "2016-08-12T18:59:40.617914: step 130, loss 0.67091, acc 0.734375\n",
      "2016-08-12T18:59:40.690432: step 131, loss 0.508909, acc 0.75\n",
      "2016-08-12T18:59:40.763142: step 132, loss 0.631273, acc 0.71875\n",
      "2016-08-12T18:59:40.834647: step 133, loss 0.45074, acc 0.828125\n",
      "2016-08-12T18:59:40.905483: step 134, loss 0.625475, acc 0.765625\n",
      "2016-08-12T18:59:40.954413: step 135, loss 0.488487, acc 0.8\n",
      "2016-08-12T18:59:41.028551: step 136, loss 0.542902, acc 0.765625\n",
      "2016-08-12T18:59:41.098878: step 137, loss 0.580966, acc 0.765625\n",
      "2016-08-12T18:59:41.171366: step 138, loss 0.407473, acc 0.84375\n",
      "2016-08-12T18:59:41.241742: step 139, loss 0.635561, acc 0.765625\n",
      "2016-08-12T18:59:41.313689: step 140, loss 0.306352, acc 0.875\n",
      "2016-08-12T18:59:41.386616: step 141, loss 0.473044, acc 0.796875\n",
      "2016-08-12T18:59:41.460153: step 142, loss 0.436334, acc 0.765625\n",
      "2016-08-12T18:59:41.529931: step 143, loss 0.390229, acc 0.8125\n",
      "2016-08-12T18:59:41.603232: step 144, loss 0.603491, acc 0.734375\n",
      "2016-08-12T18:59:41.673591: step 145, loss 0.419212, acc 0.796875\n",
      "2016-08-12T18:59:41.745415: step 146, loss 0.4526, acc 0.78125\n",
      "2016-08-12T18:59:41.818679: step 147, loss 0.526872, acc 0.828125\n",
      "2016-08-12T18:59:41.890694: step 148, loss 0.633415, acc 0.734375\n",
      "2016-08-12T18:59:41.962076: step 149, loss 0.457059, acc 0.8125\n",
      "2016-08-12T18:59:42.011704: step 150, loss 0.512632, acc 0.771429\n",
      "2016-08-12T18:59:42.083142: step 151, loss 0.450667, acc 0.796875\n",
      "2016-08-12T18:59:42.156212: step 152, loss 0.452536, acc 0.859375\n",
      "2016-08-12T18:59:42.227866: step 153, loss 0.403849, acc 0.84375\n",
      "2016-08-12T18:59:42.302364: step 154, loss 0.470081, acc 0.8125\n",
      "2016-08-12T18:59:42.374830: step 155, loss 0.497486, acc 0.734375\n",
      "2016-08-12T18:59:42.447627: step 156, loss 0.578744, acc 0.734375\n",
      "2016-08-12T18:59:42.519006: step 157, loss 0.624742, acc 0.734375\n",
      "2016-08-12T18:59:42.590850: step 158, loss 0.402266, acc 0.796875\n",
      "2016-08-12T18:59:42.661014: step 159, loss 0.341988, acc 0.84375\n",
      "2016-08-12T18:59:42.734255: step 160, loss 0.365369, acc 0.796875\n",
      "2016-08-12T18:59:42.807144: step 161, loss 0.282334, acc 0.875\n",
      "2016-08-12T18:59:42.879605: step 162, loss 0.404265, acc 0.859375\n",
      "2016-08-12T18:59:42.950852: step 163, loss 0.419401, acc 0.84375\n",
      "2016-08-12T18:59:43.020919: step 164, loss 0.563315, acc 0.71875\n",
      "2016-08-12T18:59:43.069002: step 165, loss 0.357277, acc 0.857143\n",
      "2016-08-12T18:59:43.141429: step 166, loss 0.233859, acc 0.9375\n",
      "2016-08-12T18:59:43.211924: step 167, loss 0.402351, acc 0.78125\n",
      "2016-08-12T18:59:43.284014: step 168, loss 0.436497, acc 0.796875\n",
      "2016-08-12T18:59:43.354870: step 169, loss 0.406484, acc 0.84375\n",
      "2016-08-12T18:59:43.426046: step 170, loss 0.392146, acc 0.78125\n",
      "2016-08-12T18:59:43.497319: step 171, loss 0.460415, acc 0.8125\n",
      "2016-08-12T18:59:43.569264: step 172, loss 0.590644, acc 0.765625\n",
      "2016-08-12T18:59:43.641606: step 173, loss 0.150222, acc 0.953125\n",
      "2016-08-12T18:59:43.715672: step 174, loss 0.665786, acc 0.6875\n",
      "2016-08-12T18:59:43.786574: step 175, loss 0.518531, acc 0.765625\n",
      "2016-08-12T18:59:43.858548: step 176, loss 0.297269, acc 0.890625\n",
      "2016-08-12T18:59:43.929928: step 177, loss 0.500577, acc 0.75\n",
      "2016-08-12T18:59:44.000097: step 178, loss 0.432393, acc 0.765625\n",
      "2016-08-12T18:59:44.071423: step 179, loss 0.400786, acc 0.84375\n",
      "2016-08-12T18:59:44.120831: step 180, loss 0.213197, acc 0.942857\n",
      "2016-08-12T18:59:44.192410: step 181, loss 0.399929, acc 0.828125\n",
      "2016-08-12T18:59:44.264051: step 182, loss 0.337794, acc 0.84375\n",
      "2016-08-12T18:59:44.336036: step 183, loss 0.462825, acc 0.828125\n",
      "2016-08-12T18:59:44.408748: step 184, loss 0.427141, acc 0.84375\n",
      "2016-08-12T18:59:44.480165: step 185, loss 0.307926, acc 0.84375\n",
      "2016-08-12T18:59:44.550970: step 186, loss 0.3814, acc 0.828125\n",
      "2016-08-12T18:59:44.622771: step 187, loss 0.270957, acc 0.890625\n",
      "2016-08-12T18:59:44.694590: step 188, loss 0.352062, acc 0.84375\n",
      "2016-08-12T18:59:44.766932: step 189, loss 0.435211, acc 0.765625\n",
      "2016-08-12T18:59:44.838001: step 190, loss 0.320529, acc 0.796875\n",
      "2016-08-12T18:59:44.911968: step 191, loss 0.499702, acc 0.734375\n",
      "2016-08-12T18:59:44.981829: step 192, loss 0.605296, acc 0.765625\n",
      "2016-08-12T18:59:45.054403: step 193, loss 0.50241, acc 0.84375\n",
      "2016-08-12T18:59:45.125994: step 194, loss 0.349238, acc 0.828125\n",
      "2016-08-12T18:59:45.175171: step 195, loss 0.440911, acc 0.8\n",
      "2016-08-12T18:59:45.248262: step 196, loss 0.415751, acc 0.8125\n",
      "2016-08-12T18:59:45.319049: step 197, loss 0.354525, acc 0.875\n",
      "2016-08-12T18:59:45.391191: step 198, loss 0.245803, acc 0.90625\n",
      "2016-08-12T18:59:45.465224: step 199, loss 0.203911, acc 0.953125\n",
      "2016-08-12T18:59:45.535742: step 200, loss 0.23851, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T18:59:45.847008: step 200, loss 0.519996, acc 0.746\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-200\n",
      "\n",
      "2016-08-12T18:59:46.522382: step 201, loss 0.384811, acc 0.796875\n",
      "2016-08-12T18:59:46.593768: step 202, loss 0.350481, acc 0.84375\n",
      "2016-08-12T18:59:46.666389: step 203, loss 0.256649, acc 0.875\n",
      "2016-08-12T18:59:46.739402: step 204, loss 0.404587, acc 0.859375\n",
      "2016-08-12T18:59:46.810651: step 205, loss 0.275954, acc 0.84375\n",
      "2016-08-12T18:59:46.881577: step 206, loss 0.345578, acc 0.84375\n",
      "2016-08-12T18:59:46.953927: step 207, loss 0.311669, acc 0.90625\n",
      "2016-08-12T18:59:47.025570: step 208, loss 0.413906, acc 0.796875\n",
      "2016-08-12T18:59:47.097514: step 209, loss 0.276719, acc 0.859375\n",
      "2016-08-12T18:59:47.148899: step 210, loss 0.407444, acc 0.857143\n",
      "2016-08-12T18:59:47.222390: step 211, loss 0.329683, acc 0.8125\n",
      "2016-08-12T18:59:47.292929: step 212, loss 0.376173, acc 0.796875\n",
      "2016-08-12T18:59:47.365552: step 213, loss 0.245404, acc 0.921875\n",
      "2016-08-12T18:59:47.437949: step 214, loss 0.213942, acc 0.921875\n",
      "2016-08-12T18:59:47.509202: step 215, loss 0.46646, acc 0.796875\n",
      "2016-08-12T18:59:47.577381: step 216, loss 0.307329, acc 0.875\n",
      "2016-08-12T18:59:47.649289: step 217, loss 0.227329, acc 0.90625\n",
      "2016-08-12T18:59:47.726875: step 218, loss 0.366455, acc 0.84375\n",
      "2016-08-12T18:59:47.802980: step 219, loss 0.285237, acc 0.859375\n",
      "2016-08-12T18:59:47.879337: step 220, loss 0.429524, acc 0.84375\n",
      "2016-08-12T18:59:47.950170: step 221, loss 0.26834, acc 0.890625\n",
      "2016-08-12T18:59:48.022500: step 222, loss 0.369116, acc 0.859375\n",
      "2016-08-12T18:59:48.095100: step 223, loss 0.169517, acc 0.921875\n",
      "2016-08-12T18:59:48.166377: step 224, loss 0.339385, acc 0.859375\n",
      "2016-08-12T18:59:48.214998: step 225, loss 0.280557, acc 0.828571\n",
      "2016-08-12T18:59:48.286085: step 226, loss 0.263371, acc 0.84375\n",
      "2016-08-12T18:59:48.356936: step 227, loss 0.29995, acc 0.9375\n",
      "2016-08-12T18:59:48.430060: step 228, loss 0.191905, acc 0.9375\n",
      "2016-08-12T18:59:48.499922: step 229, loss 0.308562, acc 0.859375\n",
      "2016-08-12T18:59:48.572547: step 230, loss 0.180644, acc 0.9375\n",
      "2016-08-12T18:59:48.645195: step 231, loss 0.342304, acc 0.828125\n",
      "2016-08-12T18:59:48.718292: step 232, loss 0.284339, acc 0.875\n",
      "2016-08-12T18:59:48.790887: step 233, loss 0.410852, acc 0.78125\n",
      "2016-08-12T18:59:48.861032: step 234, loss 0.217011, acc 0.890625\n",
      "2016-08-12T18:59:48.931470: step 235, loss 0.433353, acc 0.8125\n",
      "2016-08-12T18:59:49.002741: step 236, loss 0.19644, acc 0.921875\n",
      "2016-08-12T18:59:49.074288: step 237, loss 0.327086, acc 0.875\n",
      "2016-08-12T18:59:49.146505: step 238, loss 0.371013, acc 0.875\n",
      "2016-08-12T18:59:49.217336: step 239, loss 0.334351, acc 0.8125\n",
      "2016-08-12T18:59:49.268896: step 240, loss 0.256126, acc 0.828571\n",
      "2016-08-12T18:59:49.341345: step 241, loss 0.280202, acc 0.859375\n",
      "2016-08-12T18:59:49.415568: step 242, loss 0.284407, acc 0.921875\n",
      "2016-08-12T18:59:49.486768: step 243, loss 0.218704, acc 0.921875\n",
      "2016-08-12T18:59:49.557408: step 244, loss 0.222793, acc 0.90625\n",
      "2016-08-12T18:59:49.628544: step 245, loss 0.248451, acc 0.875\n",
      "2016-08-12T18:59:49.699165: step 246, loss 0.206632, acc 0.9375\n",
      "2016-08-12T18:59:49.771085: step 247, loss 0.399644, acc 0.796875\n",
      "2016-08-12T18:59:49.841742: step 248, loss 0.375604, acc 0.859375\n",
      "2016-08-12T18:59:49.913282: step 249, loss 0.200401, acc 0.890625\n",
      "2016-08-12T18:59:49.984287: step 250, loss 0.233562, acc 0.890625\n",
      "2016-08-12T18:59:50.054882: step 251, loss 0.210287, acc 0.90625\n",
      "2016-08-12T18:59:50.127711: step 252, loss 0.20894, acc 0.921875\n",
      "2016-08-12T18:59:50.200064: step 253, loss 0.322164, acc 0.84375\n",
      "2016-08-12T18:59:50.272380: step 254, loss 0.240726, acc 0.890625\n",
      "2016-08-12T18:59:50.321867: step 255, loss 0.388467, acc 0.828571\n",
      "2016-08-12T18:59:50.393107: step 256, loss 0.134257, acc 0.96875\n",
      "2016-08-12T18:59:50.463988: step 257, loss 0.170682, acc 0.9375\n",
      "2016-08-12T18:59:50.535638: step 258, loss 0.24535, acc 0.875\n",
      "2016-08-12T18:59:50.607662: step 259, loss 0.247409, acc 0.890625\n",
      "2016-08-12T18:59:50.676984: step 260, loss 0.232732, acc 0.9375\n",
      "2016-08-12T18:59:50.749063: step 261, loss 0.152858, acc 0.953125\n",
      "2016-08-12T18:59:50.820017: step 262, loss 0.202271, acc 0.90625\n",
      "2016-08-12T18:59:50.891308: step 263, loss 0.247016, acc 0.859375\n",
      "2016-08-12T18:59:50.963886: step 264, loss 0.178024, acc 0.953125\n",
      "2016-08-12T18:59:51.035208: step 265, loss 0.220866, acc 0.9375\n",
      "2016-08-12T18:59:51.106777: step 266, loss 0.151215, acc 0.953125\n",
      "2016-08-12T18:59:51.177424: step 267, loss 0.208954, acc 0.90625\n",
      "2016-08-12T18:59:51.250890: step 268, loss 0.311949, acc 0.859375\n",
      "2016-08-12T18:59:51.321750: step 269, loss 0.34249, acc 0.828125\n",
      "2016-08-12T18:59:51.369985: step 270, loss 0.27582, acc 0.914286\n",
      "2016-08-12T18:59:51.441519: step 271, loss 0.211345, acc 0.890625\n",
      "2016-08-12T18:59:51.512750: step 272, loss 0.2406, acc 0.9375\n",
      "2016-08-12T18:59:51.583575: step 273, loss 0.200699, acc 0.921875\n",
      "2016-08-12T18:59:51.655471: step 274, loss 0.143732, acc 0.9375\n",
      "2016-08-12T18:59:51.725828: step 275, loss 0.181043, acc 0.953125\n",
      "2016-08-12T18:59:51.796570: step 276, loss 0.258485, acc 0.90625\n",
      "2016-08-12T18:59:51.867036: step 277, loss 0.213902, acc 0.921875\n",
      "2016-08-12T18:59:51.944048: step 278, loss 0.357035, acc 0.859375\n",
      "2016-08-12T18:59:52.014991: step 279, loss 0.212029, acc 0.921875\n",
      "2016-08-12T18:59:52.087036: step 280, loss 0.213809, acc 0.90625\n",
      "2016-08-12T18:59:52.158841: step 281, loss 0.211216, acc 0.9375\n",
      "2016-08-12T18:59:52.231570: step 282, loss 0.195464, acc 0.90625\n",
      "2016-08-12T18:59:52.302151: step 283, loss 0.241297, acc 0.90625\n",
      "2016-08-12T18:59:52.374075: step 284, loss 0.224764, acc 0.875\n",
      "2016-08-12T18:59:52.425189: step 285, loss 0.191612, acc 0.971429\n",
      "2016-08-12T18:59:52.496609: step 286, loss 0.316421, acc 0.84375\n",
      "2016-08-12T18:59:52.568247: step 287, loss 0.279992, acc 0.84375\n",
      "2016-08-12T18:59:52.639708: step 288, loss 0.140988, acc 0.953125\n",
      "2016-08-12T18:59:52.712011: step 289, loss 0.252587, acc 0.875\n",
      "2016-08-12T18:59:52.783349: step 290, loss 0.231629, acc 0.890625\n",
      "2016-08-12T18:59:52.854633: step 291, loss 0.320712, acc 0.890625\n",
      "2016-08-12T18:59:52.928519: step 292, loss 0.119641, acc 1\n",
      "2016-08-12T18:59:53.000230: step 293, loss 0.306995, acc 0.796875\n",
      "2016-08-12T18:59:53.071666: step 294, loss 0.243338, acc 0.890625\n",
      "2016-08-12T18:59:53.144902: step 295, loss 0.308721, acc 0.875\n",
      "2016-08-12T18:59:53.214550: step 296, loss 0.263493, acc 0.90625\n",
      "2016-08-12T18:59:53.286923: step 297, loss 0.237187, acc 0.921875\n",
      "2016-08-12T18:59:53.356535: step 298, loss 0.224041, acc 0.90625\n",
      "2016-08-12T18:59:53.428424: step 299, loss 0.234449, acc 0.875\n",
      "2016-08-12T18:59:53.480121: step 300, loss 0.182627, acc 0.942857\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T18:59:53.787339: step 300, loss 0.503342, acc 0.757\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-300\n",
      "\n",
      "2016-08-12T18:59:54.463952: step 301, loss 0.242426, acc 0.859375\n",
      "2016-08-12T18:59:54.535739: step 302, loss 0.181582, acc 0.953125\n",
      "2016-08-12T18:59:54.607252: step 303, loss 0.253218, acc 0.90625\n",
      "2016-08-12T18:59:54.677216: step 304, loss 0.210725, acc 0.90625\n",
      "2016-08-12T18:59:54.747730: step 305, loss 0.197546, acc 0.953125\n",
      "2016-08-12T18:59:54.821097: step 306, loss 0.188603, acc 0.921875\n",
      "2016-08-12T18:59:54.894046: step 307, loss 0.182023, acc 0.953125\n",
      "2016-08-12T18:59:54.966530: step 308, loss 0.261424, acc 0.875\n",
      "2016-08-12T18:59:55.038164: step 309, loss 0.187567, acc 0.9375\n",
      "2016-08-12T18:59:55.107890: step 310, loss 0.22381, acc 0.9375\n",
      "2016-08-12T18:59:55.179208: step 311, loss 0.313032, acc 0.828125\n",
      "2016-08-12T18:59:55.250858: step 312, loss 0.368861, acc 0.84375\n",
      "2016-08-12T18:59:55.323697: step 313, loss 0.204336, acc 0.921875\n",
      "2016-08-12T18:59:55.397284: step 314, loss 0.24722, acc 0.875\n",
      "2016-08-12T18:59:55.446038: step 315, loss 0.09406, acc 1\n",
      "2016-08-12T18:59:55.519078: step 316, loss 0.13913, acc 0.96875\n",
      "2016-08-12T18:59:55.590273: step 317, loss 0.187755, acc 0.921875\n",
      "2016-08-12T18:59:55.661806: step 318, loss 0.220891, acc 0.9375\n",
      "2016-08-12T18:59:55.734849: step 319, loss 0.257423, acc 0.875\n",
      "2016-08-12T18:59:55.808259: step 320, loss 0.186311, acc 0.9375\n",
      "2016-08-12T18:59:55.880098: step 321, loss 0.197817, acc 0.90625\n",
      "2016-08-12T18:59:55.950989: step 322, loss 0.148746, acc 0.953125\n",
      "2016-08-12T18:59:56.025492: step 323, loss 0.187078, acc 0.9375\n",
      "2016-08-12T18:59:56.099348: step 324, loss 0.162687, acc 0.96875\n",
      "2016-08-12T18:59:56.171844: step 325, loss 0.164764, acc 0.9375\n",
      "2016-08-12T18:59:56.244084: step 326, loss 0.173191, acc 0.9375\n",
      "2016-08-12T18:59:56.315769: step 327, loss 0.216662, acc 0.890625\n",
      "2016-08-12T18:59:56.388711: step 328, loss 0.222752, acc 0.90625\n",
      "2016-08-12T18:59:56.462605: step 329, loss 0.163208, acc 0.984375\n",
      "2016-08-12T18:59:56.513185: step 330, loss 0.184869, acc 0.942857\n",
      "2016-08-12T18:59:56.584955: step 331, loss 0.228552, acc 0.90625\n",
      "2016-08-12T18:59:56.656108: step 332, loss 0.236285, acc 0.90625\n",
      "2016-08-12T18:59:56.728346: step 333, loss 0.212979, acc 0.90625\n",
      "2016-08-12T18:59:56.800258: step 334, loss 0.120744, acc 0.984375\n",
      "2016-08-12T18:59:56.872476: step 335, loss 0.234142, acc 0.90625\n",
      "2016-08-12T18:59:56.943345: step 336, loss 0.138287, acc 0.953125\n",
      "2016-08-12T18:59:57.016038: step 337, loss 0.204814, acc 0.90625\n",
      "2016-08-12T18:59:57.088722: step 338, loss 0.179936, acc 0.921875\n",
      "2016-08-12T18:59:57.158532: step 339, loss 0.15706, acc 0.96875\n",
      "2016-08-12T18:59:57.230631: step 340, loss 0.210797, acc 0.921875\n",
      "2016-08-12T18:59:57.301774: step 341, loss 0.196885, acc 0.9375\n",
      "2016-08-12T18:59:57.374061: step 342, loss 0.141994, acc 0.96875\n",
      "2016-08-12T18:59:57.447974: step 343, loss 0.124802, acc 0.984375\n",
      "2016-08-12T18:59:57.522136: step 344, loss 0.145031, acc 0.9375\n",
      "2016-08-12T18:59:57.571271: step 345, loss 0.18685, acc 0.942857\n",
      "2016-08-12T18:59:57.642503: step 346, loss 0.097863, acc 1\n",
      "2016-08-12T18:59:57.714080: step 347, loss 0.187458, acc 0.890625\n",
      "2016-08-12T18:59:57.784030: step 348, loss 0.162476, acc 0.953125\n",
      "2016-08-12T18:59:57.855560: step 349, loss 0.194484, acc 0.921875\n",
      "2016-08-12T18:59:57.926036: step 350, loss 0.166749, acc 0.921875\n",
      "2016-08-12T18:59:57.997228: step 351, loss 0.181103, acc 0.921875\n",
      "2016-08-12T18:59:58.069325: step 352, loss 0.177013, acc 0.9375\n",
      "2016-08-12T18:59:58.142930: step 353, loss 0.150396, acc 0.9375\n",
      "2016-08-12T18:59:58.213915: step 354, loss 0.25226, acc 0.875\n",
      "2016-08-12T18:59:58.286365: step 355, loss 0.146954, acc 0.96875\n",
      "2016-08-12T18:59:58.358790: step 356, loss 0.221016, acc 0.875\n",
      "2016-08-12T18:59:58.431176: step 357, loss 0.24995, acc 0.890625\n",
      "2016-08-12T18:59:58.504651: step 358, loss 0.161667, acc 0.953125\n",
      "2016-08-12T18:59:58.576806: step 359, loss 0.197736, acc 0.9375\n",
      "2016-08-12T18:59:58.628096: step 360, loss 0.336215, acc 0.828571\n",
      "2016-08-12T18:59:58.701252: step 361, loss 0.110943, acc 0.984375\n",
      "2016-08-12T18:59:58.773784: step 362, loss 0.151676, acc 0.9375\n",
      "2016-08-12T18:59:58.845498: step 363, loss 0.167335, acc 0.9375\n",
      "2016-08-12T18:59:58.917259: step 364, loss 0.13558, acc 0.96875\n",
      "2016-08-12T18:59:58.987881: step 365, loss 0.154381, acc 0.96875\n",
      "2016-08-12T18:59:59.062189: step 366, loss 0.15699, acc 0.921875\n",
      "2016-08-12T18:59:59.133924: step 367, loss 0.156856, acc 0.96875\n",
      "2016-08-12T18:59:59.206415: step 368, loss 0.190371, acc 0.90625\n",
      "2016-08-12T18:59:59.279101: step 369, loss 0.168666, acc 0.9375\n",
      "2016-08-12T18:59:59.349149: step 370, loss 0.128339, acc 1\n",
      "2016-08-12T18:59:59.421556: step 371, loss 0.15734, acc 0.953125\n",
      "2016-08-12T18:59:59.494682: step 372, loss 0.134154, acc 0.96875\n",
      "2016-08-12T18:59:59.565548: step 373, loss 0.158341, acc 0.953125\n",
      "2016-08-12T18:59:59.636421: step 374, loss 0.245331, acc 0.890625\n",
      "2016-08-12T18:59:59.684598: step 375, loss 0.113978, acc 0.971429\n",
      "2016-08-12T18:59:59.756998: step 376, loss 0.148579, acc 0.953125\n",
      "2016-08-12T18:59:59.826935: step 377, loss 0.169413, acc 0.921875\n",
      "2016-08-12T18:59:59.898248: step 378, loss 0.169322, acc 0.921875\n",
      "2016-08-12T18:59:59.967719: step 379, loss 0.115696, acc 0.984375\n",
      "2016-08-12T19:00:00.038470: step 380, loss 0.232643, acc 0.828125\n",
      "2016-08-12T19:00:00.108567: step 381, loss 0.154858, acc 0.921875\n",
      "2016-08-12T19:00:00.180445: step 382, loss 0.179648, acc 0.9375\n",
      "2016-08-12T19:00:00.250578: step 383, loss 0.112022, acc 0.921875\n",
      "2016-08-12T19:00:00.321937: step 384, loss 0.152476, acc 0.953125\n",
      "2016-08-12T19:00:00.394403: step 385, loss 0.151143, acc 0.953125\n",
      "2016-08-12T19:00:00.467915: step 386, loss 0.179224, acc 0.90625\n",
      "2016-08-12T19:00:00.539279: step 387, loss 0.112265, acc 0.984375\n",
      "2016-08-12T19:00:00.612288: step 388, loss 0.193819, acc 0.921875\n",
      "2016-08-12T19:00:00.683928: step 389, loss 0.108852, acc 0.984375\n",
      "2016-08-12T19:00:00.732073: step 390, loss 0.105812, acc 0.971429\n",
      "2016-08-12T19:00:00.807230: step 391, loss 0.0974794, acc 0.984375\n",
      "2016-08-12T19:00:00.878031: step 392, loss 0.167947, acc 0.9375\n",
      "2016-08-12T19:00:00.950217: step 393, loss 0.149839, acc 0.953125\n",
      "2016-08-12T19:00:01.021736: step 394, loss 0.201658, acc 0.921875\n",
      "2016-08-12T19:00:01.091289: step 395, loss 0.130745, acc 0.9375\n",
      "2016-08-12T19:00:01.161097: step 396, loss 0.134628, acc 0.953125\n",
      "2016-08-12T19:00:01.233157: step 397, loss 0.253096, acc 0.921875\n",
      "2016-08-12T19:00:01.305610: step 398, loss 0.155285, acc 0.96875\n",
      "2016-08-12T19:00:01.375767: step 399, loss 0.135772, acc 0.9375\n",
      "2016-08-12T19:00:01.448681: step 400, loss 0.0812476, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:01.759238: step 400, loss 0.509824, acc 0.751\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-400\n",
      "\n",
      "2016-08-12T19:00:02.436801: step 401, loss 0.0904948, acc 1\n",
      "2016-08-12T19:00:02.509132: step 402, loss 0.149803, acc 0.9375\n",
      "2016-08-12T19:00:02.581860: step 403, loss 0.128965, acc 0.9375\n",
      "2016-08-12T19:00:02.653813: step 404, loss 0.17707, acc 0.90625\n",
      "2016-08-12T19:00:02.709418: step 405, loss 0.0872967, acc 0.971429\n",
      "2016-08-12T19:00:02.791912: step 406, loss 0.215712, acc 0.921875\n",
      "2016-08-12T19:00:02.873155: step 407, loss 0.112713, acc 0.96875\n",
      "2016-08-12T19:00:02.947323: step 408, loss 0.103627, acc 0.984375\n",
      "2016-08-12T19:00:03.020549: step 409, loss 0.0957093, acc 0.96875\n",
      "2016-08-12T19:00:03.093956: step 410, loss 0.108877, acc 0.96875\n",
      "2016-08-12T19:00:03.169079: step 411, loss 0.164126, acc 0.96875\n",
      "2016-08-12T19:00:03.243586: step 412, loss 0.214528, acc 0.90625\n",
      "2016-08-12T19:00:03.316262: step 413, loss 0.129064, acc 0.96875\n",
      "2016-08-12T19:00:03.388920: step 414, loss 0.103693, acc 0.96875\n",
      "2016-08-12T19:00:03.460668: step 415, loss 0.147317, acc 0.984375\n",
      "2016-08-12T19:00:03.533096: step 416, loss 0.1168, acc 0.953125\n",
      "2016-08-12T19:00:03.607651: step 417, loss 0.162608, acc 0.9375\n",
      "2016-08-12T19:00:03.679344: step 418, loss 0.130929, acc 0.96875\n",
      "2016-08-12T19:00:03.751299: step 419, loss 0.167331, acc 0.921875\n",
      "2016-08-12T19:00:03.801074: step 420, loss 0.174462, acc 0.942857\n",
      "2016-08-12T19:00:03.872454: step 421, loss 0.186071, acc 0.90625\n",
      "2016-08-12T19:00:03.945811: step 422, loss 0.103886, acc 0.96875\n",
      "2016-08-12T19:00:04.016461: step 423, loss 0.187514, acc 0.9375\n",
      "2016-08-12T19:00:04.087934: step 424, loss 0.143122, acc 0.953125\n",
      "2016-08-12T19:00:04.160233: step 425, loss 0.0801144, acc 0.984375\n",
      "2016-08-12T19:00:04.232298: step 426, loss 0.111137, acc 0.984375\n",
      "2016-08-12T19:00:04.305619: step 427, loss 0.226885, acc 0.890625\n",
      "2016-08-12T19:00:04.376228: step 428, loss 0.0952901, acc 0.96875\n",
      "2016-08-12T19:00:04.446432: step 429, loss 0.170824, acc 0.921875\n",
      "2016-08-12T19:00:04.516485: step 430, loss 0.0872461, acc 0.984375\n",
      "2016-08-12T19:00:04.586003: step 431, loss 0.156724, acc 0.921875\n",
      "2016-08-12T19:00:04.657574: step 432, loss 0.131466, acc 0.9375\n",
      "2016-08-12T19:00:04.728824: step 433, loss 0.106205, acc 0.953125\n",
      "2016-08-12T19:00:04.803328: step 434, loss 0.110005, acc 0.96875\n",
      "2016-08-12T19:00:04.853128: step 435, loss 0.181502, acc 0.914286\n",
      "2016-08-12T19:00:04.924403: step 436, loss 0.157929, acc 0.890625\n",
      "2016-08-12T19:00:04.995486: step 437, loss 0.197324, acc 0.90625\n",
      "2016-08-12T19:00:05.066747: step 438, loss 0.16566, acc 0.953125\n",
      "2016-08-12T19:00:05.138737: step 439, loss 0.160044, acc 0.953125\n",
      "2016-08-12T19:00:05.211015: step 440, loss 0.165468, acc 0.953125\n",
      "2016-08-12T19:00:05.283189: step 441, loss 0.116786, acc 0.96875\n",
      "2016-08-12T19:00:05.355538: step 442, loss 0.155104, acc 0.953125\n",
      "2016-08-12T19:00:05.428416: step 443, loss 0.105601, acc 0.96875\n",
      "2016-08-12T19:00:05.501616: step 444, loss 0.193504, acc 0.890625\n",
      "2016-08-12T19:00:05.574585: step 445, loss 0.154808, acc 0.953125\n",
      "2016-08-12T19:00:05.645412: step 446, loss 0.102315, acc 0.984375\n",
      "2016-08-12T19:00:05.717571: step 447, loss 0.131901, acc 0.96875\n",
      "2016-08-12T19:00:05.790954: step 448, loss 0.157006, acc 0.9375\n",
      "2016-08-12T19:00:05.861963: step 449, loss 0.129601, acc 0.96875\n",
      "2016-08-12T19:00:05.913052: step 450, loss 0.266542, acc 0.885714\n",
      "2016-08-12T19:00:05.983678: step 451, loss 0.11817, acc 0.96875\n",
      "2016-08-12T19:00:06.055698: step 452, loss 0.172037, acc 0.96875\n",
      "2016-08-12T19:00:06.128382: step 453, loss 0.136754, acc 0.953125\n",
      "2016-08-12T19:00:06.201701: step 454, loss 0.158953, acc 0.921875\n",
      "2016-08-12T19:00:06.273018: step 455, loss 0.0990653, acc 0.984375\n",
      "2016-08-12T19:00:06.343176: step 456, loss 0.100196, acc 0.984375\n",
      "2016-08-12T19:00:06.414424: step 457, loss 0.102845, acc 0.953125\n",
      "2016-08-12T19:00:06.486850: step 458, loss 0.0982393, acc 0.96875\n",
      "2016-08-12T19:00:06.559912: step 459, loss 0.144612, acc 0.9375\n",
      "2016-08-12T19:00:06.631293: step 460, loss 0.149558, acc 0.953125\n",
      "2016-08-12T19:00:06.703911: step 461, loss 0.135244, acc 0.96875\n",
      "2016-08-12T19:00:06.776180: step 462, loss 0.0619671, acc 1\n",
      "2016-08-12T19:00:06.854007: step 463, loss 0.127602, acc 0.953125\n",
      "2016-08-12T19:00:06.927403: step 464, loss 0.168814, acc 0.921875\n",
      "2016-08-12T19:00:06.977748: step 465, loss 0.0886772, acc 0.971429\n",
      "2016-08-12T19:00:07.049846: step 466, loss 0.130063, acc 0.96875\n",
      "2016-08-12T19:00:07.122994: step 467, loss 0.131034, acc 0.953125\n",
      "2016-08-12T19:00:07.194846: step 468, loss 0.114893, acc 0.96875\n",
      "2016-08-12T19:00:07.266952: step 469, loss 0.114646, acc 0.96875\n",
      "2016-08-12T19:00:07.339008: step 470, loss 0.101137, acc 0.953125\n",
      "2016-08-12T19:00:07.413185: step 471, loss 0.148256, acc 0.953125\n",
      "2016-08-12T19:00:07.485356: step 472, loss 0.147603, acc 0.953125\n",
      "2016-08-12T19:00:07.559012: step 473, loss 0.127711, acc 0.953125\n",
      "2016-08-12T19:00:07.632313: step 474, loss 0.130927, acc 0.953125\n",
      "2016-08-12T19:00:07.705968: step 475, loss 0.132451, acc 0.96875\n",
      "2016-08-12T19:00:07.775568: step 476, loss 0.150699, acc 0.953125\n",
      "2016-08-12T19:00:07.847900: step 477, loss 0.110283, acc 0.984375\n",
      "2016-08-12T19:00:07.918721: step 478, loss 0.0905502, acc 0.984375\n",
      "2016-08-12T19:00:07.991930: step 479, loss 0.106637, acc 0.96875\n",
      "2016-08-12T19:00:08.043293: step 480, loss 0.154208, acc 0.942857\n",
      "2016-08-12T19:00:08.114107: step 481, loss 0.0990796, acc 0.984375\n",
      "2016-08-12T19:00:08.184577: step 482, loss 0.0969129, acc 0.984375\n",
      "2016-08-12T19:00:08.256100: step 483, loss 0.0767526, acc 1\n",
      "2016-08-12T19:00:08.327934: step 484, loss 0.21119, acc 0.90625\n",
      "2016-08-12T19:00:08.401250: step 485, loss 0.154436, acc 0.921875\n",
      "2016-08-12T19:00:08.474019: step 486, loss 0.121529, acc 0.9375\n",
      "2016-08-12T19:00:08.546032: step 487, loss 0.100679, acc 1\n",
      "2016-08-12T19:00:08.620442: step 488, loss 0.111555, acc 0.953125\n",
      "2016-08-12T19:00:08.692249: step 489, loss 0.145708, acc 0.96875\n",
      "2016-08-12T19:00:08.765692: step 490, loss 0.173009, acc 0.96875\n",
      "2016-08-12T19:00:08.838787: step 491, loss 0.164328, acc 0.921875\n",
      "2016-08-12T19:00:08.912274: step 492, loss 0.181777, acc 0.9375\n",
      "2016-08-12T19:00:08.984411: step 493, loss 0.124868, acc 0.984375\n",
      "2016-08-12T19:00:09.057521: step 494, loss 0.114999, acc 0.953125\n",
      "2016-08-12T19:00:09.108735: step 495, loss 0.0926449, acc 1\n",
      "2016-08-12T19:00:09.178441: step 496, loss 0.118699, acc 0.9375\n",
      "2016-08-12T19:00:09.249533: step 497, loss 0.141668, acc 0.9375\n",
      "2016-08-12T19:00:09.320639: step 498, loss 0.0956794, acc 0.984375\n",
      "2016-08-12T19:00:09.392399: step 499, loss 0.166129, acc 0.953125\n",
      "2016-08-12T19:00:09.464969: step 500, loss 0.087932, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:09.772726: step 500, loss 0.544008, acc 0.749\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-500\n",
      "\n",
      "2016-08-12T19:00:10.472156: step 501, loss 0.112503, acc 0.953125\n",
      "2016-08-12T19:00:10.542687: step 502, loss 0.0926684, acc 0.984375\n",
      "2016-08-12T19:00:10.614942: step 503, loss 0.158118, acc 0.9375\n",
      "2016-08-12T19:00:10.687378: step 504, loss 0.139869, acc 0.953125\n",
      "2016-08-12T19:00:10.758101: step 505, loss 0.113911, acc 0.9375\n",
      "2016-08-12T19:00:10.830357: step 506, loss 0.127997, acc 0.953125\n",
      "2016-08-12T19:00:10.906893: step 507, loss 0.0955378, acc 1\n",
      "2016-08-12T19:00:10.979568: step 508, loss 0.135769, acc 0.953125\n",
      "2016-08-12T19:00:11.049896: step 509, loss 0.0925927, acc 0.984375\n",
      "2016-08-12T19:00:11.099040: step 510, loss 0.0731357, acc 0.971429\n",
      "2016-08-12T19:00:11.169125: step 511, loss 0.0823975, acc 0.984375\n",
      "2016-08-12T19:00:11.240836: step 512, loss 0.120805, acc 0.984375\n",
      "2016-08-12T19:00:11.318400: step 513, loss 0.0995895, acc 0.96875\n",
      "2016-08-12T19:00:11.390572: step 514, loss 0.0747411, acc 1\n",
      "2016-08-12T19:00:11.461856: step 515, loss 0.09841, acc 0.96875\n",
      "2016-08-12T19:00:11.536095: step 516, loss 0.165481, acc 0.9375\n",
      "2016-08-12T19:00:11.609294: step 517, loss 0.1224, acc 0.953125\n",
      "2016-08-12T19:00:11.681305: step 518, loss 0.0894499, acc 0.984375\n",
      "2016-08-12T19:00:11.753655: step 519, loss 0.113427, acc 0.96875\n",
      "2016-08-12T19:00:11.824559: step 520, loss 0.123603, acc 0.9375\n",
      "2016-08-12T19:00:11.898675: step 521, loss 0.101315, acc 0.984375\n",
      "2016-08-12T19:00:11.970147: step 522, loss 0.153155, acc 0.9375\n",
      "2016-08-12T19:00:12.041992: step 523, loss 0.111961, acc 0.96875\n",
      "2016-08-12T19:00:12.113580: step 524, loss 0.0814101, acc 0.984375\n",
      "2016-08-12T19:00:12.162255: step 525, loss 0.0590309, acc 1\n",
      "2016-08-12T19:00:12.234937: step 526, loss 0.0663607, acc 1\n",
      "2016-08-12T19:00:12.307558: step 527, loss 0.160459, acc 0.9375\n",
      "2016-08-12T19:00:12.380822: step 528, loss 0.12067, acc 0.96875\n",
      "2016-08-12T19:00:12.452474: step 529, loss 0.0808776, acc 0.96875\n",
      "2016-08-12T19:00:12.523932: step 530, loss 0.123875, acc 0.96875\n",
      "2016-08-12T19:00:12.596797: step 531, loss 0.105314, acc 0.984375\n",
      "2016-08-12T19:00:12.669440: step 532, loss 0.0900097, acc 0.96875\n",
      "2016-08-12T19:00:12.741220: step 533, loss 0.0681238, acc 1\n",
      "2016-08-12T19:00:12.811850: step 534, loss 0.101914, acc 0.984375\n",
      "2016-08-12T19:00:12.883659: step 535, loss 0.112065, acc 0.953125\n",
      "2016-08-12T19:00:12.956763: step 536, loss 0.128246, acc 0.96875\n",
      "2016-08-12T19:00:13.028110: step 537, loss 0.112941, acc 0.96875\n",
      "2016-08-12T19:00:13.099272: step 538, loss 0.110688, acc 0.984375\n",
      "2016-08-12T19:00:13.171589: step 539, loss 0.0880764, acc 0.984375\n",
      "2016-08-12T19:00:13.221001: step 540, loss 0.0992637, acc 1\n",
      "2016-08-12T19:00:13.292752: step 541, loss 0.0704734, acc 0.984375\n",
      "2016-08-12T19:00:13.365967: step 542, loss 0.0905699, acc 1\n",
      "2016-08-12T19:00:13.437920: step 543, loss 0.137106, acc 0.96875\n",
      "2016-08-12T19:00:13.507919: step 544, loss 0.0854784, acc 0.984375\n",
      "2016-08-12T19:00:13.579017: step 545, loss 0.0759105, acc 1\n",
      "2016-08-12T19:00:13.648849: step 546, loss 0.0547916, acc 0.984375\n",
      "2016-08-12T19:00:13.719612: step 547, loss 0.102518, acc 0.984375\n",
      "2016-08-12T19:00:13.790196: step 548, loss 0.0917155, acc 0.984375\n",
      "2016-08-12T19:00:13.861713: step 549, loss 0.105227, acc 0.96875\n",
      "2016-08-12T19:00:13.934861: step 550, loss 0.10299, acc 0.96875\n",
      "2016-08-12T19:00:14.004302: step 551, loss 0.10459, acc 0.984375\n",
      "2016-08-12T19:00:14.074701: step 552, loss 0.091507, acc 1\n",
      "2016-08-12T19:00:14.146113: step 553, loss 0.0684199, acc 1\n",
      "2016-08-12T19:00:14.217641: step 554, loss 0.12849, acc 0.96875\n",
      "2016-08-12T19:00:14.267835: step 555, loss 0.176348, acc 0.914286\n",
      "2016-08-12T19:00:14.339662: step 556, loss 0.0788459, acc 0.984375\n",
      "2016-08-12T19:00:14.410594: step 557, loss 0.0910999, acc 0.984375\n",
      "2016-08-12T19:00:14.483605: step 558, loss 0.130906, acc 0.984375\n",
      "2016-08-12T19:00:14.558157: step 559, loss 0.0827075, acc 0.96875\n",
      "2016-08-12T19:00:14.631416: step 560, loss 0.109929, acc 0.96875\n",
      "2016-08-12T19:00:14.700914: step 561, loss 0.0944329, acc 0.984375\n",
      "2016-08-12T19:00:14.773473: step 562, loss 0.0579254, acc 0.984375\n",
      "2016-08-12T19:00:14.845655: step 563, loss 0.107784, acc 0.96875\n",
      "2016-08-12T19:00:14.917344: step 564, loss 0.0738741, acc 0.96875\n",
      "2016-08-12T19:00:14.989497: step 565, loss 0.0826925, acc 0.984375\n",
      "2016-08-12T19:00:15.060279: step 566, loss 0.0963875, acc 0.96875\n",
      "2016-08-12T19:00:15.132873: step 567, loss 0.0770886, acc 0.96875\n",
      "2016-08-12T19:00:15.204541: step 568, loss 0.100697, acc 0.96875\n",
      "2016-08-12T19:00:15.276479: step 569, loss 0.0855603, acc 1\n",
      "2016-08-12T19:00:15.326721: step 570, loss 0.118277, acc 0.971429\n",
      "2016-08-12T19:00:15.399335: step 571, loss 0.0620354, acc 1\n",
      "2016-08-12T19:00:15.470655: step 572, loss 0.09316, acc 0.984375\n",
      "2016-08-12T19:00:15.543088: step 573, loss 0.0513915, acc 1\n",
      "2016-08-12T19:00:15.614148: step 574, loss 0.0991517, acc 0.9375\n",
      "2016-08-12T19:00:15.686800: step 575, loss 0.0499533, acc 1\n",
      "2016-08-12T19:00:15.757801: step 576, loss 0.0642964, acc 1\n",
      "2016-08-12T19:00:15.836922: step 577, loss 0.0826322, acc 0.96875\n",
      "2016-08-12T19:00:15.909126: step 578, loss 0.0781262, acc 1\n",
      "2016-08-12T19:00:15.981337: step 579, loss 0.111585, acc 0.96875\n",
      "2016-08-12T19:00:16.052665: step 580, loss 0.0446558, acc 1\n",
      "2016-08-12T19:00:16.127127: step 581, loss 0.0886487, acc 0.96875\n",
      "2016-08-12T19:00:16.200070: step 582, loss 0.101195, acc 0.984375\n",
      "2016-08-12T19:00:16.272602: step 583, loss 0.0704339, acc 0.984375\n",
      "2016-08-12T19:00:16.344543: step 584, loss 0.0880599, acc 0.984375\n",
      "2016-08-12T19:00:16.393499: step 585, loss 0.0892018, acc 0.971429\n",
      "2016-08-12T19:00:16.467411: step 586, loss 0.0574204, acc 0.984375\n",
      "2016-08-12T19:00:16.538518: step 587, loss 0.140293, acc 0.953125\n",
      "2016-08-12T19:00:16.610346: step 588, loss 0.0739284, acc 1\n",
      "2016-08-12T19:00:16.682939: step 589, loss 0.128434, acc 0.96875\n",
      "2016-08-12T19:00:16.755253: step 590, loss 0.0828888, acc 0.984375\n",
      "2016-08-12T19:00:16.828111: step 591, loss 0.055303, acc 1\n",
      "2016-08-12T19:00:16.903256: step 592, loss 0.0580551, acc 1\n",
      "2016-08-12T19:00:16.973693: step 593, loss 0.158052, acc 0.9375\n",
      "2016-08-12T19:00:17.044711: step 594, loss 0.09786, acc 0.984375\n",
      "2016-08-12T19:00:17.116911: step 595, loss 0.0676495, acc 0.96875\n",
      "2016-08-12T19:00:17.188524: step 596, loss 0.0759486, acc 0.96875\n",
      "2016-08-12T19:00:17.260876: step 597, loss 0.20127, acc 0.96875\n",
      "2016-08-12T19:00:17.333757: step 598, loss 0.0760627, acc 0.984375\n",
      "2016-08-12T19:00:17.406342: step 599, loss 0.0984583, acc 0.984375\n",
      "2016-08-12T19:00:17.456735: step 600, loss 0.0982454, acc 0.971429\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:17.764285: step 600, loss 0.545494, acc 0.755\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-600\n",
      "\n",
      "2016-08-12T19:00:18.448870: step 601, loss 0.108858, acc 0.984375\n",
      "2016-08-12T19:00:18.521625: step 602, loss 0.0999781, acc 0.96875\n",
      "2016-08-12T19:00:18.594670: step 603, loss 0.0612808, acc 1\n",
      "2016-08-12T19:00:18.666883: step 604, loss 0.160817, acc 0.90625\n",
      "2016-08-12T19:00:18.741518: step 605, loss 0.0498702, acc 1\n",
      "2016-08-12T19:00:18.814801: step 606, loss 0.0771201, acc 0.984375\n",
      "2016-08-12T19:00:18.886672: step 607, loss 0.0885109, acc 0.96875\n",
      "2016-08-12T19:00:18.958728: step 608, loss 0.125981, acc 0.953125\n",
      "2016-08-12T19:00:19.030608: step 609, loss 0.0531174, acc 1\n",
      "2016-08-12T19:00:19.103682: step 610, loss 0.0546408, acc 0.984375\n",
      "2016-08-12T19:00:19.177685: step 611, loss 0.0743579, acc 0.984375\n",
      "2016-08-12T19:00:19.249709: step 612, loss 0.090283, acc 0.953125\n",
      "2016-08-12T19:00:19.323036: step 613, loss 0.103401, acc 0.96875\n",
      "2016-08-12T19:00:19.393704: step 614, loss 0.0783682, acc 0.984375\n",
      "2016-08-12T19:00:19.442909: step 615, loss 0.0568589, acc 1\n",
      "2016-08-12T19:00:19.515737: step 616, loss 0.0711131, acc 0.984375\n",
      "2016-08-12T19:00:19.589285: step 617, loss 0.0661109, acc 0.984375\n",
      "2016-08-12T19:00:19.658334: step 618, loss 0.0923797, acc 0.984375\n",
      "2016-08-12T19:00:19.731011: step 619, loss 0.0702754, acc 0.984375\n",
      "2016-08-12T19:00:19.802269: step 620, loss 0.08129, acc 0.96875\n",
      "2016-08-12T19:00:19.874370: step 621, loss 0.0846426, acc 0.984375\n",
      "2016-08-12T19:00:19.947153: step 622, loss 0.0537871, acc 0.984375\n",
      "2016-08-12T19:00:20.017490: step 623, loss 0.0846222, acc 1\n",
      "2016-08-12T19:00:20.088542: step 624, loss 0.0805698, acc 0.984375\n",
      "2016-08-12T19:00:20.160235: step 625, loss 0.0893344, acc 0.96875\n",
      "2016-08-12T19:00:20.232336: step 626, loss 0.154286, acc 0.984375\n",
      "2016-08-12T19:00:20.305865: step 627, loss 0.0848696, acc 0.96875\n",
      "2016-08-12T19:00:20.378512: step 628, loss 0.0553128, acc 1\n",
      "2016-08-12T19:00:20.449985: step 629, loss 0.0773683, acc 1\n",
      "2016-08-12T19:00:20.499534: step 630, loss 0.0755806, acc 0.971429\n",
      "2016-08-12T19:00:20.572205: step 631, loss 0.110543, acc 0.953125\n",
      "2016-08-12T19:00:20.644354: step 632, loss 0.10501, acc 0.953125\n",
      "2016-08-12T19:00:20.716570: step 633, loss 0.0512467, acc 0.984375\n",
      "2016-08-12T19:00:20.786048: step 634, loss 0.0727347, acc 0.984375\n",
      "2016-08-12T19:00:20.857676: step 635, loss 0.0702243, acc 0.984375\n",
      "2016-08-12T19:00:20.927438: step 636, loss 0.083533, acc 0.984375\n",
      "2016-08-12T19:00:21.004609: step 637, loss 0.0989953, acc 0.984375\n",
      "2016-08-12T19:00:21.076200: step 638, loss 0.0822455, acc 0.984375\n",
      "2016-08-12T19:00:21.149720: step 639, loss 0.0891213, acc 0.953125\n",
      "2016-08-12T19:00:21.221276: step 640, loss 0.112667, acc 0.96875\n",
      "2016-08-12T19:00:21.293345: step 641, loss 0.114855, acc 0.96875\n",
      "2016-08-12T19:00:21.366048: step 642, loss 0.0711006, acc 1\n",
      "2016-08-12T19:00:21.437865: step 643, loss 0.114583, acc 0.953125\n",
      "2016-08-12T19:00:21.511124: step 644, loss 0.0572137, acc 0.984375\n",
      "2016-08-12T19:00:21.560035: step 645, loss 0.102956, acc 0.971429\n",
      "2016-08-12T19:00:21.631821: step 646, loss 0.062408, acc 0.984375\n",
      "2016-08-12T19:00:21.704846: step 647, loss 0.059873, acc 1\n",
      "2016-08-12T19:00:21.776950: step 648, loss 0.0488421, acc 1\n",
      "2016-08-12T19:00:21.848267: step 649, loss 0.110733, acc 0.96875\n",
      "2016-08-12T19:00:21.922307: step 650, loss 0.110432, acc 0.9375\n",
      "2016-08-12T19:00:21.993687: step 651, loss 0.0696291, acc 0.984375\n",
      "2016-08-12T19:00:22.066815: step 652, loss 0.0551248, acc 1\n",
      "2016-08-12T19:00:22.137952: step 653, loss 0.0650023, acc 0.984375\n",
      "2016-08-12T19:00:22.211250: step 654, loss 0.0963327, acc 0.96875\n",
      "2016-08-12T19:00:22.283943: step 655, loss 0.0619747, acc 0.984375\n",
      "2016-08-12T19:00:22.356242: step 656, loss 0.0890141, acc 0.984375\n",
      "2016-08-12T19:00:22.427215: step 657, loss 0.122042, acc 0.9375\n",
      "2016-08-12T19:00:22.501147: step 658, loss 0.0820642, acc 0.984375\n",
      "2016-08-12T19:00:22.573629: step 659, loss 0.106106, acc 0.984375\n",
      "2016-08-12T19:00:22.625506: step 660, loss 0.0906588, acc 1\n",
      "2016-08-12T19:00:22.697833: step 661, loss 0.150243, acc 0.90625\n",
      "2016-08-12T19:00:22.770350: step 662, loss 0.0896, acc 0.984375\n",
      "2016-08-12T19:00:22.844850: step 663, loss 0.0597495, acc 1\n",
      "2016-08-12T19:00:22.916192: step 664, loss 0.086173, acc 0.984375\n",
      "2016-08-12T19:00:22.989340: step 665, loss 0.0840414, acc 0.984375\n",
      "2016-08-12T19:00:23.060559: step 666, loss 0.0896692, acc 0.96875\n",
      "2016-08-12T19:00:23.131426: step 667, loss 0.0551567, acc 1\n",
      "2016-08-12T19:00:23.203681: step 668, loss 0.0373009, acc 1\n",
      "2016-08-12T19:00:23.276145: step 669, loss 0.0755549, acc 1\n",
      "2016-08-12T19:00:23.347096: step 670, loss 0.0821165, acc 0.984375\n",
      "2016-08-12T19:00:23.420120: step 671, loss 0.136686, acc 0.984375\n",
      "2016-08-12T19:00:23.493822: step 672, loss 0.0908012, acc 0.984375\n",
      "2016-08-12T19:00:23.565663: step 673, loss 0.0861347, acc 0.984375\n",
      "2016-08-12T19:00:23.638481: step 674, loss 0.0634083, acc 0.984375\n",
      "2016-08-12T19:00:23.688772: step 675, loss 0.0474576, acc 1\n",
      "2016-08-12T19:00:23.760608: step 676, loss 0.0572011, acc 0.984375\n",
      "2016-08-12T19:00:23.833135: step 677, loss 0.0837454, acc 0.984375\n",
      "2016-08-12T19:00:23.905201: step 678, loss 0.142707, acc 0.953125\n",
      "2016-08-12T19:00:23.978756: step 679, loss 0.0884212, acc 1\n",
      "2016-08-12T19:00:24.050818: step 680, loss 0.0970511, acc 0.953125\n",
      "2016-08-12T19:00:24.122596: step 681, loss 0.0563241, acc 1\n",
      "2016-08-12T19:00:24.194382: step 682, loss 0.103697, acc 0.96875\n",
      "2016-08-12T19:00:24.268481: step 683, loss 0.11763, acc 0.953125\n",
      "2016-08-12T19:00:24.340923: step 684, loss 0.0733207, acc 0.984375\n",
      "2016-08-12T19:00:24.413694: step 685, loss 0.0705664, acc 0.984375\n",
      "2016-08-12T19:00:24.485665: step 686, loss 0.0659197, acc 0.984375\n",
      "2016-08-12T19:00:24.558375: step 687, loss 0.0900286, acc 0.96875\n",
      "2016-08-12T19:00:24.630253: step 688, loss 0.0895835, acc 0.984375\n",
      "2016-08-12T19:00:24.703591: step 689, loss 0.108654, acc 0.953125\n",
      "2016-08-12T19:00:24.754359: step 690, loss 0.0390058, acc 1\n",
      "2016-08-12T19:00:24.827445: step 691, loss 0.0453139, acc 1\n",
      "2016-08-12T19:00:24.900443: step 692, loss 0.0908535, acc 0.96875\n",
      "2016-08-12T19:00:24.974710: step 693, loss 0.0959752, acc 0.953125\n",
      "2016-08-12T19:00:25.048289: step 694, loss 0.0451743, acc 1\n",
      "2016-08-12T19:00:25.119358: step 695, loss 0.105954, acc 0.96875\n",
      "2016-08-12T19:00:25.192751: step 696, loss 0.0843862, acc 0.984375\n",
      "2016-08-12T19:00:25.263659: step 697, loss 0.103323, acc 0.953125\n",
      "2016-08-12T19:00:25.336757: step 698, loss 0.093744, acc 0.984375\n",
      "2016-08-12T19:00:25.408495: step 699, loss 0.0511738, acc 0.984375\n",
      "2016-08-12T19:00:25.482177: step 700, loss 0.1708, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:25.788225: step 700, loss 0.555772, acc 0.749\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-700\n",
      "\n",
      "2016-08-12T19:00:26.482489: step 701, loss 0.0249113, acc 1\n",
      "2016-08-12T19:00:26.553429: step 702, loss 0.0585443, acc 1\n",
      "2016-08-12T19:00:26.625341: step 703, loss 0.0732851, acc 1\n",
      "2016-08-12T19:00:26.696614: step 704, loss 0.0557384, acc 1\n",
      "2016-08-12T19:00:26.747880: step 705, loss 0.0914255, acc 1\n",
      "2016-08-12T19:00:26.820668: step 706, loss 0.0314348, acc 1\n",
      "2016-08-12T19:00:26.892641: step 707, loss 0.0855602, acc 0.984375\n",
      "2016-08-12T19:00:26.967108: step 708, loss 0.0823489, acc 0.984375\n",
      "2016-08-12T19:00:27.038422: step 709, loss 0.0559425, acc 0.984375\n",
      "2016-08-12T19:00:27.112250: step 710, loss 0.0790211, acc 0.984375\n",
      "2016-08-12T19:00:27.184297: step 711, loss 0.0695437, acc 0.984375\n",
      "2016-08-12T19:00:27.257516: step 712, loss 0.0743411, acc 0.984375\n",
      "2016-08-12T19:00:27.331130: step 713, loss 0.0671373, acc 1\n",
      "2016-08-12T19:00:27.403710: step 714, loss 0.110597, acc 0.953125\n",
      "2016-08-12T19:00:27.477015: step 715, loss 0.0865966, acc 0.984375\n",
      "2016-08-12T19:00:27.549213: step 716, loss 0.0676918, acc 1\n",
      "2016-08-12T19:00:27.622009: step 717, loss 0.0786439, acc 0.984375\n",
      "2016-08-12T19:00:27.694640: step 718, loss 0.0617739, acc 1\n",
      "2016-08-12T19:00:27.767300: step 719, loss 0.0730347, acc 1\n",
      "2016-08-12T19:00:27.816007: step 720, loss 0.0461647, acc 1\n",
      "2016-08-12T19:00:27.889570: step 721, loss 0.0347019, acc 1\n",
      "2016-08-12T19:00:27.962542: step 722, loss 0.0316123, acc 1\n",
      "2016-08-12T19:00:28.036449: step 723, loss 0.0948088, acc 0.96875\n",
      "2016-08-12T19:00:28.109278: step 724, loss 0.0846624, acc 0.984375\n",
      "2016-08-12T19:00:28.181737: step 725, loss 0.0526682, acc 1\n",
      "2016-08-12T19:00:28.254955: step 726, loss 0.0525453, acc 1\n",
      "2016-08-12T19:00:28.327019: step 727, loss 0.0543635, acc 0.984375\n",
      "2016-08-12T19:00:28.398935: step 728, loss 0.04418, acc 1\n",
      "2016-08-12T19:00:28.470422: step 729, loss 0.0766212, acc 1\n",
      "2016-08-12T19:00:28.542315: step 730, loss 0.103142, acc 0.96875\n",
      "2016-08-12T19:00:28.614724: step 731, loss 0.0792294, acc 0.96875\n",
      "2016-08-12T19:00:28.687746: step 732, loss 0.129283, acc 0.9375\n",
      "2016-08-12T19:00:28.758041: step 733, loss 0.0602362, acc 1\n",
      "2016-08-12T19:00:28.830851: step 734, loss 0.0871101, acc 0.984375\n",
      "2016-08-12T19:00:28.879181: step 735, loss 0.0509786, acc 1\n",
      "2016-08-12T19:00:28.953377: step 736, loss 0.0874081, acc 0.96875\n",
      "2016-08-12T19:00:29.027459: step 737, loss 0.0760665, acc 0.96875\n",
      "2016-08-12T19:00:29.100930: step 738, loss 0.0735058, acc 1\n",
      "2016-08-12T19:00:29.173150: step 739, loss 0.0344324, acc 1\n",
      "2016-08-12T19:00:29.247453: step 740, loss 0.10026, acc 0.953125\n",
      "2016-08-12T19:00:29.320686: step 741, loss 0.0635271, acc 0.953125\n",
      "2016-08-12T19:00:29.392503: step 742, loss 0.0255397, acc 1\n",
      "2016-08-12T19:00:29.465134: step 743, loss 0.049509, acc 1\n",
      "2016-08-12T19:00:29.535828: step 744, loss 0.0792534, acc 0.984375\n",
      "2016-08-12T19:00:29.607972: step 745, loss 0.0631748, acc 0.984375\n",
      "2016-08-12T19:00:29.678278: step 746, loss 0.0407017, acc 1\n",
      "2016-08-12T19:00:29.750777: step 747, loss 0.0795236, acc 0.984375\n",
      "2016-08-12T19:00:29.822346: step 748, loss 0.0690325, acc 0.96875\n",
      "2016-08-12T19:00:29.895259: step 749, loss 0.0779922, acc 0.96875\n",
      "2016-08-12T19:00:29.943464: step 750, loss 0.0873192, acc 0.971429\n",
      "2016-08-12T19:00:30.015809: step 751, loss 0.041404, acc 1\n",
      "2016-08-12T19:00:30.088032: step 752, loss 0.0741383, acc 1\n",
      "2016-08-12T19:00:30.160646: step 753, loss 0.0226039, acc 1\n",
      "2016-08-12T19:00:30.232465: step 754, loss 0.131448, acc 0.96875\n",
      "2016-08-12T19:00:30.304334: step 755, loss 0.0622725, acc 0.984375\n",
      "2016-08-12T19:00:30.374993: step 756, loss 0.0642012, acc 0.984375\n",
      "2016-08-12T19:00:30.447310: step 757, loss 0.0459657, acc 1\n",
      "2016-08-12T19:00:30.519223: step 758, loss 0.0803937, acc 0.984375\n",
      "2016-08-12T19:00:30.592208: step 759, loss 0.0311648, acc 1\n",
      "2016-08-12T19:00:30.662784: step 760, loss 0.0617977, acc 0.984375\n",
      "2016-08-12T19:00:30.735998: step 761, loss 0.0509834, acc 1\n",
      "2016-08-12T19:00:30.808995: step 762, loss 0.0592525, acc 1\n",
      "2016-08-12T19:00:30.881841: step 763, loss 0.074572, acc 0.984375\n",
      "2016-08-12T19:00:30.954500: step 764, loss 0.0655824, acc 1\n",
      "2016-08-12T19:00:31.004311: step 765, loss 0.0801608, acc 0.971429\n",
      "2016-08-12T19:00:31.079312: step 766, loss 0.0875519, acc 0.96875\n",
      "2016-08-12T19:00:31.151502: step 767, loss 0.074808, acc 0.96875\n",
      "2016-08-12T19:00:31.224302: step 768, loss 0.0924519, acc 0.96875\n",
      "2016-08-12T19:00:31.298226: step 769, loss 0.0503057, acc 1\n",
      "2016-08-12T19:00:31.369986: step 770, loss 0.0709431, acc 0.984375\n",
      "2016-08-12T19:00:31.442832: step 771, loss 0.0887421, acc 0.96875\n",
      "2016-08-12T19:00:31.516572: step 772, loss 0.0396543, acc 1\n",
      "2016-08-12T19:00:31.588749: step 773, loss 0.0533623, acc 0.984375\n",
      "2016-08-12T19:00:31.661523: step 774, loss 0.116149, acc 0.96875\n",
      "2016-08-12T19:00:31.733542: step 775, loss 0.0720458, acc 0.96875\n",
      "2016-08-12T19:00:31.805824: step 776, loss 0.0329411, acc 1\n",
      "2016-08-12T19:00:31.878268: step 777, loss 0.059858, acc 0.984375\n",
      "2016-08-12T19:00:31.950839: step 778, loss 0.042012, acc 0.984375\n",
      "2016-08-12T19:00:32.024542: step 779, loss 0.0389195, acc 1\n",
      "2016-08-12T19:00:32.074122: step 780, loss 0.0449011, acc 1\n",
      "2016-08-12T19:00:32.145808: step 781, loss 0.0609298, acc 0.984375\n",
      "2016-08-12T19:00:32.220563: step 782, loss 0.0618234, acc 0.96875\n",
      "2016-08-12T19:00:32.292445: step 783, loss 0.0379861, acc 1\n",
      "2016-08-12T19:00:32.365763: step 784, loss 0.0857758, acc 0.96875\n",
      "2016-08-12T19:00:32.438198: step 785, loss 0.0494328, acc 1\n",
      "2016-08-12T19:00:32.511421: step 786, loss 0.034639, acc 1\n",
      "2016-08-12T19:00:32.584020: step 787, loss 0.0710594, acc 0.953125\n",
      "2016-08-12T19:00:32.657835: step 788, loss 0.0585581, acc 1\n",
      "2016-08-12T19:00:32.728510: step 789, loss 0.0755466, acc 0.96875\n",
      "2016-08-12T19:00:32.801529: step 790, loss 0.0852409, acc 0.984375\n",
      "2016-08-12T19:00:32.873318: step 791, loss 0.0455797, acc 1\n",
      "2016-08-12T19:00:32.945706: step 792, loss 0.0752184, acc 0.984375\n",
      "2016-08-12T19:00:33.018055: step 793, loss 0.0879365, acc 0.984375\n",
      "2016-08-12T19:00:33.093846: step 794, loss 0.0513838, acc 0.984375\n",
      "2016-08-12T19:00:33.144530: step 795, loss 0.137476, acc 0.942857\n",
      "2016-08-12T19:00:33.217303: step 796, loss 0.120923, acc 0.953125\n",
      "2016-08-12T19:00:33.290318: step 797, loss 0.0353544, acc 1\n",
      "2016-08-12T19:00:33.363544: step 798, loss 0.0527395, acc 1\n",
      "2016-08-12T19:00:33.438137: step 799, loss 0.0412922, acc 1\n",
      "2016-08-12T19:00:33.510417: step 800, loss 0.041267, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:33.821781: step 800, loss 0.578514, acc 0.75\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-800\n",
      "\n",
      "2016-08-12T19:00:34.510194: step 801, loss 0.094259, acc 0.96875\n",
      "2016-08-12T19:00:34.581076: step 802, loss 0.111097, acc 0.96875\n",
      "2016-08-12T19:00:34.656805: step 803, loss 0.0372249, acc 1\n",
      "2016-08-12T19:00:34.728902: step 804, loss 0.0581498, acc 0.984375\n",
      "2016-08-12T19:00:34.800573: step 805, loss 0.0589292, acc 0.984375\n",
      "2016-08-12T19:00:34.873460: step 806, loss 0.0804177, acc 1\n",
      "2016-08-12T19:00:34.945674: step 807, loss 0.0669351, acc 0.984375\n",
      "2016-08-12T19:00:35.019346: step 808, loss 0.0636823, acc 1\n",
      "2016-08-12T19:00:35.090855: step 809, loss 0.0507027, acc 0.984375\n",
      "2016-08-12T19:00:35.137113: step 810, loss 0.0589855, acc 1\n",
      "2016-08-12T19:00:35.211832: step 811, loss 0.0659201, acc 0.96875\n",
      "2016-08-12T19:00:35.286657: step 812, loss 0.0671321, acc 0.96875\n",
      "2016-08-12T19:00:35.361723: step 813, loss 0.0447395, acc 1\n",
      "2016-08-12T19:00:35.432779: step 814, loss 0.083074, acc 0.96875\n",
      "2016-08-12T19:00:35.507593: step 815, loss 0.0582658, acc 0.96875\n",
      "2016-08-12T19:00:35.580039: step 816, loss 0.0419011, acc 1\n",
      "2016-08-12T19:00:35.652925: step 817, loss 0.0792073, acc 0.96875\n",
      "2016-08-12T19:00:35.725267: step 818, loss 0.0357193, acc 1\n",
      "2016-08-12T19:00:35.798981: step 819, loss 0.0564222, acc 0.984375\n",
      "2016-08-12T19:00:35.870168: step 820, loss 0.0954984, acc 0.953125\n",
      "2016-08-12T19:00:35.944078: step 821, loss 0.0812529, acc 0.96875\n",
      "2016-08-12T19:00:36.016617: step 822, loss 0.0494119, acc 1\n",
      "2016-08-12T19:00:36.089187: step 823, loss 0.0420242, acc 0.984375\n",
      "2016-08-12T19:00:36.164514: step 824, loss 0.0518792, acc 1\n",
      "2016-08-12T19:00:36.216641: step 825, loss 0.0418519, acc 1\n",
      "2016-08-12T19:00:36.290599: step 826, loss 0.0555095, acc 0.96875\n",
      "2016-08-12T19:00:36.363236: step 827, loss 0.129863, acc 0.96875\n",
      "2016-08-12T19:00:36.436374: step 828, loss 0.0575394, acc 0.984375\n",
      "2016-08-12T19:00:36.508384: step 829, loss 0.0721883, acc 0.984375\n",
      "2016-08-12T19:00:36.582219: step 830, loss 0.0406029, acc 1\n",
      "2016-08-12T19:00:36.653835: step 831, loss 0.0473026, acc 1\n",
      "2016-08-12T19:00:36.725220: step 832, loss 0.090196, acc 0.984375\n",
      "2016-08-12T19:00:36.796424: step 833, loss 0.0386543, acc 1\n",
      "2016-08-12T19:00:36.870185: step 834, loss 0.0808609, acc 0.984375\n",
      "2016-08-12T19:00:36.944054: step 835, loss 0.0265023, acc 1\n",
      "2016-08-12T19:00:37.015868: step 836, loss 0.0631442, acc 0.984375\n",
      "2016-08-12T19:00:37.090751: step 837, loss 0.0718052, acc 0.984375\n",
      "2016-08-12T19:00:37.162065: step 838, loss 0.0214822, acc 1\n",
      "2016-08-12T19:00:37.235528: step 839, loss 0.0551394, acc 0.984375\n",
      "2016-08-12T19:00:37.284722: step 840, loss 0.071354, acc 1\n",
      "2016-08-12T19:00:37.358524: step 841, loss 0.0550793, acc 0.984375\n",
      "2016-08-12T19:00:37.430587: step 842, loss 0.0518377, acc 1\n",
      "2016-08-12T19:00:37.502581: step 843, loss 0.064212, acc 0.96875\n",
      "2016-08-12T19:00:37.577565: step 844, loss 0.0466717, acc 1\n",
      "2016-08-12T19:00:37.649215: step 845, loss 0.0642117, acc 0.984375\n",
      "2016-08-12T19:00:37.719873: step 846, loss 0.0669329, acc 0.984375\n",
      "2016-08-12T19:00:37.793494: step 847, loss 0.0584674, acc 1\n",
      "2016-08-12T19:00:37.866921: step 848, loss 0.088812, acc 0.96875\n",
      "2016-08-12T19:00:37.939796: step 849, loss 0.0402768, acc 1\n",
      "2016-08-12T19:00:38.011097: step 850, loss 0.0416044, acc 1\n",
      "2016-08-12T19:00:38.083382: step 851, loss 0.035742, acc 1\n",
      "2016-08-12T19:00:38.155497: step 852, loss 0.0437439, acc 1\n",
      "2016-08-12T19:00:38.228272: step 853, loss 0.0616335, acc 1\n",
      "2016-08-12T19:00:38.298888: step 854, loss 0.0598037, acc 0.984375\n",
      "2016-08-12T19:00:38.348243: step 855, loss 0.0979284, acc 0.971429\n",
      "2016-08-12T19:00:38.419144: step 856, loss 0.0427512, acc 1\n",
      "2016-08-12T19:00:38.494435: step 857, loss 0.0669904, acc 0.96875\n",
      "2016-08-12T19:00:38.566530: step 858, loss 0.0392489, acc 1\n",
      "2016-08-12T19:00:38.639511: step 859, loss 0.0681414, acc 0.984375\n",
      "2016-08-12T19:00:38.710962: step 860, loss 0.056716, acc 0.984375\n",
      "2016-08-12T19:00:38.781965: step 861, loss 0.0846837, acc 0.96875\n",
      "2016-08-12T19:00:38.855169: step 862, loss 0.0513804, acc 1\n",
      "2016-08-12T19:00:38.925450: step 863, loss 0.0491209, acc 0.984375\n",
      "2016-08-12T19:00:38.997208: step 864, loss 0.0976983, acc 0.953125\n",
      "2016-08-12T19:00:39.068597: step 865, loss 0.0574599, acc 0.984375\n",
      "2016-08-12T19:00:39.140293: step 866, loss 0.0567861, acc 0.984375\n",
      "2016-08-12T19:00:39.213458: step 867, loss 0.0415978, acc 1\n",
      "2016-08-12T19:00:39.287484: step 868, loss 0.0702985, acc 0.96875\n",
      "2016-08-12T19:00:39.359437: step 869, loss 0.0536178, acc 0.984375\n",
      "2016-08-12T19:00:39.409381: step 870, loss 0.0379161, acc 1\n",
      "2016-08-12T19:00:39.482703: step 871, loss 0.058775, acc 0.984375\n",
      "2016-08-12T19:00:39.554696: step 872, loss 0.0709133, acc 0.96875\n",
      "2016-08-12T19:00:39.627473: step 873, loss 0.0281537, acc 1\n",
      "2016-08-12T19:00:39.700966: step 874, loss 0.0255449, acc 1\n",
      "2016-08-12T19:00:39.773613: step 875, loss 0.0593456, acc 0.984375\n",
      "2016-08-12T19:00:39.847373: step 876, loss 0.042594, acc 1\n",
      "2016-08-12T19:00:39.918622: step 877, loss 0.0417709, acc 1\n",
      "2016-08-12T19:00:39.991289: step 878, loss 0.0527017, acc 1\n",
      "2016-08-12T19:00:40.064761: step 879, loss 0.0409915, acc 1\n",
      "2016-08-12T19:00:40.137483: step 880, loss 0.0540176, acc 1\n",
      "2016-08-12T19:00:40.211152: step 881, loss 0.0741144, acc 0.984375\n",
      "2016-08-12T19:00:40.284495: step 882, loss 0.0515098, acc 1\n",
      "2016-08-12T19:00:40.357082: step 883, loss 0.0935298, acc 0.96875\n",
      "2016-08-12T19:00:40.429804: step 884, loss 0.0632024, acc 0.984375\n",
      "2016-08-12T19:00:40.480884: step 885, loss 0.0463796, acc 0.971429\n",
      "2016-08-12T19:00:40.553484: step 886, loss 0.0610268, acc 1\n",
      "2016-08-12T19:00:40.625377: step 887, loss 0.0670827, acc 0.984375\n",
      "2016-08-12T19:00:40.697361: step 888, loss 0.0545083, acc 1\n",
      "2016-08-12T19:00:40.769578: step 889, loss 0.0314965, acc 1\n",
      "2016-08-12T19:00:40.841051: step 890, loss 0.0681314, acc 0.96875\n",
      "2016-08-12T19:00:40.913672: step 891, loss 0.0583532, acc 0.984375\n",
      "2016-08-12T19:00:40.987211: step 892, loss 0.0484006, acc 0.984375\n",
      "2016-08-12T19:00:41.061109: step 893, loss 0.0471183, acc 0.984375\n",
      "2016-08-12T19:00:41.132505: step 894, loss 0.0801848, acc 0.953125\n",
      "2016-08-12T19:00:41.205120: step 895, loss 0.0204766, acc 1\n",
      "2016-08-12T19:00:41.278074: step 896, loss 0.0285947, acc 1\n",
      "2016-08-12T19:00:41.350382: step 897, loss 0.0383023, acc 1\n",
      "2016-08-12T19:00:41.423223: step 898, loss 0.0718759, acc 0.96875\n",
      "2016-08-12T19:00:41.495954: step 899, loss 0.0576605, acc 0.984375\n",
      "2016-08-12T19:00:41.546062: step 900, loss 0.0481552, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:41.858079: step 900, loss 0.588154, acc 0.754\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-900\n",
      "\n",
      "2016-08-12T19:00:42.540735: step 901, loss 0.030977, acc 1\n",
      "2016-08-12T19:00:42.612626: step 902, loss 0.0908149, acc 0.984375\n",
      "2016-08-12T19:00:42.686048: step 903, loss 0.0394101, acc 1\n",
      "2016-08-12T19:00:42.757793: step 904, loss 0.0612515, acc 0.984375\n",
      "2016-08-12T19:00:42.830857: step 905, loss 0.049794, acc 0.984375\n",
      "2016-08-12T19:00:42.902678: step 906, loss 0.0590544, acc 1\n",
      "2016-08-12T19:00:42.976520: step 907, loss 0.10132, acc 0.953125\n",
      "2016-08-12T19:00:43.050663: step 908, loss 0.0554668, acc 1\n",
      "2016-08-12T19:00:43.122535: step 909, loss 0.0435953, acc 0.984375\n",
      "2016-08-12T19:00:43.194776: step 910, loss 0.0187741, acc 1\n",
      "2016-08-12T19:00:43.265931: step 911, loss 0.0563028, acc 0.984375\n",
      "2016-08-12T19:00:43.338601: step 912, loss 0.0629598, acc 1\n",
      "2016-08-12T19:00:43.410602: step 913, loss 0.0924925, acc 0.984375\n",
      "2016-08-12T19:00:43.482928: step 914, loss 0.0331176, acc 0.984375\n",
      "2016-08-12T19:00:43.530854: step 915, loss 0.0411119, acc 1\n",
      "2016-08-12T19:00:43.602636: step 916, loss 0.0477095, acc 1\n",
      "2016-08-12T19:00:43.675810: step 917, loss 0.0350249, acc 1\n",
      "2016-08-12T19:00:43.747603: step 918, loss 0.0517708, acc 1\n",
      "2016-08-12T19:00:43.821002: step 919, loss 0.0391132, acc 0.984375\n",
      "2016-08-12T19:00:43.892962: step 920, loss 0.0808163, acc 0.96875\n",
      "2016-08-12T19:00:43.966162: step 921, loss 0.0541912, acc 1\n",
      "2016-08-12T19:00:44.042342: step 922, loss 0.0197651, acc 1\n",
      "2016-08-12T19:00:44.114134: step 923, loss 0.0600519, acc 0.984375\n",
      "2016-08-12T19:00:44.188467: step 924, loss 0.0820116, acc 0.953125\n",
      "2016-08-12T19:00:44.261385: step 925, loss 0.0300212, acc 1\n",
      "2016-08-12T19:00:44.331623: step 926, loss 0.0258916, acc 1\n",
      "2016-08-12T19:00:44.405855: step 927, loss 0.0328175, acc 1\n",
      "2016-08-12T19:00:44.478605: step 928, loss 0.0390805, acc 1\n",
      "2016-08-12T19:00:44.551835: step 929, loss 0.0531874, acc 0.984375\n",
      "2016-08-12T19:00:44.599952: step 930, loss 0.0419714, acc 1\n",
      "2016-08-12T19:00:44.672877: step 931, loss 0.0321893, acc 1\n",
      "2016-08-12T19:00:44.747505: step 932, loss 0.0529534, acc 0.984375\n",
      "2016-08-12T19:00:44.821356: step 933, loss 0.0559287, acc 1\n",
      "2016-08-12T19:00:44.891005: step 934, loss 0.059962, acc 0.984375\n",
      "2016-08-12T19:00:44.964806: step 935, loss 0.0469868, acc 1\n",
      "2016-08-12T19:00:45.037914: step 936, loss 0.0448102, acc 0.984375\n",
      "2016-08-12T19:00:45.111645: step 937, loss 0.0437116, acc 1\n",
      "2016-08-12T19:00:45.186626: step 938, loss 0.0274811, acc 1\n",
      "2016-08-12T19:00:45.262409: step 939, loss 0.0440367, acc 0.984375\n",
      "2016-08-12T19:00:45.338200: step 940, loss 0.069101, acc 0.984375\n",
      "2016-08-12T19:00:45.410761: step 941, loss 0.0724151, acc 0.984375\n",
      "2016-08-12T19:00:45.484451: step 942, loss 0.0799985, acc 0.984375\n",
      "2016-08-12T19:00:45.557685: step 943, loss 0.05473, acc 0.984375\n",
      "2016-08-12T19:00:45.630380: step 944, loss 0.0825872, acc 0.96875\n",
      "2016-08-12T19:00:45.681259: step 945, loss 0.0534509, acc 1\n",
      "2016-08-12T19:00:45.757154: step 946, loss 0.0258595, acc 1\n",
      "2016-08-12T19:00:45.829251: step 947, loss 0.0316032, acc 1\n",
      "2016-08-12T19:00:45.904020: step 948, loss 0.0562141, acc 1\n",
      "2016-08-12T19:00:45.977986: step 949, loss 0.0187924, acc 1\n",
      "2016-08-12T19:00:46.050246: step 950, loss 0.072965, acc 0.984375\n",
      "2016-08-12T19:00:46.121772: step 951, loss 0.0518944, acc 0.984375\n",
      "2016-08-12T19:00:46.194483: step 952, loss 0.0368346, acc 1\n",
      "2016-08-12T19:00:46.266768: step 953, loss 0.0392324, acc 1\n",
      "2016-08-12T19:00:46.341210: step 954, loss 0.0878324, acc 0.9375\n",
      "2016-08-12T19:00:46.414903: step 955, loss 0.0548592, acc 0.984375\n",
      "2016-08-12T19:00:46.487708: step 956, loss 0.0335124, acc 0.984375\n",
      "2016-08-12T19:00:46.562327: step 957, loss 0.0274985, acc 1\n",
      "2016-08-12T19:00:46.640449: step 958, loss 0.0860173, acc 0.984375\n",
      "2016-08-12T19:00:46.714131: step 959, loss 0.0526345, acc 0.984375\n",
      "2016-08-12T19:00:46.767154: step 960, loss 0.0976404, acc 0.942857\n",
      "2016-08-12T19:00:46.841322: step 961, loss 0.0602494, acc 0.984375\n",
      "2016-08-12T19:00:46.918217: step 962, loss 0.0275034, acc 1\n",
      "2016-08-12T19:00:46.990950: step 963, loss 0.0458318, acc 0.984375\n",
      "2016-08-12T19:00:47.063428: step 964, loss 0.0358103, acc 0.984375\n",
      "2016-08-12T19:00:47.135832: step 965, loss 0.046112, acc 0.984375\n",
      "2016-08-12T19:00:47.210967: step 966, loss 0.0367253, acc 1\n",
      "2016-08-12T19:00:47.284542: step 967, loss 0.0321944, acc 1\n",
      "2016-08-12T19:00:47.357330: step 968, loss 0.0769594, acc 0.96875\n",
      "2016-08-12T19:00:47.431204: step 969, loss 0.0358241, acc 1\n",
      "2016-08-12T19:00:47.502757: step 970, loss 0.0385056, acc 1\n",
      "2016-08-12T19:00:47.575832: step 971, loss 0.0570075, acc 1\n",
      "2016-08-12T19:00:47.649683: step 972, loss 0.0612463, acc 0.984375\n",
      "2016-08-12T19:00:47.722257: step 973, loss 0.0307786, acc 1\n",
      "2016-08-12T19:00:47.795066: step 974, loss 0.0257219, acc 1\n",
      "2016-08-12T19:00:47.846909: step 975, loss 0.0313639, acc 1\n",
      "2016-08-12T19:00:47.922318: step 976, loss 0.0892964, acc 0.96875\n",
      "2016-08-12T19:00:47.993183: step 977, loss 0.0322122, acc 1\n",
      "2016-08-12T19:00:48.068299: step 978, loss 0.0461813, acc 1\n",
      "2016-08-12T19:00:48.141667: step 979, loss 0.0398392, acc 0.984375\n",
      "2016-08-12T19:00:48.214178: step 980, loss 0.0428903, acc 0.984375\n",
      "2016-08-12T19:00:48.287241: step 981, loss 0.0473837, acc 0.984375\n",
      "2016-08-12T19:00:48.361710: step 982, loss 0.0409963, acc 0.984375\n",
      "2016-08-12T19:00:48.436429: step 983, loss 0.0458376, acc 1\n",
      "2016-08-12T19:00:48.506850: step 984, loss 0.038017, acc 1\n",
      "2016-08-12T19:00:48.578702: step 985, loss 0.0326661, acc 1\n",
      "2016-08-12T19:00:48.653829: step 986, loss 0.0429931, acc 1\n",
      "2016-08-12T19:00:48.726659: step 987, loss 0.0498646, acc 0.984375\n",
      "2016-08-12T19:00:48.799338: step 988, loss 0.0488901, acc 1\n",
      "2016-08-12T19:00:48.871687: step 989, loss 0.0364838, acc 1\n",
      "2016-08-12T19:00:48.922777: step 990, loss 0.020608, acc 1\n",
      "2016-08-12T19:00:48.995232: step 991, loss 0.0414188, acc 1\n",
      "2016-08-12T19:00:49.066731: step 992, loss 0.0610702, acc 0.984375\n",
      "2016-08-12T19:00:49.138439: step 993, loss 0.0569794, acc 0.984375\n",
      "2016-08-12T19:00:49.211961: step 994, loss 0.0544937, acc 1\n",
      "2016-08-12T19:00:49.284030: step 995, loss 0.0331144, acc 1\n",
      "2016-08-12T19:00:49.355398: step 996, loss 0.0234355, acc 1\n",
      "2016-08-12T19:00:49.427362: step 997, loss 0.0271185, acc 1\n",
      "2016-08-12T19:00:49.501082: step 998, loss 0.0436187, acc 0.984375\n",
      "2016-08-12T19:00:49.574895: step 999, loss 0.0421193, acc 1\n",
      "2016-08-12T19:00:49.646965: step 1000, loss 0.0522092, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:49.958112: step 1000, loss 0.60223, acc 0.759\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1000\n",
      "\n",
      "2016-08-12T19:00:50.647064: step 1001, loss 0.0256184, acc 1\n",
      "2016-08-12T19:00:50.718325: step 1002, loss 0.0670021, acc 0.984375\n",
      "2016-08-12T19:00:50.793100: step 1003, loss 0.0294312, acc 1\n",
      "2016-08-12T19:00:50.864019: step 1004, loss 0.0416847, acc 1\n",
      "2016-08-12T19:00:50.915792: step 1005, loss 0.023431, acc 1\n",
      "2016-08-12T19:00:50.987938: step 1006, loss 0.0647946, acc 0.984375\n",
      "2016-08-12T19:00:51.060688: step 1007, loss 0.0302581, acc 1\n",
      "2016-08-12T19:00:51.133000: step 1008, loss 0.0540615, acc 0.984375\n",
      "2016-08-12T19:00:51.205245: step 1009, loss 0.0640645, acc 0.96875\n",
      "2016-08-12T19:00:51.278372: step 1010, loss 0.065835, acc 1\n",
      "2016-08-12T19:00:51.350349: step 1011, loss 0.0244772, acc 1\n",
      "2016-08-12T19:00:51.422128: step 1012, loss 0.04743, acc 1\n",
      "2016-08-12T19:00:51.494120: step 1013, loss 0.0215401, acc 1\n",
      "2016-08-12T19:00:51.566280: step 1014, loss 0.0633195, acc 0.984375\n",
      "2016-08-12T19:00:51.639044: step 1015, loss 0.0700926, acc 0.984375\n",
      "2016-08-12T19:00:51.711189: step 1016, loss 0.0480114, acc 0.984375\n",
      "2016-08-12T19:00:51.782589: step 1017, loss 0.0871066, acc 0.96875\n",
      "2016-08-12T19:00:51.855006: step 1018, loss 0.0462673, acc 1\n",
      "2016-08-12T19:00:51.926982: step 1019, loss 0.0397922, acc 1\n",
      "2016-08-12T19:00:51.978061: step 1020, loss 0.0459729, acc 0.971429\n",
      "2016-08-12T19:00:52.050896: step 1021, loss 0.054552, acc 0.984375\n",
      "2016-08-12T19:00:52.123448: step 1022, loss 0.0469007, acc 1\n",
      "2016-08-12T19:00:52.195517: step 1023, loss 0.0436676, acc 1\n",
      "2016-08-12T19:00:52.267083: step 1024, loss 0.0571062, acc 1\n",
      "2016-08-12T19:00:52.338794: step 1025, loss 0.0416338, acc 1\n",
      "2016-08-12T19:00:52.409769: step 1026, loss 0.0398086, acc 1\n",
      "2016-08-12T19:00:52.480457: step 1027, loss 0.0738826, acc 0.953125\n",
      "2016-08-12T19:00:52.552330: step 1028, loss 0.0371167, acc 0.984375\n",
      "2016-08-12T19:00:52.625690: step 1029, loss 0.0317051, acc 1\n",
      "2016-08-12T19:00:52.698226: step 1030, loss 0.0303401, acc 1\n",
      "2016-08-12T19:00:52.770787: step 1031, loss 0.0468211, acc 0.984375\n",
      "2016-08-12T19:00:52.842894: step 1032, loss 0.0444634, acc 0.984375\n",
      "2016-08-12T19:00:52.916244: step 1033, loss 0.0409458, acc 1\n",
      "2016-08-12T19:00:52.988041: step 1034, loss 0.0581862, acc 0.96875\n",
      "2016-08-12T19:00:53.039963: step 1035, loss 0.0827965, acc 0.971429\n",
      "2016-08-12T19:00:53.113518: step 1036, loss 0.0228405, acc 1\n",
      "2016-08-12T19:00:53.186202: step 1037, loss 0.0247207, acc 1\n",
      "2016-08-12T19:00:53.258949: step 1038, loss 0.0193451, acc 1\n",
      "2016-08-12T19:00:53.331078: step 1039, loss 0.0409493, acc 1\n",
      "2016-08-12T19:00:53.403708: step 1040, loss 0.0196836, acc 1\n",
      "2016-08-12T19:00:53.476219: step 1041, loss 0.0380405, acc 0.984375\n",
      "2016-08-12T19:00:53.546988: step 1042, loss 0.0497633, acc 0.96875\n",
      "2016-08-12T19:00:53.619571: step 1043, loss 0.0625116, acc 0.984375\n",
      "2016-08-12T19:00:53.693504: step 1044, loss 0.0463359, acc 0.984375\n",
      "2016-08-12T19:00:53.765523: step 1045, loss 0.0324829, acc 1\n",
      "2016-08-12T19:00:53.840002: step 1046, loss 0.0495277, acc 1\n",
      "2016-08-12T19:00:53.913231: step 1047, loss 0.0586222, acc 0.96875\n",
      "2016-08-12T19:00:53.986266: step 1048, loss 0.0391448, acc 0.984375\n",
      "2016-08-12T19:00:54.058502: step 1049, loss 0.0348537, acc 1\n",
      "2016-08-12T19:00:54.109928: step 1050, loss 0.0292676, acc 1\n",
      "2016-08-12T19:00:54.182186: step 1051, loss 0.0298847, acc 1\n",
      "2016-08-12T19:00:54.253976: step 1052, loss 0.0151811, acc 1\n",
      "2016-08-12T19:00:54.327256: step 1053, loss 0.0263581, acc 1\n",
      "2016-08-12T19:00:54.399388: step 1054, loss 0.0462609, acc 0.984375\n",
      "2016-08-12T19:00:54.471599: step 1055, loss 0.0475217, acc 1\n",
      "2016-08-12T19:00:54.543781: step 1056, loss 0.0451221, acc 1\n",
      "2016-08-12T19:00:54.618509: step 1057, loss 0.015682, acc 1\n",
      "2016-08-12T19:00:54.692624: step 1058, loss 0.0610274, acc 0.984375\n",
      "2016-08-12T19:00:54.766002: step 1059, loss 0.0232315, acc 1\n",
      "2016-08-12T19:00:54.840377: step 1060, loss 0.0430094, acc 1\n",
      "2016-08-12T19:00:54.913788: step 1061, loss 0.0479395, acc 1\n",
      "2016-08-12T19:00:54.985935: step 1062, loss 0.0600108, acc 0.984375\n",
      "2016-08-12T19:00:55.061180: step 1063, loss 0.0228132, acc 1\n",
      "2016-08-12T19:00:55.134153: step 1064, loss 0.0205975, acc 1\n",
      "2016-08-12T19:00:55.183349: step 1065, loss 0.0280764, acc 1\n",
      "2016-08-12T19:00:55.255785: step 1066, loss 0.0632535, acc 0.96875\n",
      "2016-08-12T19:00:55.328745: step 1067, loss 0.0339137, acc 1\n",
      "2016-08-12T19:00:55.402570: step 1068, loss 0.016711, acc 1\n",
      "2016-08-12T19:00:55.473821: step 1069, loss 0.0270509, acc 1\n",
      "2016-08-12T19:00:55.545868: step 1070, loss 0.0939714, acc 0.96875\n",
      "2016-08-12T19:00:55.619980: step 1071, loss 0.0518529, acc 0.96875\n",
      "2016-08-12T19:00:55.693496: step 1072, loss 0.0290883, acc 1\n",
      "2016-08-12T19:00:55.765967: step 1073, loss 0.0395441, acc 1\n",
      "2016-08-12T19:00:55.843296: step 1074, loss 0.0414025, acc 0.984375\n",
      "2016-08-12T19:00:55.920431: step 1075, loss 0.0283298, acc 1\n",
      "2016-08-12T19:00:55.996958: step 1076, loss 0.0368791, acc 0.984375\n",
      "2016-08-12T19:00:56.074700: step 1077, loss 0.0251318, acc 1\n",
      "2016-08-12T19:00:56.151595: step 1078, loss 0.0391926, acc 0.984375\n",
      "2016-08-12T19:00:56.227702: step 1079, loss 0.0464295, acc 1\n",
      "2016-08-12T19:00:56.279119: step 1080, loss 0.0226348, acc 1\n",
      "2016-08-12T19:00:56.350686: step 1081, loss 0.0187097, acc 1\n",
      "2016-08-12T19:00:56.424798: step 1082, loss 0.0558365, acc 0.984375\n",
      "2016-08-12T19:00:56.496838: step 1083, loss 0.0247879, acc 1\n",
      "2016-08-12T19:00:56.569526: step 1084, loss 0.0469801, acc 0.984375\n",
      "2016-08-12T19:00:56.642577: step 1085, loss 0.0487719, acc 0.984375\n",
      "2016-08-12T19:00:56.718412: step 1086, loss 0.0309921, acc 1\n",
      "2016-08-12T19:00:56.789884: step 1087, loss 0.0612921, acc 0.96875\n",
      "2016-08-12T19:00:56.860246: step 1088, loss 0.0385697, acc 1\n",
      "2016-08-12T19:00:56.932839: step 1089, loss 0.0416033, acc 1\n",
      "2016-08-12T19:00:57.006192: step 1090, loss 0.0593913, acc 0.96875\n",
      "2016-08-12T19:00:57.079003: step 1091, loss 0.068098, acc 0.984375\n",
      "2016-08-12T19:00:57.150580: step 1092, loss 0.054607, acc 0.984375\n",
      "2016-08-12T19:00:57.222605: step 1093, loss 0.0291554, acc 1\n",
      "2016-08-12T19:00:57.295468: step 1094, loss 0.0313588, acc 1\n",
      "2016-08-12T19:00:57.347748: step 1095, loss 0.0631276, acc 1\n",
      "2016-08-12T19:00:57.420913: step 1096, loss 0.0354586, acc 1\n",
      "2016-08-12T19:00:57.494483: step 1097, loss 0.0259374, acc 1\n",
      "2016-08-12T19:00:57.566583: step 1098, loss 0.044427, acc 0.984375\n",
      "2016-08-12T19:00:57.639296: step 1099, loss 0.0455455, acc 1\n",
      "2016-08-12T19:00:57.710989: step 1100, loss 0.0538032, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:00:58.019585: step 1100, loss 0.625144, acc 0.756\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1100\n",
      "\n",
      "2016-08-12T19:00:58.618910: step 1101, loss 0.0502478, acc 0.984375\n",
      "2016-08-12T19:00:58.691912: step 1102, loss 0.0569674, acc 0.984375\n",
      "2016-08-12T19:00:58.764599: step 1103, loss 0.0298112, acc 1\n",
      "2016-08-12T19:00:58.838358: step 1104, loss 0.0142975, acc 1\n",
      "2016-08-12T19:00:58.909243: step 1105, loss 0.0479595, acc 0.984375\n",
      "2016-08-12T19:00:58.981675: step 1106, loss 0.0188513, acc 1\n",
      "2016-08-12T19:00:59.053570: step 1107, loss 0.0170411, acc 1\n",
      "2016-08-12T19:00:59.127150: step 1108, loss 0.0297933, acc 1\n",
      "2016-08-12T19:00:59.199986: step 1109, loss 0.0560702, acc 0.984375\n",
      "2016-08-12T19:00:59.249796: step 1110, loss 0.0267778, acc 1\n",
      "2016-08-12T19:00:59.323622: step 1111, loss 0.0241732, acc 0.984375\n",
      "2016-08-12T19:00:59.397380: step 1112, loss 0.0260156, acc 1\n",
      "2016-08-12T19:00:59.470099: step 1113, loss 0.0410437, acc 1\n",
      "2016-08-12T19:00:59.541755: step 1114, loss 0.020878, acc 1\n",
      "2016-08-12T19:00:59.614422: step 1115, loss 0.0270313, acc 0.984375\n",
      "2016-08-12T19:00:59.687780: step 1116, loss 0.0579524, acc 0.984375\n",
      "2016-08-12T19:00:59.761286: step 1117, loss 0.0388173, acc 1\n",
      "2016-08-12T19:00:59.833101: step 1118, loss 0.0241977, acc 1\n",
      "2016-08-12T19:00:59.905430: step 1119, loss 0.040746, acc 0.984375\n",
      "2016-08-12T19:00:59.979049: step 1120, loss 0.0285639, acc 1\n",
      "2016-08-12T19:01:00.050239: step 1121, loss 0.0399317, acc 1\n",
      "2016-08-12T19:01:00.123652: step 1122, loss 0.0178251, acc 1\n",
      "2016-08-12T19:01:00.195885: step 1123, loss 0.0360303, acc 0.984375\n",
      "2016-08-12T19:01:00.268815: step 1124, loss 0.0558396, acc 0.984375\n",
      "2016-08-12T19:01:00.320557: step 1125, loss 0.0735361, acc 0.971429\n",
      "2016-08-12T19:01:00.393792: step 1126, loss 0.0344519, acc 0.984375\n",
      "2016-08-12T19:01:00.466154: step 1127, loss 0.0283751, acc 1\n",
      "2016-08-12T19:01:00.538724: step 1128, loss 0.0181854, acc 1\n",
      "2016-08-12T19:01:00.612032: step 1129, loss 0.0390808, acc 1\n",
      "2016-08-12T19:01:00.684350: step 1130, loss 0.0502652, acc 0.984375\n",
      "2016-08-12T19:01:00.757269: step 1131, loss 0.0367661, acc 1\n",
      "2016-08-12T19:01:00.829590: step 1132, loss 0.0365708, acc 0.984375\n",
      "2016-08-12T19:01:00.905463: step 1133, loss 0.0227166, acc 1\n",
      "2016-08-12T19:01:00.976512: step 1134, loss 0.0229178, acc 1\n",
      "2016-08-12T19:01:01.048154: step 1135, loss 0.0260099, acc 1\n",
      "2016-08-12T19:01:01.119639: step 1136, loss 0.0401966, acc 1\n",
      "2016-08-12T19:01:01.189730: step 1137, loss 0.0943929, acc 0.984375\n",
      "2016-08-12T19:01:01.260839: step 1138, loss 0.0379196, acc 1\n",
      "2016-08-12T19:01:01.333728: step 1139, loss 0.0765784, acc 0.953125\n",
      "2016-08-12T19:01:01.384733: step 1140, loss 0.0219589, acc 1\n",
      "2016-08-12T19:01:01.456676: step 1141, loss 0.0313329, acc 1\n",
      "2016-08-12T19:01:01.529974: step 1142, loss 0.0507987, acc 0.984375\n",
      "2016-08-12T19:01:01.602485: step 1143, loss 0.0294442, acc 1\n",
      "2016-08-12T19:01:01.675340: step 1144, loss 0.0776138, acc 0.96875\n",
      "2016-08-12T19:01:01.747760: step 1145, loss 0.0474184, acc 0.984375\n",
      "2016-08-12T19:01:01.820714: step 1146, loss 0.032999, acc 1\n",
      "2016-08-12T19:01:01.892717: step 1147, loss 0.0596732, acc 0.953125\n",
      "2016-08-12T19:01:01.967470: step 1148, loss 0.0341677, acc 1\n",
      "2016-08-12T19:01:02.040130: step 1149, loss 0.0258388, acc 1\n",
      "2016-08-12T19:01:02.109853: step 1150, loss 0.0293578, acc 1\n",
      "2016-08-12T19:01:02.184064: step 1151, loss 0.0273321, acc 1\n",
      "2016-08-12T19:01:02.257412: step 1152, loss 0.0333871, acc 1\n",
      "2016-08-12T19:01:02.331144: step 1153, loss 0.0335815, acc 1\n",
      "2016-08-12T19:01:02.404314: step 1154, loss 0.0480055, acc 1\n",
      "2016-08-12T19:01:02.455711: step 1155, loss 0.0364546, acc 1\n",
      "2016-08-12T19:01:02.528455: step 1156, loss 0.0455334, acc 0.984375\n",
      "2016-08-12T19:01:02.600231: step 1157, loss 0.0408039, acc 0.984375\n",
      "2016-08-12T19:01:02.673647: step 1158, loss 0.0740951, acc 0.984375\n",
      "2016-08-12T19:01:02.746544: step 1159, loss 0.0427551, acc 0.96875\n",
      "2016-08-12T19:01:02.819345: step 1160, loss 0.0395508, acc 0.984375\n",
      "2016-08-12T19:01:02.891921: step 1161, loss 0.0524953, acc 1\n",
      "2016-08-12T19:01:02.968595: step 1162, loss 0.0442593, acc 1\n",
      "2016-08-12T19:01:03.045711: step 1163, loss 0.0348656, acc 1\n",
      "2016-08-12T19:01:03.121265: step 1164, loss 0.036091, acc 1\n",
      "2016-08-12T19:01:03.194799: step 1165, loss 0.0445746, acc 1\n",
      "2016-08-12T19:01:03.266129: step 1166, loss 0.0268023, acc 1\n",
      "2016-08-12T19:01:03.337840: step 1167, loss 0.0184784, acc 1\n",
      "2016-08-12T19:01:03.410763: step 1168, loss 0.0154526, acc 1\n",
      "2016-08-12T19:01:03.482952: step 1169, loss 0.0317574, acc 1\n",
      "2016-08-12T19:01:03.533467: step 1170, loss 0.0114975, acc 1\n",
      "2016-08-12T19:01:03.606934: step 1171, loss 0.0211087, acc 1\n",
      "2016-08-12T19:01:03.683616: step 1172, loss 0.100648, acc 0.96875\n",
      "2016-08-12T19:01:03.760335: step 1173, loss 0.0565733, acc 0.984375\n",
      "2016-08-12T19:01:03.837362: step 1174, loss 0.0372009, acc 0.984375\n",
      "2016-08-12T19:01:03.909643: step 1175, loss 0.0258403, acc 1\n",
      "2016-08-12T19:01:03.983330: step 1176, loss 0.0173388, acc 1\n",
      "2016-08-12T19:01:04.055898: step 1177, loss 0.0402678, acc 1\n",
      "2016-08-12T19:01:04.130504: step 1178, loss 0.0263686, acc 0.984375\n",
      "2016-08-12T19:01:04.206048: step 1179, loss 0.0349485, acc 0.984375\n",
      "2016-08-12T19:01:04.278560: step 1180, loss 0.0383677, acc 0.984375\n",
      "2016-08-12T19:01:04.351024: step 1181, loss 0.0252744, acc 1\n",
      "2016-08-12T19:01:04.422601: step 1182, loss 0.045601, acc 1\n",
      "2016-08-12T19:01:04.493968: step 1183, loss 0.0523742, acc 0.96875\n",
      "2016-08-12T19:01:04.567257: step 1184, loss 0.0278533, acc 1\n",
      "2016-08-12T19:01:04.619390: step 1185, loss 0.0450096, acc 1\n",
      "2016-08-12T19:01:04.692409: step 1186, loss 0.00961664, acc 1\n",
      "2016-08-12T19:01:04.765238: step 1187, loss 0.0353615, acc 1\n",
      "2016-08-12T19:01:04.836741: step 1188, loss 0.0365277, acc 0.984375\n",
      "2016-08-12T19:01:04.909452: step 1189, loss 0.0292062, acc 0.984375\n",
      "2016-08-12T19:01:04.980953: step 1190, loss 0.0359729, acc 0.984375\n",
      "2016-08-12T19:01:05.054573: step 1191, loss 0.0354087, acc 1\n",
      "2016-08-12T19:01:05.127439: step 1192, loss 0.0333848, acc 1\n",
      "2016-08-12T19:01:05.200710: step 1193, loss 0.0359765, acc 1\n",
      "2016-08-12T19:01:05.273254: step 1194, loss 0.0427331, acc 0.984375\n",
      "2016-08-12T19:01:05.345065: step 1195, loss 0.0631276, acc 0.984375\n",
      "2016-08-12T19:01:05.419042: step 1196, loss 0.0291261, acc 1\n",
      "2016-08-12T19:01:05.492709: step 1197, loss 0.0213451, acc 1\n",
      "2016-08-12T19:01:05.567010: step 1198, loss 0.0355171, acc 1\n",
      "2016-08-12T19:01:05.639051: step 1199, loss 0.0193036, acc 1\n",
      "2016-08-12T19:01:05.689994: step 1200, loss 0.0207418, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:01:05.998589: step 1200, loss 0.626306, acc 0.759\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1200\n",
      "\n",
      "2016-08-12T19:01:06.675402: step 1201, loss 0.0353979, acc 0.984375\n",
      "2016-08-12T19:01:06.748528: step 1202, loss 0.0187534, acc 1\n",
      "2016-08-12T19:01:06.820763: step 1203, loss 0.0377532, acc 1\n",
      "2016-08-12T19:01:06.894944: step 1204, loss 0.0611074, acc 0.984375\n",
      "2016-08-12T19:01:06.969331: step 1205, loss 0.0220809, acc 1\n",
      "2016-08-12T19:01:07.042804: step 1206, loss 0.0272858, acc 1\n",
      "2016-08-12T19:01:07.114402: step 1207, loss 0.036669, acc 0.984375\n",
      "2016-08-12T19:01:07.187759: step 1208, loss 0.0340518, acc 1\n",
      "2016-08-12T19:01:07.259599: step 1209, loss 0.0260225, acc 1\n",
      "2016-08-12T19:01:07.331463: step 1210, loss 0.0562117, acc 0.96875\n",
      "2016-08-12T19:01:07.404933: step 1211, loss 0.0158951, acc 1\n",
      "2016-08-12T19:01:07.477096: step 1212, loss 0.0403061, acc 0.984375\n",
      "2016-08-12T19:01:07.549891: step 1213, loss 0.0589829, acc 0.984375\n",
      "2016-08-12T19:01:07.621738: step 1214, loss 0.0323959, acc 1\n",
      "2016-08-12T19:01:07.672834: step 1215, loss 0.0616069, acc 0.971429\n",
      "2016-08-12T19:01:07.745621: step 1216, loss 0.0765606, acc 0.984375\n",
      "2016-08-12T19:01:07.818780: step 1217, loss 0.0567105, acc 0.96875\n",
      "2016-08-12T19:01:07.891676: step 1218, loss 0.0314507, acc 1\n",
      "2016-08-12T19:01:07.968933: step 1219, loss 0.0397635, acc 0.984375\n",
      "2016-08-12T19:01:08.046601: step 1220, loss 0.0271953, acc 1\n",
      "2016-08-12T19:01:08.120664: step 1221, loss 0.0333493, acc 1\n",
      "2016-08-12T19:01:08.192830: step 1222, loss 0.021049, acc 1\n",
      "2016-08-12T19:01:08.264665: step 1223, loss 0.0297301, acc 1\n",
      "2016-08-12T19:01:08.335965: step 1224, loss 0.0392225, acc 0.984375\n",
      "2016-08-12T19:01:08.408348: step 1225, loss 0.0155032, acc 1\n",
      "2016-08-12T19:01:08.480737: step 1226, loss 0.0509887, acc 0.984375\n",
      "2016-08-12T19:01:08.552540: step 1227, loss 0.0509285, acc 0.96875\n",
      "2016-08-12T19:01:08.625748: step 1228, loss 0.0386881, acc 1\n",
      "2016-08-12T19:01:08.701587: step 1229, loss 0.0278023, acc 1\n",
      "2016-08-12T19:01:08.756894: step 1230, loss 0.0454182, acc 0.971429\n",
      "2016-08-12T19:01:08.835586: step 1231, loss 0.0793937, acc 0.984375\n",
      "2016-08-12T19:01:08.908395: step 1232, loss 0.0236689, acc 1\n",
      "2016-08-12T19:01:08.985380: step 1233, loss 0.0234991, acc 1\n",
      "2016-08-12T19:01:09.057018: step 1234, loss 0.0261868, acc 1\n",
      "2016-08-12T19:01:09.133627: step 1235, loss 0.0221563, acc 1\n",
      "2016-08-12T19:01:09.206132: step 1236, loss 0.0156819, acc 1\n",
      "2016-08-12T19:01:09.273635: step 1237, loss 0.0400674, acc 0.984375\n",
      "2016-08-12T19:01:09.345644: step 1238, loss 0.0291156, acc 1\n",
      "2016-08-12T19:01:09.418129: step 1239, loss 0.0689428, acc 0.96875\n",
      "2016-08-12T19:01:09.491977: step 1240, loss 0.0351435, acc 0.984375\n",
      "2016-08-12T19:01:09.565018: step 1241, loss 0.053027, acc 1\n",
      "2016-08-12T19:01:09.636677: step 1242, loss 0.0759879, acc 0.984375\n",
      "2016-08-12T19:01:09.708962: step 1243, loss 0.0322784, acc 1\n",
      "2016-08-12T19:01:09.781343: step 1244, loss 0.0144717, acc 1\n",
      "2016-08-12T19:01:09.831110: step 1245, loss 0.0339131, acc 1\n",
      "2016-08-12T19:01:09.904292: step 1246, loss 0.0176191, acc 1\n",
      "2016-08-12T19:01:09.978329: step 1247, loss 0.0308918, acc 1\n",
      "2016-08-12T19:01:10.050606: step 1248, loss 0.0428967, acc 0.984375\n",
      "2016-08-12T19:01:10.126095: step 1249, loss 0.0419498, acc 0.984375\n",
      "2016-08-12T19:01:10.198558: step 1250, loss 0.043673, acc 0.984375\n",
      "2016-08-12T19:01:10.269734: step 1251, loss 0.0765953, acc 0.984375\n",
      "2016-08-12T19:01:10.342789: step 1252, loss 0.00774381, acc 1\n",
      "2016-08-12T19:01:10.414088: step 1253, loss 0.0211174, acc 1\n",
      "2016-08-12T19:01:10.486627: step 1254, loss 0.0179502, acc 1\n",
      "2016-08-12T19:01:10.558749: step 1255, loss 0.0334941, acc 1\n",
      "2016-08-12T19:01:10.631010: step 1256, loss 0.0334685, acc 1\n",
      "2016-08-12T19:01:10.703196: step 1257, loss 0.0385566, acc 0.984375\n",
      "2016-08-12T19:01:10.776274: step 1258, loss 0.0350265, acc 1\n",
      "2016-08-12T19:01:10.847466: step 1259, loss 0.0159218, acc 1\n",
      "2016-08-12T19:01:10.896738: step 1260, loss 0.0191805, acc 1\n",
      "2016-08-12T19:01:10.969923: step 1261, loss 0.0410072, acc 1\n",
      "2016-08-12T19:01:11.044259: step 1262, loss 0.043191, acc 0.984375\n",
      "2016-08-12T19:01:11.116929: step 1263, loss 0.0301573, acc 1\n",
      "2016-08-12T19:01:11.190692: step 1264, loss 0.019953, acc 1\n",
      "2016-08-12T19:01:11.264370: step 1265, loss 0.0194943, acc 1\n",
      "2016-08-12T19:01:11.337884: step 1266, loss 0.045703, acc 0.984375\n",
      "2016-08-12T19:01:11.411109: step 1267, loss 0.0397873, acc 1\n",
      "2016-08-12T19:01:11.483188: step 1268, loss 0.0538063, acc 0.96875\n",
      "2016-08-12T19:01:11.558095: step 1269, loss 0.0299693, acc 1\n",
      "2016-08-12T19:01:11.632199: step 1270, loss 0.0314943, acc 0.984375\n",
      "2016-08-12T19:01:11.703650: step 1271, loss 0.0588172, acc 0.984375\n",
      "2016-08-12T19:01:11.778172: step 1272, loss 0.0311416, acc 0.984375\n",
      "2016-08-12T19:01:11.850251: step 1273, loss 0.0419285, acc 1\n",
      "2016-08-12T19:01:11.924785: step 1274, loss 0.0337859, acc 0.984375\n",
      "2016-08-12T19:01:11.974200: step 1275, loss 0.0541191, acc 0.971429\n",
      "2016-08-12T19:01:12.046441: step 1276, loss 0.0298108, acc 1\n",
      "2016-08-12T19:01:12.118307: step 1277, loss 0.0323228, acc 1\n",
      "2016-08-12T19:01:12.191386: step 1278, loss 0.0342621, acc 0.984375\n",
      "2016-08-12T19:01:12.265500: step 1279, loss 0.0281798, acc 1\n",
      "2016-08-12T19:01:12.337931: step 1280, loss 0.021776, acc 1\n",
      "2016-08-12T19:01:12.411232: step 1281, loss 0.018396, acc 1\n",
      "2016-08-12T19:01:12.485206: step 1282, loss 0.0291791, acc 1\n",
      "2016-08-12T19:01:12.558607: step 1283, loss 0.0460945, acc 0.96875\n",
      "2016-08-12T19:01:12.633415: step 1284, loss 0.0253226, acc 0.984375\n",
      "2016-08-12T19:01:12.706639: step 1285, loss 0.0282332, acc 1\n",
      "2016-08-12T19:01:12.779122: step 1286, loss 0.0235723, acc 1\n",
      "2016-08-12T19:01:12.851838: step 1287, loss 0.117669, acc 0.96875\n",
      "2016-08-12T19:01:12.924526: step 1288, loss 0.024954, acc 1\n",
      "2016-08-12T19:01:12.996187: step 1289, loss 0.0494027, acc 0.984375\n",
      "2016-08-12T19:01:13.046538: step 1290, loss 0.0157747, acc 1\n",
      "2016-08-12T19:01:13.118766: step 1291, loss 0.0504395, acc 1\n",
      "2016-08-12T19:01:13.192388: step 1292, loss 0.0354392, acc 0.984375\n",
      "2016-08-12T19:01:13.265809: step 1293, loss 0.101956, acc 0.96875\n",
      "2016-08-12T19:01:13.337815: step 1294, loss 0.0569259, acc 0.984375\n",
      "2016-08-12T19:01:13.411005: step 1295, loss 0.0128235, acc 1\n",
      "2016-08-12T19:01:13.484222: step 1296, loss 0.0238384, acc 1\n",
      "2016-08-12T19:01:13.557345: step 1297, loss 0.0217697, acc 1\n",
      "2016-08-12T19:01:13.630489: step 1298, loss 0.0396913, acc 1\n",
      "2016-08-12T19:01:13.703360: step 1299, loss 0.0381227, acc 1\n",
      "2016-08-12T19:01:13.774829: step 1300, loss 0.036649, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:01:14.082098: step 1300, loss 0.670562, acc 0.757\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1300\n",
      "\n",
      "2016-08-12T19:01:14.741752: step 1301, loss 0.0465941, acc 1\n",
      "2016-08-12T19:01:14.815202: step 1302, loss 0.0190999, acc 1\n",
      "2016-08-12T19:01:14.888435: step 1303, loss 0.0322644, acc 1\n",
      "2016-08-12T19:01:14.961928: step 1304, loss 0.0290963, acc 0.984375\n",
      "2016-08-12T19:01:15.014047: step 1305, loss 0.0774347, acc 0.942857\n",
      "2016-08-12T19:01:15.086004: step 1306, loss 0.0278929, acc 1\n",
      "2016-08-12T19:01:15.159166: step 1307, loss 0.0139579, acc 1\n",
      "2016-08-12T19:01:15.228682: step 1308, loss 0.0336869, acc 0.984375\n",
      "2016-08-12T19:01:15.302225: step 1309, loss 0.0195772, acc 1\n",
      "2016-08-12T19:01:15.373509: step 1310, loss 0.036199, acc 1\n",
      "2016-08-12T19:01:15.446031: step 1311, loss 0.0505483, acc 0.984375\n",
      "2016-08-12T19:01:15.518597: step 1312, loss 0.109503, acc 0.96875\n",
      "2016-08-12T19:01:15.590784: step 1313, loss 0.0156396, acc 1\n",
      "2016-08-12T19:01:15.663319: step 1314, loss 0.0275543, acc 0.984375\n",
      "2016-08-12T19:01:15.736513: step 1315, loss 0.0281501, acc 1\n",
      "2016-08-12T19:01:15.808798: step 1316, loss 0.0279983, acc 1\n",
      "2016-08-12T19:01:15.882993: step 1317, loss 0.0712939, acc 0.953125\n",
      "2016-08-12T19:01:15.955972: step 1318, loss 0.0106168, acc 1\n",
      "2016-08-12T19:01:16.028599: step 1319, loss 0.0446505, acc 0.984375\n",
      "2016-08-12T19:01:16.082128: step 1320, loss 0.0246923, acc 1\n",
      "2016-08-12T19:01:16.154679: step 1321, loss 0.0175882, acc 1\n",
      "2016-08-12T19:01:16.228091: step 1322, loss 0.032568, acc 1\n",
      "2016-08-12T19:01:16.299098: step 1323, loss 0.0375508, acc 1\n",
      "2016-08-12T19:01:16.371863: step 1324, loss 0.0348163, acc 1\n",
      "2016-08-12T19:01:16.443941: step 1325, loss 0.0460724, acc 0.984375\n",
      "2016-08-12T19:01:16.517138: step 1326, loss 0.0767029, acc 0.96875\n",
      "2016-08-12T19:01:16.588907: step 1327, loss 0.0284629, acc 1\n",
      "2016-08-12T19:01:16.662186: step 1328, loss 0.0873416, acc 0.953125\n",
      "2016-08-12T19:01:16.735030: step 1329, loss 0.0155693, acc 1\n",
      "2016-08-12T19:01:16.807670: step 1330, loss 0.0311684, acc 0.984375\n",
      "2016-08-12T19:01:16.879374: step 1331, loss 0.0426018, acc 1\n",
      "2016-08-12T19:01:16.951374: step 1332, loss 0.0295389, acc 0.984375\n",
      "2016-08-12T19:01:17.024420: step 1333, loss 0.0316594, acc 1\n",
      "2016-08-12T19:01:17.095942: step 1334, loss 0.0451772, acc 0.984375\n",
      "2016-08-12T19:01:17.146714: step 1335, loss 0.0146044, acc 1\n",
      "2016-08-12T19:01:17.219630: step 1336, loss 0.0407142, acc 1\n",
      "2016-08-12T19:01:17.291821: step 1337, loss 0.0522763, acc 0.96875\n",
      "2016-08-12T19:01:17.366022: step 1338, loss 0.0212535, acc 1\n",
      "2016-08-12T19:01:17.437509: step 1339, loss 0.0279633, acc 1\n",
      "2016-08-12T19:01:17.509796: step 1340, loss 0.0297272, acc 1\n",
      "2016-08-12T19:01:17.582691: step 1341, loss 0.0274899, acc 1\n",
      "2016-08-12T19:01:17.655954: step 1342, loss 0.0740692, acc 0.984375\n",
      "2016-08-12T19:01:17.729568: step 1343, loss 0.0549466, acc 0.96875\n",
      "2016-08-12T19:01:17.801785: step 1344, loss 0.0246275, acc 1\n",
      "2016-08-12T19:01:17.875159: step 1345, loss 0.0330465, acc 1\n",
      "2016-08-12T19:01:17.947727: step 1346, loss 0.0209972, acc 1\n",
      "2016-08-12T19:01:18.021339: step 1347, loss 0.0632646, acc 0.984375\n",
      "2016-08-12T19:01:18.093987: step 1348, loss 0.0351377, acc 1\n",
      "2016-08-12T19:01:18.165754: step 1349, loss 0.0515052, acc 0.984375\n",
      "2016-08-12T19:01:18.215573: step 1350, loss 0.0203606, acc 1\n",
      "2016-08-12T19:01:18.287004: step 1351, loss 0.0286716, acc 0.984375\n",
      "2016-08-12T19:01:18.358880: step 1352, loss 0.0509407, acc 0.984375\n",
      "2016-08-12T19:01:18.431466: step 1353, loss 0.0201193, acc 1\n",
      "2016-08-12T19:01:18.504626: step 1354, loss 0.037102, acc 1\n",
      "2016-08-12T19:01:18.578601: step 1355, loss 0.0371471, acc 0.984375\n",
      "2016-08-12T19:01:18.653518: step 1356, loss 0.0301367, acc 1\n",
      "2016-08-12T19:01:18.725130: step 1357, loss 0.0362435, acc 0.984375\n",
      "2016-08-12T19:01:18.798875: step 1358, loss 0.0292196, acc 1\n",
      "2016-08-12T19:01:18.872384: step 1359, loss 0.0294714, acc 1\n",
      "2016-08-12T19:01:18.944704: step 1360, loss 0.0230634, acc 1\n",
      "2016-08-12T19:01:19.016310: step 1361, loss 0.0181528, acc 1\n",
      "2016-08-12T19:01:19.089874: step 1362, loss 0.0266987, acc 1\n",
      "2016-08-12T19:01:19.161607: step 1363, loss 0.0351044, acc 0.984375\n",
      "2016-08-12T19:01:19.234638: step 1364, loss 0.0205097, acc 1\n",
      "2016-08-12T19:01:19.284671: step 1365, loss 0.0471154, acc 0.971429\n",
      "2016-08-12T19:01:19.358108: step 1366, loss 0.0501437, acc 1\n",
      "2016-08-12T19:01:19.430666: step 1367, loss 0.033693, acc 1\n",
      "2016-08-12T19:01:19.503873: step 1368, loss 0.0430899, acc 0.984375\n",
      "2016-08-12T19:01:19.576223: step 1369, loss 0.0266, acc 1\n",
      "2016-08-12T19:01:19.649351: step 1370, loss 0.0280998, acc 1\n",
      "2016-08-12T19:01:19.723765: step 1371, loss 0.0177565, acc 1\n",
      "2016-08-12T19:01:19.796598: step 1372, loss 0.0215884, acc 1\n",
      "2016-08-12T19:01:19.870024: step 1373, loss 0.0420294, acc 0.96875\n",
      "2016-08-12T19:01:19.942325: step 1374, loss 0.0210015, acc 1\n",
      "2016-08-12T19:01:20.014783: step 1375, loss 0.0223937, acc 1\n",
      "2016-08-12T19:01:20.087500: step 1376, loss 0.0195105, acc 1\n",
      "2016-08-12T19:01:20.160408: step 1377, loss 0.0175465, acc 1\n",
      "2016-08-12T19:01:20.235817: step 1378, loss 0.00957128, acc 1\n",
      "2016-08-12T19:01:20.308721: step 1379, loss 0.031321, acc 1\n",
      "2016-08-12T19:01:20.359272: step 1380, loss 0.0208719, acc 1\n",
      "2016-08-12T19:01:20.430949: step 1381, loss 0.0257521, acc 1\n",
      "2016-08-12T19:01:20.502314: step 1382, loss 0.0171651, acc 1\n",
      "2016-08-12T19:01:20.574834: step 1383, loss 0.0274912, acc 0.984375\n",
      "2016-08-12T19:01:20.646899: step 1384, loss 0.0180996, acc 1\n",
      "2016-08-12T19:01:20.719336: step 1385, loss 0.0121195, acc 1\n",
      "2016-08-12T19:01:20.791009: step 1386, loss 0.028149, acc 1\n",
      "2016-08-12T19:01:20.863349: step 1387, loss 0.0434173, acc 1\n",
      "2016-08-12T19:01:20.936751: step 1388, loss 0.0229467, acc 1\n",
      "2016-08-12T19:01:21.009674: step 1389, loss 0.0491916, acc 1\n",
      "2016-08-12T19:01:21.083865: step 1390, loss 0.03797, acc 0.984375\n",
      "2016-08-12T19:01:21.156512: step 1391, loss 0.0189055, acc 1\n",
      "2016-08-12T19:01:21.229375: step 1392, loss 0.103739, acc 0.96875\n",
      "2016-08-12T19:01:21.301623: step 1393, loss 0.0118867, acc 1\n",
      "2016-08-12T19:01:21.374846: step 1394, loss 0.0378447, acc 0.984375\n",
      "2016-08-12T19:01:21.422488: step 1395, loss 0.0812713, acc 0.971429\n",
      "2016-08-12T19:01:21.496461: step 1396, loss 0.019657, acc 1\n",
      "2016-08-12T19:01:21.569197: step 1397, loss 0.0387367, acc 0.984375\n",
      "2016-08-12T19:01:21.641752: step 1398, loss 0.0176124, acc 1\n",
      "2016-08-12T19:01:21.712702: step 1399, loss 0.0230382, acc 1\n",
      "2016-08-12T19:01:21.784168: step 1400, loss 0.037132, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:01:22.093474: step 1400, loss 0.669683, acc 0.751\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1400\n",
      "\n",
      "2016-08-12T19:01:22.757344: step 1401, loss 0.017931, acc 0.984375\n",
      "2016-08-12T19:01:22.829167: step 1402, loss 0.00644243, acc 1\n",
      "2016-08-12T19:01:22.901805: step 1403, loss 0.0390853, acc 0.984375\n",
      "2016-08-12T19:01:22.974098: step 1404, loss 0.0218203, acc 1\n",
      "2016-08-12T19:01:23.045855: step 1405, loss 0.0193185, acc 1\n",
      "2016-08-12T19:01:23.119503: step 1406, loss 0.0370344, acc 0.984375\n",
      "2016-08-12T19:01:23.193540: step 1407, loss 0.0495751, acc 0.984375\n",
      "2016-08-12T19:01:23.265729: step 1408, loss 0.0258895, acc 1\n",
      "2016-08-12T19:01:23.338521: step 1409, loss 0.021967, acc 1\n",
      "2016-08-12T19:01:23.388805: step 1410, loss 0.0130469, acc 1\n",
      "2016-08-12T19:01:23.461650: step 1411, loss 0.0392264, acc 0.984375\n",
      "2016-08-12T19:01:23.535886: step 1412, loss 0.0184011, acc 1\n",
      "2016-08-12T19:01:23.607509: step 1413, loss 0.0228643, acc 1\n",
      "2016-08-12T19:01:23.679380: step 1414, loss 0.0430671, acc 1\n",
      "2016-08-12T19:01:23.751007: step 1415, loss 0.0164502, acc 1\n",
      "2016-08-12T19:01:23.823721: step 1416, loss 0.0225894, acc 1\n",
      "2016-08-12T19:01:23.896073: step 1417, loss 0.028313, acc 0.984375\n",
      "2016-08-12T19:01:23.970957: step 1418, loss 0.0081202, acc 1\n",
      "2016-08-12T19:01:24.043106: step 1419, loss 0.0247409, acc 0.984375\n",
      "2016-08-12T19:01:24.116564: step 1420, loss 0.0267223, acc 1\n",
      "2016-08-12T19:01:24.188460: step 1421, loss 0.0252593, acc 1\n",
      "2016-08-12T19:01:24.262249: step 1422, loss 0.0138499, acc 1\n",
      "2016-08-12T19:01:24.334636: step 1423, loss 0.0274563, acc 1\n",
      "2016-08-12T19:01:24.406326: step 1424, loss 0.0275466, acc 1\n",
      "2016-08-12T19:01:24.456004: step 1425, loss 0.0324484, acc 1\n",
      "2016-08-12T19:01:24.529538: step 1426, loss 0.00985635, acc 1\n",
      "2016-08-12T19:01:24.602873: step 1427, loss 0.024919, acc 1\n",
      "2016-08-12T19:01:24.674659: step 1428, loss 0.0268785, acc 0.984375\n",
      "2016-08-12T19:01:24.750200: step 1429, loss 0.021924, acc 1\n",
      "2016-08-12T19:01:24.822357: step 1430, loss 0.0122734, acc 1\n",
      "2016-08-12T19:01:24.894158: step 1431, loss 0.0252986, acc 1\n",
      "2016-08-12T19:01:24.966551: step 1432, loss 0.0167408, acc 1\n",
      "2016-08-12T19:01:25.039838: step 1433, loss 0.0135242, acc 1\n",
      "2016-08-12T19:01:25.112675: step 1434, loss 0.0224719, acc 1\n",
      "2016-08-12T19:01:25.184777: step 1435, loss 0.0548952, acc 0.96875\n",
      "2016-08-12T19:01:25.260049: step 1436, loss 0.0302034, acc 1\n",
      "2016-08-12T19:01:25.333337: step 1437, loss 0.0296224, acc 1\n",
      "2016-08-12T19:01:25.405219: step 1438, loss 0.0361337, acc 0.984375\n",
      "2016-08-12T19:01:25.479141: step 1439, loss 0.0125388, acc 1\n",
      "2016-08-12T19:01:25.531546: step 1440, loss 0.0441461, acc 0.971429\n",
      "2016-08-12T19:01:25.606759: step 1441, loss 0.0550015, acc 0.984375\n",
      "2016-08-12T19:01:25.681538: step 1442, loss 0.0667987, acc 0.96875\n",
      "2016-08-12T19:01:25.754491: step 1443, loss 0.0152988, acc 1\n",
      "2016-08-12T19:01:25.827274: step 1444, loss 0.025746, acc 1\n",
      "2016-08-12T19:01:25.897566: step 1445, loss 0.0772303, acc 0.953125\n",
      "2016-08-12T19:01:25.969290: step 1446, loss 0.0205982, acc 1\n",
      "2016-08-12T19:01:26.042239: step 1447, loss 0.0457885, acc 1\n",
      "2016-08-12T19:01:26.118967: step 1448, loss 0.0517577, acc 0.984375\n",
      "2016-08-12T19:01:26.192152: step 1449, loss 0.0518161, acc 0.984375\n",
      "2016-08-12T19:01:26.266623: step 1450, loss 0.0388072, acc 0.984375\n",
      "2016-08-12T19:01:26.340603: step 1451, loss 0.0147951, acc 1\n",
      "2016-08-12T19:01:26.413242: step 1452, loss 0.0294802, acc 0.984375\n",
      "2016-08-12T19:01:26.489391: step 1453, loss 0.0494442, acc 0.96875\n",
      "2016-08-12T19:01:26.560765: step 1454, loss 0.0202146, acc 1\n",
      "2016-08-12T19:01:26.609683: step 1455, loss 0.0271773, acc 1\n",
      "2016-08-12T19:01:26.682040: step 1456, loss 0.0428471, acc 1\n",
      "2016-08-12T19:01:26.754224: step 1457, loss 0.0241964, acc 1\n",
      "2016-08-12T19:01:26.827473: step 1458, loss 0.0103212, acc 1\n",
      "2016-08-12T19:01:26.900837: step 1459, loss 0.0419887, acc 0.984375\n",
      "2016-08-12T19:01:26.974472: step 1460, loss 0.0345838, acc 0.984375\n",
      "2016-08-12T19:01:27.047048: step 1461, loss 0.025886, acc 1\n",
      "2016-08-12T19:01:27.120072: step 1462, loss 0.0285954, acc 0.984375\n",
      "2016-08-12T19:01:27.193987: step 1463, loss 0.0399382, acc 0.984375\n",
      "2016-08-12T19:01:27.267172: step 1464, loss 0.0467655, acc 1\n",
      "2016-08-12T19:01:27.340078: step 1465, loss 0.0287613, acc 1\n",
      "2016-08-12T19:01:27.413565: step 1466, loss 0.0472396, acc 1\n",
      "2016-08-12T19:01:27.486314: step 1467, loss 0.011003, acc 1\n",
      "2016-08-12T19:01:27.559370: step 1468, loss 0.0778712, acc 0.96875\n",
      "2016-08-12T19:01:27.633111: step 1469, loss 0.0233046, acc 1\n",
      "2016-08-12T19:01:27.682505: step 1470, loss 0.0267122, acc 1\n",
      "2016-08-12T19:01:27.756635: step 1471, loss 0.0344441, acc 0.984375\n",
      "2016-08-12T19:01:27.829701: step 1472, loss 0.029085, acc 1\n",
      "2016-08-12T19:01:27.902784: step 1473, loss 0.0233527, acc 1\n",
      "2016-08-12T19:01:27.974941: step 1474, loss 0.0251725, acc 1\n",
      "2016-08-12T19:01:28.048852: step 1475, loss 0.0534417, acc 1\n",
      "2016-08-12T19:01:28.121530: step 1476, loss 0.0319247, acc 0.984375\n",
      "2016-08-12T19:01:28.195271: step 1477, loss 0.0231176, acc 0.984375\n",
      "2016-08-12T19:01:28.267044: step 1478, loss 0.0263114, acc 1\n",
      "2016-08-12T19:01:28.339225: step 1479, loss 0.0434211, acc 0.984375\n",
      "2016-08-12T19:01:28.413350: step 1480, loss 0.0246596, acc 0.984375\n",
      "2016-08-12T19:01:28.485455: step 1481, loss 0.0170869, acc 1\n",
      "2016-08-12T19:01:28.558745: step 1482, loss 0.0118712, acc 1\n",
      "2016-08-12T19:01:28.630891: step 1483, loss 0.0479857, acc 0.984375\n",
      "2016-08-12T19:01:28.704117: step 1484, loss 0.0111283, acc 1\n",
      "2016-08-12T19:01:28.751825: step 1485, loss 0.0495473, acc 0.971429\n",
      "2016-08-12T19:01:28.824036: step 1486, loss 0.0223525, acc 1\n",
      "2016-08-12T19:01:28.895389: step 1487, loss 0.0141361, acc 1\n",
      "2016-08-12T19:01:28.967170: step 1488, loss 0.0175136, acc 1\n",
      "2016-08-12T19:01:29.039663: step 1489, loss 0.0214864, acc 1\n",
      "2016-08-12T19:01:29.113654: step 1490, loss 0.0257347, acc 1\n",
      "2016-08-12T19:01:29.186290: step 1491, loss 0.0265858, acc 1\n",
      "2016-08-12T19:01:29.258885: step 1492, loss 0.0115939, acc 1\n",
      "2016-08-12T19:01:29.331598: step 1493, loss 0.0289824, acc 1\n",
      "2016-08-12T19:01:29.402962: step 1494, loss 0.0212744, acc 1\n",
      "2016-08-12T19:01:29.478717: step 1495, loss 0.0194761, acc 1\n",
      "2016-08-12T19:01:29.551675: step 1496, loss 0.0259064, acc 1\n",
      "2016-08-12T19:01:29.625417: step 1497, loss 0.0200974, acc 1\n",
      "2016-08-12T19:01:29.698188: step 1498, loss 0.0716605, acc 0.96875\n",
      "2016-08-12T19:01:29.770888: step 1499, loss 0.0357604, acc 0.984375\n",
      "2016-08-12T19:01:29.821796: step 1500, loss 0.0268911, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:01:30.130579: step 1500, loss 0.675742, acc 0.757\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1500\n",
      "\n",
      "2016-08-12T19:01:30.790215: step 1501, loss 0.0261242, acc 1\n",
      "2016-08-12T19:01:30.863927: step 1502, loss 0.0157063, acc 1\n",
      "2016-08-12T19:01:30.939058: step 1503, loss 0.043618, acc 0.984375\n",
      "2016-08-12T19:01:31.012708: step 1504, loss 0.0187397, acc 1\n",
      "2016-08-12T19:01:31.085481: step 1505, loss 0.0358831, acc 1\n",
      "2016-08-12T19:01:31.159769: step 1506, loss 0.0194813, acc 1\n",
      "2016-08-12T19:01:31.233307: step 1507, loss 0.0354952, acc 0.984375\n",
      "2016-08-12T19:01:31.306549: step 1508, loss 0.0152741, acc 1\n",
      "2016-08-12T19:01:31.377886: step 1509, loss 0.0159547, acc 1\n",
      "2016-08-12T19:01:31.451299: step 1510, loss 0.0487379, acc 0.984375\n",
      "2016-08-12T19:01:31.523073: step 1511, loss 0.0339464, acc 0.984375\n",
      "2016-08-12T19:01:31.596565: step 1512, loss 0.0425505, acc 0.984375\n",
      "2016-08-12T19:01:31.670364: step 1513, loss 0.0159791, acc 1\n",
      "2016-08-12T19:01:31.741907: step 1514, loss 0.0880115, acc 0.96875\n",
      "2016-08-12T19:01:31.792018: step 1515, loss 0.00729397, acc 1\n",
      "2016-08-12T19:01:31.864297: step 1516, loss 0.0206604, acc 1\n",
      "2016-08-12T19:01:31.936714: step 1517, loss 0.0287164, acc 1\n",
      "2016-08-12T19:01:32.011645: step 1518, loss 0.0510209, acc 0.984375\n",
      "2016-08-12T19:01:32.085741: step 1519, loss 0.033165, acc 1\n",
      "2016-08-12T19:01:32.157366: step 1520, loss 0.0305899, acc 0.984375\n",
      "2016-08-12T19:01:32.230508: step 1521, loss 0.0271165, acc 1\n",
      "2016-08-12T19:01:32.304613: step 1522, loss 0.0076099, acc 1\n",
      "2016-08-12T19:01:32.376885: step 1523, loss 0.0278236, acc 1\n",
      "2016-08-12T19:01:32.448827: step 1524, loss 0.0207786, acc 1\n",
      "2016-08-12T19:01:32.522359: step 1525, loss 0.041717, acc 0.984375\n",
      "2016-08-12T19:01:32.597379: step 1526, loss 0.0286638, acc 0.984375\n",
      "2016-08-12T19:01:32.673516: step 1527, loss 0.036345, acc 1\n",
      "2016-08-12T19:01:32.746353: step 1528, loss 0.0119528, acc 1\n",
      "2016-08-12T19:01:32.819705: step 1529, loss 0.0174703, acc 1\n",
      "2016-08-12T19:01:32.866759: step 1530, loss 0.0454004, acc 1\n",
      "2016-08-12T19:01:32.940314: step 1531, loss 0.0228673, acc 1\n",
      "2016-08-12T19:01:33.011786: step 1532, loss 0.0183733, acc 1\n",
      "2016-08-12T19:01:33.083639: step 1533, loss 0.0198074, acc 1\n",
      "2016-08-12T19:01:33.156870: step 1534, loss 0.0180957, acc 1\n",
      "2016-08-12T19:01:33.229732: step 1535, loss 0.0257663, acc 1\n",
      "2016-08-12T19:01:33.302625: step 1536, loss 0.0226756, acc 1\n",
      "2016-08-12T19:01:33.375036: step 1537, loss 0.0274388, acc 1\n",
      "2016-08-12T19:01:33.448761: step 1538, loss 0.015272, acc 1\n",
      "2016-08-12T19:01:33.522160: step 1539, loss 0.0467245, acc 0.984375\n",
      "2016-08-12T19:01:33.596162: step 1540, loss 0.0287904, acc 1\n",
      "2016-08-12T19:01:33.669616: step 1541, loss 0.036846, acc 1\n",
      "2016-08-12T19:01:33.743488: step 1542, loss 0.0618029, acc 0.96875\n",
      "2016-08-12T19:01:33.816647: step 1543, loss 0.0429305, acc 0.984375\n",
      "2016-08-12T19:01:33.889250: step 1544, loss 0.0297079, acc 1\n",
      "2016-08-12T19:01:33.940493: step 1545, loss 0.0243848, acc 1\n",
      "2016-08-12T19:01:34.013760: step 1546, loss 0.0320909, acc 1\n",
      "2016-08-12T19:01:34.084811: step 1547, loss 0.0214963, acc 1\n",
      "2016-08-12T19:01:34.156913: step 1548, loss 0.0281712, acc 1\n",
      "2016-08-12T19:01:34.228048: step 1549, loss 0.0203168, acc 1\n",
      "2016-08-12T19:01:34.298318: step 1550, loss 0.0380962, acc 0.984375\n",
      "2016-08-12T19:01:34.370672: step 1551, loss 0.0277868, acc 1\n",
      "2016-08-12T19:01:34.441917: step 1552, loss 0.0226859, acc 1\n",
      "2016-08-12T19:01:34.513348: step 1553, loss 0.0207507, acc 1\n",
      "2016-08-12T19:01:34.585569: step 1554, loss 0.0285198, acc 1\n",
      "2016-08-12T19:01:34.656198: step 1555, loss 0.0123003, acc 1\n",
      "2016-08-12T19:01:34.730500: step 1556, loss 0.0277968, acc 1\n",
      "2016-08-12T19:01:34.803265: step 1557, loss 0.0346667, acc 0.984375\n",
      "2016-08-12T19:01:34.877467: step 1558, loss 0.0333414, acc 1\n",
      "2016-08-12T19:01:34.949903: step 1559, loss 0.0316044, acc 1\n",
      "2016-08-12T19:01:34.999452: step 1560, loss 0.00712328, acc 1\n",
      "2016-08-12T19:01:35.072804: step 1561, loss 0.0259354, acc 1\n",
      "2016-08-12T19:01:35.145501: step 1562, loss 0.0368519, acc 0.984375\n",
      "2016-08-12T19:01:35.217746: step 1563, loss 0.0296236, acc 1\n",
      "2016-08-12T19:01:35.291675: step 1564, loss 0.0231179, acc 1\n",
      "2016-08-12T19:01:35.365275: step 1565, loss 0.0215015, acc 1\n",
      "2016-08-12T19:01:35.438195: step 1566, loss 0.0380261, acc 1\n",
      "2016-08-12T19:01:35.509559: step 1567, loss 0.0625288, acc 0.984375\n",
      "2016-08-12T19:01:35.583150: step 1568, loss 0.0161528, acc 1\n",
      "2016-08-12T19:01:35.657286: step 1569, loss 0.0111076, acc 1\n",
      "2016-08-12T19:01:35.731166: step 1570, loss 0.0237098, acc 0.984375\n",
      "2016-08-12T19:01:35.804446: step 1571, loss 0.0397119, acc 0.984375\n",
      "2016-08-12T19:01:35.877673: step 1572, loss 0.00813641, acc 1\n",
      "2016-08-12T19:01:35.949260: step 1573, loss 0.0321734, acc 1\n",
      "2016-08-12T19:01:36.022126: step 1574, loss 0.0108978, acc 1\n",
      "2016-08-12T19:01:36.074792: step 1575, loss 0.00334134, acc 1\n",
      "2016-08-12T19:01:36.148547: step 1576, loss 0.0273414, acc 1\n",
      "2016-08-12T19:01:36.221367: step 1577, loss 0.0324016, acc 0.984375\n",
      "2016-08-12T19:01:36.296283: step 1578, loss 0.0284726, acc 1\n",
      "2016-08-12T19:01:36.370942: step 1579, loss 0.0193023, acc 1\n",
      "2016-08-12T19:01:36.444187: step 1580, loss 0.0244939, acc 1\n",
      "2016-08-12T19:01:36.516359: step 1581, loss 0.04267, acc 0.984375\n",
      "2016-08-12T19:01:36.590779: step 1582, loss 0.0113982, acc 1\n",
      "2016-08-12T19:01:36.665717: step 1583, loss 0.040358, acc 0.984375\n",
      "2016-08-12T19:01:36.736484: step 1584, loss 0.0185743, acc 1\n",
      "2016-08-12T19:01:36.808859: step 1585, loss 0.0193222, acc 1\n",
      "2016-08-12T19:01:36.881694: step 1586, loss 0.00661865, acc 1\n",
      "2016-08-12T19:01:36.955436: step 1587, loss 0.0317061, acc 1\n",
      "2016-08-12T19:01:37.027440: step 1588, loss 0.0349583, acc 0.984375\n",
      "2016-08-12T19:01:37.101262: step 1589, loss 0.0230226, acc 1\n",
      "2016-08-12T19:01:37.151500: step 1590, loss 0.0374027, acc 1\n",
      "2016-08-12T19:01:37.225333: step 1591, loss 0.0198658, acc 1\n",
      "2016-08-12T19:01:37.297765: step 1592, loss 0.0336553, acc 0.984375\n",
      "2016-08-12T19:01:37.371332: step 1593, loss 0.0233658, acc 0.984375\n",
      "2016-08-12T19:01:37.443835: step 1594, loss 0.0986524, acc 0.984375\n",
      "2016-08-12T19:01:37.518821: step 1595, loss 0.037628, acc 0.984375\n",
      "2016-08-12T19:01:37.591627: step 1596, loss 0.0328457, acc 0.984375\n",
      "2016-08-12T19:01:37.664068: step 1597, loss 0.00719216, acc 1\n",
      "2016-08-12T19:01:37.736872: step 1598, loss 0.0173731, acc 1\n",
      "2016-08-12T19:01:37.808670: step 1599, loss 0.0126971, acc 1\n",
      "2016-08-12T19:01:37.881105: step 1600, loss 0.0448997, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:01:38.189392: step 1600, loss 0.723615, acc 0.753\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1600\n",
      "\n",
      "2016-08-12T19:01:38.860471: step 1601, loss 0.0570107, acc 0.984375\n",
      "2016-08-12T19:01:38.932124: step 1602, loss 0.0236693, acc 1\n",
      "2016-08-12T19:01:39.002719: step 1603, loss 0.0239055, acc 1\n",
      "2016-08-12T19:01:39.076059: step 1604, loss 0.0390784, acc 0.984375\n",
      "2016-08-12T19:01:39.126581: step 1605, loss 0.0841233, acc 0.971429\n",
      "2016-08-12T19:01:39.200325: step 1606, loss 0.0168286, acc 1\n",
      "2016-08-12T19:01:39.272719: step 1607, loss 0.0123465, acc 1\n",
      "2016-08-12T19:01:39.344263: step 1608, loss 0.0166996, acc 1\n",
      "2016-08-12T19:01:39.417521: step 1609, loss 0.0172755, acc 1\n",
      "2016-08-12T19:01:39.490196: step 1610, loss 0.0269865, acc 0.984375\n",
      "2016-08-12T19:01:39.564337: step 1611, loss 0.0146526, acc 1\n",
      "2016-08-12T19:01:39.636228: step 1612, loss 0.0417718, acc 0.984375\n",
      "2016-08-12T19:01:39.708717: step 1613, loss 0.0346795, acc 1\n",
      "2016-08-12T19:01:39.781303: step 1614, loss 0.0351978, acc 0.984375\n",
      "2016-08-12T19:01:39.853925: step 1615, loss 0.0341979, acc 1\n",
      "2016-08-12T19:01:39.926932: step 1616, loss 0.0226654, acc 1\n",
      "2016-08-12T19:01:39.999588: step 1617, loss 0.0372035, acc 0.96875\n",
      "2016-08-12T19:01:40.072640: step 1618, loss 0.00997297, acc 1\n",
      "2016-08-12T19:01:40.145881: step 1619, loss 0.0107572, acc 1\n",
      "2016-08-12T19:01:40.195856: step 1620, loss 0.0216878, acc 1\n",
      "2016-08-12T19:01:40.269830: step 1621, loss 0.0141327, acc 1\n",
      "2016-08-12T19:01:40.341095: step 1622, loss 0.0118646, acc 1\n",
      "2016-08-12T19:01:40.413610: step 1623, loss 0.0133265, acc 1\n",
      "2016-08-12T19:01:40.486941: step 1624, loss 0.0527775, acc 0.984375\n",
      "2016-08-12T19:01:40.559914: step 1625, loss 0.0181639, acc 1\n",
      "2016-08-12T19:01:40.633925: step 1626, loss 0.016393, acc 1\n",
      "2016-08-12T19:01:40.707868: step 1627, loss 0.0276913, acc 0.984375\n",
      "2016-08-12T19:01:40.780927: step 1628, loss 0.019012, acc 1\n",
      "2016-08-12T19:01:40.852906: step 1629, loss 0.0211958, acc 1\n",
      "2016-08-12T19:01:40.926479: step 1630, loss 0.0320514, acc 0.984375\n",
      "2016-08-12T19:01:40.999057: step 1631, loss 0.0205469, acc 1\n",
      "2016-08-12T19:01:41.072284: step 1632, loss 0.0165248, acc 1\n",
      "2016-08-12T19:01:41.144509: step 1633, loss 0.0392984, acc 0.984375\n",
      "2016-08-12T19:01:41.215235: step 1634, loss 0.0139694, acc 1\n",
      "2016-08-12T19:01:41.264819: step 1635, loss 0.0477613, acc 0.971429\n",
      "2016-08-12T19:01:41.336669: step 1636, loss 0.0218544, acc 1\n",
      "2016-08-12T19:01:41.411127: step 1637, loss 0.0310886, acc 1\n",
      "2016-08-12T19:01:41.484456: step 1638, loss 0.0712872, acc 0.96875\n",
      "2016-08-12T19:01:41.557778: step 1639, loss 0.0166481, acc 1\n",
      "2016-08-12T19:01:41.629029: step 1640, loss 0.0248387, acc 0.984375\n",
      "2016-08-12T19:01:41.701314: step 1641, loss 0.0158196, acc 1\n",
      "2016-08-12T19:01:41.773883: step 1642, loss 0.0198694, acc 1\n",
      "2016-08-12T19:01:41.846414: step 1643, loss 0.0185329, acc 1\n",
      "2016-08-12T19:01:41.919319: step 1644, loss 0.0541156, acc 0.984375\n",
      "2016-08-12T19:01:41.994125: step 1645, loss 0.0255512, acc 1\n",
      "2016-08-12T19:01:42.067749: step 1646, loss 0.0141978, acc 1\n",
      "2016-08-12T19:01:42.138143: step 1647, loss 0.0151297, acc 1\n",
      "2016-08-12T19:01:42.208695: step 1648, loss 0.0253655, acc 1\n",
      "2016-08-12T19:01:42.281917: step 1649, loss 0.0363229, acc 0.984375\n",
      "2016-08-12T19:01:42.332814: step 1650, loss 0.00871173, acc 1\n",
      "2016-08-12T19:01:42.405965: step 1651, loss 0.027052, acc 0.984375\n",
      "2016-08-12T19:01:42.476416: step 1652, loss 0.0103799, acc 1\n",
      "2016-08-12T19:01:42.551165: step 1653, loss 0.00931925, acc 1\n",
      "2016-08-12T19:01:42.624524: step 1654, loss 0.0150811, acc 1\n",
      "2016-08-12T19:01:42.697819: step 1655, loss 0.0324668, acc 1\n",
      "2016-08-12T19:01:42.769917: step 1656, loss 0.0182239, acc 0.984375\n",
      "2016-08-12T19:01:42.844379: step 1657, loss 0.0262075, acc 1\n",
      "2016-08-12T19:01:42.915720: step 1658, loss 0.023356, acc 1\n",
      "2016-08-12T19:01:42.986718: step 1659, loss 0.02233, acc 1\n",
      "2016-08-12T19:01:43.059068: step 1660, loss 0.0324569, acc 0.984375\n",
      "2016-08-12T19:01:43.132674: step 1661, loss 0.00715289, acc 1\n",
      "2016-08-12T19:01:43.205000: step 1662, loss 0.0568699, acc 0.96875\n",
      "2016-08-12T19:01:43.280116: step 1663, loss 0.0108418, acc 1\n",
      "2016-08-12T19:01:43.351341: step 1664, loss 0.0187171, acc 1\n",
      "2016-08-12T19:01:43.397196: step 1665, loss 0.0278263, acc 1\n",
      "2016-08-12T19:01:43.470741: step 1666, loss 0.0882447, acc 0.984375\n",
      "2016-08-12T19:01:43.543463: step 1667, loss 0.00773883, acc 1\n",
      "2016-08-12T19:01:43.615068: step 1668, loss 0.0210073, acc 0.984375\n",
      "2016-08-12T19:01:43.687577: step 1669, loss 0.0151784, acc 1\n",
      "2016-08-12T19:01:43.759520: step 1670, loss 0.00827123, acc 1\n",
      "2016-08-12T19:01:43.832288: step 1671, loss 0.0363345, acc 0.984375\n",
      "2016-08-12T19:01:43.904983: step 1672, loss 0.023322, acc 1\n",
      "2016-08-12T19:01:43.978200: step 1673, loss 0.0335917, acc 1\n",
      "2016-08-12T19:01:44.049963: step 1674, loss 0.0180288, acc 1\n",
      "2016-08-12T19:01:44.122595: step 1675, loss 0.016872, acc 1\n",
      "2016-08-12T19:01:44.196114: step 1676, loss 0.0336552, acc 0.984375\n",
      "2016-08-12T19:01:44.269900: step 1677, loss 0.0641354, acc 0.96875\n",
      "2016-08-12T19:01:44.342652: step 1678, loss 0.028025, acc 0.984375\n",
      "2016-08-12T19:01:44.414238: step 1679, loss 0.0291403, acc 1\n",
      "2016-08-12T19:01:44.463871: step 1680, loss 0.0196877, acc 1\n",
      "2016-08-12T19:01:44.537688: step 1681, loss 0.0126375, acc 1\n",
      "2016-08-12T19:01:44.609947: step 1682, loss 0.0141305, acc 1\n",
      "2016-08-12T19:01:44.682425: step 1683, loss 0.0126942, acc 1\n",
      "2016-08-12T19:01:44.755732: step 1684, loss 0.0178105, acc 1\n",
      "2016-08-12T19:01:44.827996: step 1685, loss 0.00823646, acc 1\n",
      "2016-08-12T19:01:44.901191: step 1686, loss 0.0122205, acc 1\n",
      "2016-08-12T19:01:44.973573: step 1687, loss 0.0148327, acc 1\n",
      "2016-08-12T19:01:45.046018: step 1688, loss 0.0195701, acc 1\n",
      "2016-08-12T19:01:45.119200: step 1689, loss 0.012751, acc 1\n",
      "2016-08-12T19:01:45.191960: step 1690, loss 0.0150892, acc 1\n",
      "2016-08-12T19:01:45.263549: step 1691, loss 0.0172488, acc 1\n",
      "2016-08-12T19:01:45.336311: step 1692, loss 0.0299366, acc 0.984375\n",
      "2016-08-12T19:01:45.408230: step 1693, loss 0.0270736, acc 1\n",
      "2016-08-12T19:01:45.481890: step 1694, loss 0.0522884, acc 0.984375\n",
      "2016-08-12T19:01:45.532393: step 1695, loss 0.0133179, acc 1\n",
      "2016-08-12T19:01:45.604336: step 1696, loss 0.0196838, acc 1\n",
      "2016-08-12T19:01:45.677711: step 1697, loss 0.021031, acc 1\n",
      "2016-08-12T19:01:45.750553: step 1698, loss 0.0845504, acc 0.984375\n",
      "2016-08-12T19:01:45.825204: step 1699, loss 0.0159611, acc 1\n",
      "2016-08-12T19:01:45.898967: step 1700, loss 0.02885, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:01:46.208285: step 1700, loss 0.739761, acc 0.744\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1700\n",
      "\n",
      "2016-08-12T19:01:46.879306: step 1701, loss 0.0541764, acc 0.984375\n",
      "2016-08-12T19:01:46.953930: step 1702, loss 0.0102012, acc 1\n",
      "2016-08-12T19:01:47.027752: step 1703, loss 0.0228156, acc 1\n",
      "2016-08-12T19:01:47.099376: step 1704, loss 0.0214162, acc 1\n",
      "2016-08-12T19:01:47.172145: step 1705, loss 0.0127074, acc 1\n",
      "2016-08-12T19:01:47.245613: step 1706, loss 0.0429473, acc 0.984375\n",
      "2016-08-12T19:01:47.319477: step 1707, loss 0.0147118, acc 1\n",
      "2016-08-12T19:01:47.390952: step 1708, loss 0.0164011, acc 1\n",
      "2016-08-12T19:01:47.463246: step 1709, loss 0.00976521, acc 1\n",
      "2016-08-12T19:01:47.512679: step 1710, loss 0.0268602, acc 1\n",
      "2016-08-12T19:01:47.586404: step 1711, loss 0.0109442, acc 1\n",
      "2016-08-12T19:01:47.658636: step 1712, loss 0.0320562, acc 0.984375\n",
      "2016-08-12T19:01:47.731675: step 1713, loss 0.0510358, acc 0.984375\n",
      "2016-08-12T19:01:47.806014: step 1714, loss 0.0332604, acc 0.984375\n",
      "2016-08-12T19:01:47.880322: step 1715, loss 0.0122066, acc 1\n",
      "2016-08-12T19:01:47.954238: step 1716, loss 0.0522811, acc 0.984375\n",
      "2016-08-12T19:01:48.027697: step 1717, loss 0.0468367, acc 0.984375\n",
      "2016-08-12T19:01:48.099965: step 1718, loss 0.0318049, acc 1\n",
      "2016-08-12T19:01:48.173688: step 1719, loss 0.00886114, acc 1\n",
      "2016-08-12T19:01:48.245880: step 1720, loss 0.0240052, acc 0.984375\n",
      "2016-08-12T19:01:48.319475: step 1721, loss 0.0133464, acc 1\n",
      "2016-08-12T19:01:48.392440: step 1722, loss 0.0163228, acc 1\n",
      "2016-08-12T19:01:48.464865: step 1723, loss 0.022796, acc 0.984375\n",
      "2016-08-12T19:01:48.537895: step 1724, loss 0.0105609, acc 1\n",
      "2016-08-12T19:01:48.588045: step 1725, loss 0.0388701, acc 1\n",
      "2016-08-12T19:01:48.661544: step 1726, loss 0.00934506, acc 1\n",
      "2016-08-12T19:01:48.733576: step 1727, loss 0.00864143, acc 1\n",
      "2016-08-12T19:01:48.805303: step 1728, loss 0.0114527, acc 1\n",
      "2016-08-12T19:01:48.876022: step 1729, loss 0.0291632, acc 1\n",
      "2016-08-12T19:01:48.948710: step 1730, loss 0.0110984, acc 1\n",
      "2016-08-12T19:01:49.021833: step 1731, loss 0.009657, acc 1\n",
      "2016-08-12T19:01:49.095897: step 1732, loss 0.0272949, acc 0.984375\n",
      "2016-08-12T19:01:49.168609: step 1733, loss 0.0164817, acc 1\n",
      "2016-08-12T19:01:49.242522: step 1734, loss 0.0075759, acc 1\n",
      "2016-08-12T19:01:49.315843: step 1735, loss 0.034497, acc 0.984375\n",
      "2016-08-12T19:01:49.388079: step 1736, loss 0.0269922, acc 1\n",
      "2016-08-12T19:01:49.460390: step 1737, loss 0.0205458, acc 1\n",
      "2016-08-12T19:01:49.534051: step 1738, loss 0.0130721, acc 1\n",
      "2016-08-12T19:01:49.607304: step 1739, loss 0.028575, acc 0.984375\n",
      "2016-08-12T19:01:49.657377: step 1740, loss 0.0124395, acc 1\n",
      "2016-08-12T19:01:49.729696: step 1741, loss 0.0132472, acc 1\n",
      "2016-08-12T19:01:49.801730: step 1742, loss 0.0113986, acc 1\n",
      "2016-08-12T19:01:49.875399: step 1743, loss 0.0109181, acc 1\n",
      "2016-08-12T19:01:49.949112: step 1744, loss 0.0141865, acc 1\n",
      "2016-08-12T19:01:50.022076: step 1745, loss 0.049332, acc 0.953125\n",
      "2016-08-12T19:01:50.096697: step 1746, loss 0.0575942, acc 0.96875\n",
      "2016-08-12T19:01:50.169596: step 1747, loss 0.0197243, acc 1\n",
      "2016-08-12T19:01:50.241242: step 1748, loss 0.0152655, acc 1\n",
      "2016-08-12T19:01:50.313392: step 1749, loss 0.02963, acc 1\n",
      "2016-08-12T19:01:50.385953: step 1750, loss 0.00683281, acc 1\n",
      "2016-08-12T19:01:50.462772: step 1751, loss 0.0793289, acc 0.96875\n",
      "2016-08-12T19:01:50.537884: step 1752, loss 0.0331627, acc 1\n",
      "2016-08-12T19:01:50.610483: step 1753, loss 0.0103803, acc 1\n",
      "2016-08-12T19:01:50.682976: step 1754, loss 0.0204178, acc 1\n",
      "2016-08-12T19:01:50.739474: step 1755, loss 0.0131581, acc 1\n",
      "2016-08-12T19:01:50.812826: step 1756, loss 0.018591, acc 1\n",
      "2016-08-12T19:01:50.886584: step 1757, loss 0.0115123, acc 1\n",
      "2016-08-12T19:01:50.960472: step 1758, loss 0.0115184, acc 1\n",
      "2016-08-12T19:01:51.032614: step 1759, loss 0.0174692, acc 1\n",
      "2016-08-12T19:01:51.106388: step 1760, loss 0.00916889, acc 1\n",
      "2016-08-12T19:01:51.178745: step 1761, loss 0.0286922, acc 1\n",
      "2016-08-12T19:01:51.251141: step 1762, loss 0.0187328, acc 0.984375\n",
      "2016-08-12T19:01:51.325433: step 1763, loss 0.00786108, acc 1\n",
      "2016-08-12T19:01:51.398387: step 1764, loss 0.0263004, acc 1\n",
      "2016-08-12T19:01:51.472418: step 1765, loss 0.038496, acc 0.984375\n",
      "2016-08-12T19:01:51.545588: step 1766, loss 0.040954, acc 0.984375\n",
      "2016-08-12T19:01:51.618154: step 1767, loss 0.0419698, acc 0.984375\n",
      "2016-08-12T19:01:51.693300: step 1768, loss 0.0235495, acc 0.984375\n",
      "2016-08-12T19:01:51.765880: step 1769, loss 0.0197485, acc 1\n",
      "2016-08-12T19:01:51.814187: step 1770, loss 0.0604353, acc 0.971429\n",
      "2016-08-12T19:01:51.884924: step 1771, loss 0.0148366, acc 1\n",
      "2016-08-12T19:01:51.957718: step 1772, loss 0.0285443, acc 0.984375\n",
      "2016-08-12T19:01:52.031536: step 1773, loss 0.0163539, acc 1\n",
      "2016-08-12T19:01:52.105253: step 1774, loss 0.0528472, acc 0.984375\n",
      "2016-08-12T19:01:52.178477: step 1775, loss 0.0121939, acc 1\n",
      "2016-08-12T19:01:52.251176: step 1776, loss 0.0555036, acc 0.984375\n",
      "2016-08-12T19:01:52.325779: step 1777, loss 0.0137693, acc 1\n",
      "2016-08-12T19:01:52.399479: step 1778, loss 0.0173251, acc 1\n",
      "2016-08-12T19:01:52.473729: step 1779, loss 0.014372, acc 1\n",
      "2016-08-12T19:01:52.546792: step 1780, loss 0.0345874, acc 0.984375\n",
      "2016-08-12T19:01:52.621504: step 1781, loss 0.0228151, acc 1\n",
      "2016-08-12T19:01:52.693862: step 1782, loss 0.0262042, acc 1\n",
      "2016-08-12T19:01:52.766308: step 1783, loss 0.0207153, acc 1\n",
      "2016-08-12T19:01:52.839225: step 1784, loss 0.0237378, acc 1\n",
      "2016-08-12T19:01:52.889171: step 1785, loss 0.0134432, acc 1\n",
      "2016-08-12T19:01:52.963861: step 1786, loss 0.0428466, acc 0.984375\n",
      "2016-08-12T19:01:53.038072: step 1787, loss 0.0250393, acc 0.984375\n",
      "2016-08-12T19:01:53.109551: step 1788, loss 0.0221317, acc 1\n",
      "2016-08-12T19:01:53.180304: step 1789, loss 0.0106302, acc 1\n",
      "2016-08-12T19:01:53.252811: step 1790, loss 0.0171551, acc 1\n",
      "2016-08-12T19:01:53.323614: step 1791, loss 0.0106833, acc 1\n",
      "2016-08-12T19:01:53.396953: step 1792, loss 0.0614775, acc 0.984375\n",
      "2016-08-12T19:01:53.470618: step 1793, loss 0.0535183, acc 0.984375\n",
      "2016-08-12T19:01:53.543949: step 1794, loss 0.0156219, acc 1\n",
      "2016-08-12T19:01:53.618608: step 1795, loss 0.0233343, acc 1\n",
      "2016-08-12T19:01:53.694728: step 1796, loss 0.0512544, acc 0.96875\n",
      "2016-08-12T19:01:53.769264: step 1797, loss 0.0831951, acc 0.984375\n",
      "2016-08-12T19:01:53.843391: step 1798, loss 0.0315352, acc 0.984375\n",
      "2016-08-12T19:01:53.918168: step 1799, loss 0.00838787, acc 1\n",
      "2016-08-12T19:01:53.967852: step 1800, loss 0.0162447, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:01:54.280098: step 1800, loss 0.815164, acc 0.735\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1800\n",
      "\n",
      "2016-08-12T19:01:54.964166: step 1801, loss 0.0152072, acc 1\n",
      "2016-08-12T19:01:55.036503: step 1802, loss 0.0406079, acc 0.984375\n",
      "2016-08-12T19:01:55.109946: step 1803, loss 0.0358319, acc 0.96875\n",
      "2016-08-12T19:01:55.183381: step 1804, loss 0.0179316, acc 0.984375\n",
      "2016-08-12T19:01:55.256724: step 1805, loss 0.0247611, acc 1\n",
      "2016-08-12T19:01:55.328219: step 1806, loss 0.0271218, acc 1\n",
      "2016-08-12T19:01:55.400993: step 1807, loss 0.0398104, acc 0.984375\n",
      "2016-08-12T19:01:55.471886: step 1808, loss 0.0230967, acc 1\n",
      "2016-08-12T19:01:55.543299: step 1809, loss 0.0161321, acc 1\n",
      "2016-08-12T19:01:55.616440: step 1810, loss 0.0203707, acc 1\n",
      "2016-08-12T19:01:55.689663: step 1811, loss 0.0126723, acc 1\n",
      "2016-08-12T19:01:55.768675: step 1812, loss 0.0141957, acc 1\n",
      "2016-08-12T19:01:55.840429: step 1813, loss 0.00967509, acc 1\n",
      "2016-08-12T19:01:55.914814: step 1814, loss 0.0168165, acc 0.984375\n",
      "2016-08-12T19:01:55.964700: step 1815, loss 0.0119039, acc 1\n",
      "2016-08-12T19:01:56.036793: step 1816, loss 0.0275569, acc 1\n",
      "2016-08-12T19:01:56.111591: step 1817, loss 0.020183, acc 1\n",
      "2016-08-12T19:01:56.184173: step 1818, loss 0.0335199, acc 1\n",
      "2016-08-12T19:01:56.257297: step 1819, loss 0.0110923, acc 1\n",
      "2016-08-12T19:01:56.329233: step 1820, loss 0.0119467, acc 1\n",
      "2016-08-12T19:01:56.401974: step 1821, loss 0.00572328, acc 1\n",
      "2016-08-12T19:01:56.474703: step 1822, loss 0.0172537, acc 1\n",
      "2016-08-12T19:01:56.548596: step 1823, loss 0.0351251, acc 0.984375\n",
      "2016-08-12T19:01:56.622513: step 1824, loss 0.0184556, acc 1\n",
      "2016-08-12T19:01:56.694734: step 1825, loss 0.0246366, acc 1\n",
      "2016-08-12T19:01:56.767443: step 1826, loss 0.012026, acc 1\n",
      "2016-08-12T19:01:56.841159: step 1827, loss 0.0155156, acc 1\n",
      "2016-08-12T19:01:56.912716: step 1828, loss 0.0183443, acc 1\n",
      "2016-08-12T19:01:56.986948: step 1829, loss 0.010869, acc 1\n",
      "2016-08-12T19:01:57.037658: step 1830, loss 0.0115125, acc 1\n",
      "2016-08-12T19:01:57.111130: step 1831, loss 0.0122218, acc 1\n",
      "2016-08-12T19:01:57.183807: step 1832, loss 0.0125127, acc 1\n",
      "2016-08-12T19:01:57.257024: step 1833, loss 0.00978215, acc 1\n",
      "2016-08-12T19:01:57.329178: step 1834, loss 0.0157659, acc 1\n",
      "2016-08-12T19:01:57.401631: step 1835, loss 0.0284337, acc 0.984375\n",
      "2016-08-12T19:01:57.473766: step 1836, loss 0.0216887, acc 1\n",
      "2016-08-12T19:01:57.546856: step 1837, loss 0.0122201, acc 1\n",
      "2016-08-12T19:01:57.617316: step 1838, loss 0.0508981, acc 0.984375\n",
      "2016-08-12T19:01:57.691514: step 1839, loss 0.0559883, acc 0.984375\n",
      "2016-08-12T19:01:57.766609: step 1840, loss 0.0186528, acc 1\n",
      "2016-08-12T19:01:57.839961: step 1841, loss 0.0267932, acc 0.984375\n",
      "2016-08-12T19:01:57.913904: step 1842, loss 0.0141513, acc 1\n",
      "2016-08-12T19:01:57.986750: step 1843, loss 0.0500208, acc 0.984375\n",
      "2016-08-12T19:01:58.061074: step 1844, loss 0.0285199, acc 0.984375\n",
      "2016-08-12T19:01:58.114819: step 1845, loss 0.0104225, acc 1\n",
      "2016-08-12T19:01:58.187822: step 1846, loss 0.0161564, acc 1\n",
      "2016-08-12T19:01:58.260954: step 1847, loss 0.0202463, acc 1\n",
      "2016-08-12T19:01:58.334567: step 1848, loss 0.0227774, acc 1\n",
      "2016-08-12T19:01:58.409446: step 1849, loss 0.0132648, acc 1\n",
      "2016-08-12T19:01:58.481135: step 1850, loss 0.0130119, acc 1\n",
      "2016-08-12T19:01:58.555507: step 1851, loss 0.00832689, acc 1\n",
      "2016-08-12T19:01:58.629160: step 1852, loss 0.0250793, acc 0.984375\n",
      "2016-08-12T19:01:58.702259: step 1853, loss 0.0141931, acc 1\n",
      "2016-08-12T19:01:58.775408: step 1854, loss 0.0134307, acc 1\n",
      "2016-08-12T19:01:58.847465: step 1855, loss 0.0223215, acc 1\n",
      "2016-08-12T19:01:58.920699: step 1856, loss 0.0307645, acc 1\n",
      "2016-08-12T19:01:58.994576: step 1857, loss 0.0106191, acc 1\n",
      "2016-08-12T19:01:59.066043: step 1858, loss 0.0215305, acc 0.984375\n",
      "2016-08-12T19:01:59.137796: step 1859, loss 0.0671725, acc 0.96875\n",
      "2016-08-12T19:01:59.189169: step 1860, loss 0.0341544, acc 0.971429\n",
      "2016-08-12T19:01:59.262717: step 1861, loss 0.00816113, acc 1\n",
      "2016-08-12T19:01:59.335830: step 1862, loss 0.0282685, acc 1\n",
      "2016-08-12T19:01:59.409630: step 1863, loss 0.0217604, acc 1\n",
      "2016-08-12T19:01:59.482383: step 1864, loss 0.0145697, acc 1\n",
      "2016-08-12T19:01:59.555568: step 1865, loss 0.0212077, acc 1\n",
      "2016-08-12T19:01:59.628728: step 1866, loss 0.0299139, acc 0.984375\n",
      "2016-08-12T19:01:59.702357: step 1867, loss 0.0171723, acc 1\n",
      "2016-08-12T19:01:59.776401: step 1868, loss 0.025341, acc 0.984375\n",
      "2016-08-12T19:01:59.850184: step 1869, loss 0.0255083, acc 1\n",
      "2016-08-12T19:01:59.924610: step 1870, loss 0.0196008, acc 1\n",
      "2016-08-12T19:01:59.996912: step 1871, loss 0.0954243, acc 0.984375\n",
      "2016-08-12T19:02:00.070825: step 1872, loss 0.0116576, acc 1\n",
      "2016-08-12T19:02:00.143375: step 1873, loss 0.0201363, acc 1\n",
      "2016-08-12T19:02:00.216286: step 1874, loss 0.0264506, acc 0.984375\n",
      "2016-08-12T19:02:00.267073: step 1875, loss 0.0111261, acc 1\n",
      "2016-08-12T19:02:00.339814: step 1876, loss 0.00633653, acc 1\n",
      "2016-08-12T19:02:00.412139: step 1877, loss 0.0583894, acc 0.96875\n",
      "2016-08-12T19:02:00.489991: step 1878, loss 0.0282886, acc 0.984375\n",
      "2016-08-12T19:02:00.565248: step 1879, loss 0.022506, acc 0.984375\n",
      "2016-08-12T19:02:00.640857: step 1880, loss 0.0137836, acc 1\n",
      "2016-08-12T19:02:00.714256: step 1881, loss 0.0138645, acc 1\n",
      "2016-08-12T19:02:00.788565: step 1882, loss 0.0189285, acc 1\n",
      "2016-08-12T19:02:00.860113: step 1883, loss 0.00255033, acc 1\n",
      "2016-08-12T19:02:00.932372: step 1884, loss 0.0108654, acc 1\n",
      "2016-08-12T19:02:01.004692: step 1885, loss 0.0255664, acc 1\n",
      "2016-08-12T19:02:01.078073: step 1886, loss 0.0162929, acc 1\n",
      "2016-08-12T19:02:01.150332: step 1887, loss 0.00564046, acc 1\n",
      "2016-08-12T19:02:01.227352: step 1888, loss 0.0408666, acc 0.984375\n",
      "2016-08-12T19:02:01.301020: step 1889, loss 0.00739248, acc 1\n",
      "2016-08-12T19:02:01.351363: step 1890, loss 0.0060853, acc 1\n",
      "2016-08-12T19:02:01.423931: step 1891, loss 0.0118629, acc 1\n",
      "2016-08-12T19:02:01.498576: step 1892, loss 0.00706634, acc 1\n",
      "2016-08-12T19:02:01.570124: step 1893, loss 0.0235658, acc 0.984375\n",
      "2016-08-12T19:02:01.642803: step 1894, loss 0.0146798, acc 1\n",
      "2016-08-12T19:02:01.715350: step 1895, loss 0.0323283, acc 0.984375\n",
      "2016-08-12T19:02:01.787672: step 1896, loss 0.0170864, acc 1\n",
      "2016-08-12T19:02:01.860710: step 1897, loss 0.0360598, acc 0.984375\n",
      "2016-08-12T19:02:01.933747: step 1898, loss 0.0325187, acc 0.984375\n",
      "2016-08-12T19:02:02.005299: step 1899, loss 0.0267723, acc 1\n",
      "2016-08-12T19:02:02.078391: step 1900, loss 0.0460014, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:02.388432: step 1900, loss 0.769756, acc 0.751\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-1900\n",
      "\n",
      "2016-08-12T19:02:03.058618: step 1901, loss 0.0146176, acc 1\n",
      "2016-08-12T19:02:03.131079: step 1902, loss 0.0248741, acc 1\n",
      "2016-08-12T19:02:03.206745: step 1903, loss 0.0393716, acc 0.984375\n",
      "2016-08-12T19:02:03.285719: step 1904, loss 0.0168693, acc 0.984375\n",
      "2016-08-12T19:02:03.339901: step 1905, loss 0.0274104, acc 1\n",
      "2016-08-12T19:02:03.414056: step 1906, loss 0.0266968, acc 1\n",
      "2016-08-12T19:02:03.487739: step 1907, loss 0.024733, acc 1\n",
      "2016-08-12T19:02:03.561453: step 1908, loss 0.0781771, acc 0.984375\n",
      "2016-08-12T19:02:03.635199: step 1909, loss 0.0425836, acc 1\n",
      "2016-08-12T19:02:03.707692: step 1910, loss 0.00969201, acc 1\n",
      "2016-08-12T19:02:03.782512: step 1911, loss 0.0190208, acc 1\n",
      "2016-08-12T19:02:03.856471: step 1912, loss 0.0257328, acc 1\n",
      "2016-08-12T19:02:03.930111: step 1913, loss 0.0394896, acc 0.984375\n",
      "2016-08-12T19:02:04.004961: step 1914, loss 0.013354, acc 1\n",
      "2016-08-12T19:02:04.078672: step 1915, loss 0.0106517, acc 1\n",
      "2016-08-12T19:02:04.152372: step 1916, loss 0.0065985, acc 1\n",
      "2016-08-12T19:02:04.223513: step 1917, loss 0.006773, acc 1\n",
      "2016-08-12T19:02:04.295869: step 1918, loss 0.0174681, acc 1\n",
      "2016-08-12T19:02:04.369639: step 1919, loss 0.0343171, acc 0.984375\n",
      "2016-08-12T19:02:04.419782: step 1920, loss 0.00666357, acc 1\n",
      "2016-08-12T19:02:04.491649: step 1921, loss 0.0220664, acc 0.984375\n",
      "2016-08-12T19:02:04.565761: step 1922, loss 0.0183109, acc 0.984375\n",
      "2016-08-12T19:02:04.638522: step 1923, loss 0.0128869, acc 1\n",
      "2016-08-12T19:02:04.713720: step 1924, loss 0.0171119, acc 1\n",
      "2016-08-12T19:02:04.787995: step 1925, loss 0.0168945, acc 1\n",
      "2016-08-12T19:02:04.860659: step 1926, loss 0.0100442, acc 1\n",
      "2016-08-12T19:02:04.934309: step 1927, loss 0.050423, acc 0.984375\n",
      "2016-08-12T19:02:05.008332: step 1928, loss 0.0554954, acc 0.984375\n",
      "2016-08-12T19:02:05.081018: step 1929, loss 0.0489511, acc 0.984375\n",
      "2016-08-12T19:02:05.154304: step 1930, loss 0.019878, acc 1\n",
      "2016-08-12T19:02:05.227648: step 1931, loss 0.0154652, acc 1\n",
      "2016-08-12T19:02:05.300880: step 1932, loss 0.0112423, acc 1\n",
      "2016-08-12T19:02:05.375923: step 1933, loss 0.0156611, acc 1\n",
      "2016-08-12T19:02:05.448627: step 1934, loss 0.0589921, acc 0.96875\n",
      "2016-08-12T19:02:05.495973: step 1935, loss 0.0284781, acc 1\n",
      "2016-08-12T19:02:05.569030: step 1936, loss 0.0771194, acc 0.984375\n",
      "2016-08-12T19:02:05.643162: step 1937, loss 0.0771283, acc 0.984375\n",
      "2016-08-12T19:02:05.718938: step 1938, loss 0.00937939, acc 1\n",
      "2016-08-12T19:02:05.793264: step 1939, loss 0.0269513, acc 1\n",
      "2016-08-12T19:02:05.866513: step 1940, loss 0.0203064, acc 1\n",
      "2016-08-12T19:02:05.938766: step 1941, loss 0.0267902, acc 0.984375\n",
      "2016-08-12T19:02:06.011894: step 1942, loss 0.0383729, acc 0.984375\n",
      "2016-08-12T19:02:06.083332: step 1943, loss 0.0555942, acc 0.984375\n",
      "2016-08-12T19:02:06.154853: step 1944, loss 0.0445945, acc 1\n",
      "2016-08-12T19:02:06.227812: step 1945, loss 0.0105649, acc 1\n",
      "2016-08-12T19:02:06.301420: step 1946, loss 0.00946143, acc 1\n",
      "2016-08-12T19:02:06.373963: step 1947, loss 0.0132248, acc 1\n",
      "2016-08-12T19:02:06.446962: step 1948, loss 0.0271038, acc 1\n",
      "2016-08-12T19:02:06.520798: step 1949, loss 0.0190692, acc 1\n",
      "2016-08-12T19:02:06.571781: step 1950, loss 0.0372988, acc 0.971429\n",
      "2016-08-12T19:02:06.644227: step 1951, loss 0.0655676, acc 0.984375\n",
      "2016-08-12T19:02:06.717877: step 1952, loss 0.0221359, acc 1\n",
      "2016-08-12T19:02:06.787635: step 1953, loss 0.0154481, acc 1\n",
      "2016-08-12T19:02:06.859434: step 1954, loss 0.00795063, acc 1\n",
      "2016-08-12T19:02:06.929771: step 1955, loss 0.0298758, acc 0.984375\n",
      "2016-08-12T19:02:07.004615: step 1956, loss 0.0145778, acc 1\n",
      "2016-08-12T19:02:07.079050: step 1957, loss 0.0376928, acc 0.96875\n",
      "2016-08-12T19:02:07.151721: step 1958, loss 0.0250813, acc 1\n",
      "2016-08-12T19:02:07.225558: step 1959, loss 0.0201319, acc 1\n",
      "2016-08-12T19:02:07.297747: step 1960, loss 0.0172201, acc 1\n",
      "2016-08-12T19:02:07.372447: step 1961, loss 0.0285682, acc 0.984375\n",
      "2016-08-12T19:02:07.446917: step 1962, loss 0.0602331, acc 0.96875\n",
      "2016-08-12T19:02:07.521547: step 1963, loss 0.0193664, acc 1\n",
      "2016-08-12T19:02:07.593681: step 1964, loss 0.0174203, acc 1\n",
      "2016-08-12T19:02:07.644977: step 1965, loss 0.0170254, acc 1\n",
      "2016-08-12T19:02:07.718639: step 1966, loss 0.0157351, acc 1\n",
      "2016-08-12T19:02:07.791564: step 1967, loss 0.01374, acc 1\n",
      "2016-08-12T19:02:07.864324: step 1968, loss 0.00832565, acc 1\n",
      "2016-08-12T19:02:07.935674: step 1969, loss 0.0154076, acc 1\n",
      "2016-08-12T19:02:08.008561: step 1970, loss 0.0247268, acc 1\n",
      "2016-08-12T19:02:08.082874: step 1971, loss 0.0327548, acc 0.984375\n",
      "2016-08-12T19:02:08.154097: step 1972, loss 0.0304875, acc 1\n",
      "2016-08-12T19:02:08.228982: step 1973, loss 0.0759547, acc 0.96875\n",
      "2016-08-12T19:02:08.301043: step 1974, loss 0.0166853, acc 1\n",
      "2016-08-12T19:02:08.375509: step 1975, loss 0.00979214, acc 1\n",
      "2016-08-12T19:02:08.446633: step 1976, loss 0.0231545, acc 1\n",
      "2016-08-12T19:02:08.520920: step 1977, loss 0.0293574, acc 0.984375\n",
      "2016-08-12T19:02:08.595289: step 1978, loss 0.0310086, acc 1\n",
      "2016-08-12T19:02:08.666374: step 1979, loss 0.032951, acc 0.984375\n",
      "2016-08-12T19:02:08.713587: step 1980, loss 0.0082162, acc 1\n",
      "2016-08-12T19:02:08.788070: step 1981, loss 0.0247536, acc 0.984375\n",
      "2016-08-12T19:02:08.861393: step 1982, loss 0.0487263, acc 0.96875\n",
      "2016-08-12T19:02:08.934841: step 1983, loss 0.0105738, acc 1\n",
      "2016-08-12T19:02:09.007437: step 1984, loss 0.0225008, acc 1\n",
      "2016-08-12T19:02:09.080751: step 1985, loss 0.010526, acc 1\n",
      "2016-08-12T19:02:09.152336: step 1986, loss 0.0446037, acc 0.984375\n",
      "2016-08-12T19:02:09.226216: step 1987, loss 0.0216141, acc 1\n",
      "2016-08-12T19:02:09.301638: step 1988, loss 0.0300297, acc 1\n",
      "2016-08-12T19:02:09.373831: step 1989, loss 0.0249082, acc 1\n",
      "2016-08-12T19:02:09.446352: step 1990, loss 0.0148327, acc 1\n",
      "2016-08-12T19:02:09.518509: step 1991, loss 0.00642987, acc 1\n",
      "2016-08-12T19:02:09.591016: step 1992, loss 0.0172695, acc 1\n",
      "2016-08-12T19:02:09.663627: step 1993, loss 0.0272448, acc 1\n",
      "2016-08-12T19:02:09.735011: step 1994, loss 0.0268225, acc 1\n",
      "2016-08-12T19:02:09.784390: step 1995, loss 0.0150602, acc 1\n",
      "2016-08-12T19:02:09.857606: step 1996, loss 0.0385578, acc 0.984375\n",
      "2016-08-12T19:02:09.931077: step 1997, loss 0.00844433, acc 1\n",
      "2016-08-12T19:02:10.004075: step 1998, loss 0.00819285, acc 1\n",
      "2016-08-12T19:02:10.076752: step 1999, loss 0.00802357, acc 1\n",
      "2016-08-12T19:02:10.149689: step 2000, loss 0.0433517, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:10.458282: step 2000, loss 0.7881, acc 0.756\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2000\n",
      "\n",
      "2016-08-12T19:02:11.134272: step 2001, loss 0.0365123, acc 0.984375\n",
      "2016-08-12T19:02:11.208390: step 2002, loss 0.0125499, acc 1\n",
      "2016-08-12T19:02:11.281792: step 2003, loss 0.006576, acc 1\n",
      "2016-08-12T19:02:11.355081: step 2004, loss 0.0422311, acc 0.984375\n",
      "2016-08-12T19:02:11.428195: step 2005, loss 0.0522743, acc 0.984375\n",
      "2016-08-12T19:02:11.500200: step 2006, loss 0.0219064, acc 1\n",
      "2016-08-12T19:02:11.573708: step 2007, loss 0.0380243, acc 0.984375\n",
      "2016-08-12T19:02:11.645793: step 2008, loss 0.019249, acc 1\n",
      "2016-08-12T19:02:11.719500: step 2009, loss 0.0230068, acc 1\n",
      "2016-08-12T19:02:11.772632: step 2010, loss 0.0118231, acc 1\n",
      "2016-08-12T19:02:11.846681: step 2011, loss 0.00424918, acc 1\n",
      "2016-08-12T19:02:11.919273: step 2012, loss 0.0175731, acc 1\n",
      "2016-08-12T19:02:11.993409: step 2013, loss 0.0172884, acc 1\n",
      "2016-08-12T19:02:12.067871: step 2014, loss 0.0421453, acc 0.984375\n",
      "2016-08-12T19:02:12.141761: step 2015, loss 0.0468517, acc 0.984375\n",
      "2016-08-12T19:02:12.213658: step 2016, loss 0.066743, acc 0.96875\n",
      "2016-08-12T19:02:12.286071: step 2017, loss 0.0608375, acc 0.96875\n",
      "2016-08-12T19:02:12.359681: step 2018, loss 0.0123786, acc 1\n",
      "2016-08-12T19:02:12.432204: step 2019, loss 0.028761, acc 0.984375\n",
      "2016-08-12T19:02:12.506217: step 2020, loss 0.024864, acc 0.984375\n",
      "2016-08-12T19:02:12.579529: step 2021, loss 0.0221349, acc 1\n",
      "2016-08-12T19:02:12.653126: step 2022, loss 0.00586741, acc 1\n",
      "2016-08-12T19:02:12.725572: step 2023, loss 0.0237852, acc 1\n",
      "2016-08-12T19:02:12.800940: step 2024, loss 0.00968698, acc 1\n",
      "2016-08-12T19:02:12.851314: step 2025, loss 0.00426358, acc 1\n",
      "2016-08-12T19:02:12.924596: step 2026, loss 0.0217462, acc 0.984375\n",
      "2016-08-12T19:02:12.997275: step 2027, loss 0.0105303, acc 1\n",
      "2016-08-12T19:02:13.068760: step 2028, loss 0.00656405, acc 1\n",
      "2016-08-12T19:02:13.140850: step 2029, loss 0.0394744, acc 0.96875\n",
      "2016-08-12T19:02:13.213752: step 2030, loss 0.0134515, acc 1\n",
      "2016-08-12T19:02:13.285525: step 2031, loss 0.0228548, acc 1\n",
      "2016-08-12T19:02:13.358245: step 2032, loss 0.00969023, acc 1\n",
      "2016-08-12T19:02:13.431211: step 2033, loss 0.0345615, acc 0.984375\n",
      "2016-08-12T19:02:13.506068: step 2034, loss 0.0504215, acc 0.984375\n",
      "2016-08-12T19:02:13.579643: step 2035, loss 0.0290615, acc 1\n",
      "2016-08-12T19:02:13.651733: step 2036, loss 0.00652638, acc 1\n",
      "2016-08-12T19:02:13.724223: step 2037, loss 0.0110215, acc 1\n",
      "2016-08-12T19:02:13.798706: step 2038, loss 0.0137602, acc 1\n",
      "2016-08-12T19:02:13.872546: step 2039, loss 0.00842842, acc 1\n",
      "2016-08-12T19:02:13.923692: step 2040, loss 0.0221342, acc 1\n",
      "2016-08-12T19:02:13.996739: step 2041, loss 0.0344696, acc 1\n",
      "2016-08-12T19:02:14.070098: step 2042, loss 0.0242345, acc 0.984375\n",
      "2016-08-12T19:02:14.143491: step 2043, loss 0.00922516, acc 1\n",
      "2016-08-12T19:02:14.217075: step 2044, loss 0.0180047, acc 1\n",
      "2016-08-12T19:02:14.290570: step 2045, loss 0.0354214, acc 1\n",
      "2016-08-12T19:02:14.364060: step 2046, loss 0.0359458, acc 0.984375\n",
      "2016-08-12T19:02:14.437016: step 2047, loss 0.0315328, acc 1\n",
      "2016-08-12T19:02:14.510062: step 2048, loss 0.0390861, acc 0.984375\n",
      "2016-08-12T19:02:14.582349: step 2049, loss 0.00798535, acc 1\n",
      "2016-08-12T19:02:14.655991: step 2050, loss 0.0103692, acc 1\n",
      "2016-08-12T19:02:14.728704: step 2051, loss 0.033995, acc 0.984375\n",
      "2016-08-12T19:02:14.802810: step 2052, loss 0.0149456, acc 1\n",
      "2016-08-12T19:02:14.876867: step 2053, loss 0.0518273, acc 0.984375\n",
      "2016-08-12T19:02:14.951745: step 2054, loss 0.025147, acc 0.984375\n",
      "2016-08-12T19:02:15.001810: step 2055, loss 0.0107725, acc 1\n",
      "2016-08-12T19:02:15.075233: step 2056, loss 0.0224182, acc 0.984375\n",
      "2016-08-12T19:02:15.149230: step 2057, loss 0.00769445, acc 1\n",
      "2016-08-12T19:02:15.222664: step 2058, loss 0.00894224, acc 1\n",
      "2016-08-12T19:02:15.295838: step 2059, loss 0.0126477, acc 1\n",
      "2016-08-12T19:02:15.367170: step 2060, loss 0.017094, acc 1\n",
      "2016-08-12T19:02:15.437014: step 2061, loss 0.0188479, acc 1\n",
      "2016-08-12T19:02:15.507398: step 2062, loss 0.00983111, acc 1\n",
      "2016-08-12T19:02:15.579815: step 2063, loss 0.0393326, acc 0.984375\n",
      "2016-08-12T19:02:15.654810: step 2064, loss 0.014631, acc 1\n",
      "2016-08-12T19:02:15.727716: step 2065, loss 0.0165294, acc 1\n",
      "2016-08-12T19:02:15.800349: step 2066, loss 0.0331396, acc 0.984375\n",
      "2016-08-12T19:02:15.874916: step 2067, loss 0.0394356, acc 0.984375\n",
      "2016-08-12T19:02:15.949362: step 2068, loss 0.0285655, acc 1\n",
      "2016-08-12T19:02:16.023982: step 2069, loss 0.0112208, acc 1\n",
      "2016-08-12T19:02:16.075222: step 2070, loss 0.0144496, acc 1\n",
      "2016-08-12T19:02:16.146602: step 2071, loss 0.0140085, acc 1\n",
      "2016-08-12T19:02:16.218133: step 2072, loss 0.0118213, acc 1\n",
      "2016-08-12T19:02:16.291297: step 2073, loss 0.015408, acc 1\n",
      "2016-08-12T19:02:16.365691: step 2074, loss 0.0541996, acc 0.984375\n",
      "2016-08-12T19:02:16.439425: step 2075, loss 0.0160899, acc 1\n",
      "2016-08-12T19:02:16.512008: step 2076, loss 0.0417161, acc 0.984375\n",
      "2016-08-12T19:02:16.585945: step 2077, loss 0.0502539, acc 0.984375\n",
      "2016-08-12T19:02:16.659852: step 2078, loss 0.0145245, acc 1\n",
      "2016-08-12T19:02:16.732751: step 2079, loss 0.00953078, acc 1\n",
      "2016-08-12T19:02:16.805655: step 2080, loss 0.0214994, acc 0.984375\n",
      "2016-08-12T19:02:16.879858: step 2081, loss 0.0160312, acc 1\n",
      "2016-08-12T19:02:16.953868: step 2082, loss 0.0115715, acc 1\n",
      "2016-08-12T19:02:17.027317: step 2083, loss 0.0512015, acc 0.984375\n",
      "2016-08-12T19:02:17.100828: step 2084, loss 0.0128102, acc 1\n",
      "2016-08-12T19:02:17.152454: step 2085, loss 0.0239163, acc 1\n",
      "2016-08-12T19:02:17.226963: step 2086, loss 0.0358214, acc 1\n",
      "2016-08-12T19:02:17.300795: step 2087, loss 0.0248351, acc 0.984375\n",
      "2016-08-12T19:02:17.375111: step 2088, loss 0.0320636, acc 0.984375\n",
      "2016-08-12T19:02:17.448218: step 2089, loss 0.0077925, acc 1\n",
      "2016-08-12T19:02:17.521713: step 2090, loss 0.0307894, acc 0.984375\n",
      "2016-08-12T19:02:17.596260: step 2091, loss 0.0128487, acc 1\n",
      "2016-08-12T19:02:17.671741: step 2092, loss 0.0549631, acc 0.984375\n",
      "2016-08-12T19:02:17.744449: step 2093, loss 0.0128349, acc 1\n",
      "2016-08-12T19:02:17.819555: step 2094, loss 0.00911802, acc 1\n",
      "2016-08-12T19:02:17.892744: step 2095, loss 0.012298, acc 1\n",
      "2016-08-12T19:02:17.964152: step 2096, loss 0.0326793, acc 1\n",
      "2016-08-12T19:02:18.037670: step 2097, loss 0.0226535, acc 1\n",
      "2016-08-12T19:02:18.111350: step 2098, loss 0.0473243, acc 0.984375\n",
      "2016-08-12T19:02:18.186801: step 2099, loss 0.00800952, acc 1\n",
      "2016-08-12T19:02:18.237445: step 2100, loss 0.0258354, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:18.545692: step 2100, loss 0.815545, acc 0.739\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2100\n",
      "\n",
      "2016-08-12T19:02:19.243199: step 2101, loss 0.0311061, acc 1\n",
      "2016-08-12T19:02:19.316643: step 2102, loss 0.0110038, acc 1\n",
      "2016-08-12T19:02:19.391335: step 2103, loss 0.0125871, acc 1\n",
      "2016-08-12T19:02:19.464062: step 2104, loss 0.0498264, acc 0.984375\n",
      "2016-08-12T19:02:19.536899: step 2105, loss 0.0195797, acc 1\n",
      "2016-08-12T19:02:19.608664: step 2106, loss 0.00763657, acc 1\n",
      "2016-08-12T19:02:19.683355: step 2107, loss 0.00888348, acc 1\n",
      "2016-08-12T19:02:19.755759: step 2108, loss 0.0235912, acc 0.984375\n",
      "2016-08-12T19:02:19.829974: step 2109, loss 0.0226173, acc 1\n",
      "2016-08-12T19:02:19.901713: step 2110, loss 0.00764351, acc 1\n",
      "2016-08-12T19:02:19.975428: step 2111, loss 0.00892367, acc 1\n",
      "2016-08-12T19:02:20.049282: step 2112, loss 0.0112028, acc 1\n",
      "2016-08-12T19:02:20.121538: step 2113, loss 0.0522198, acc 0.984375\n",
      "2016-08-12T19:02:20.198289: step 2114, loss 0.0161281, acc 1\n",
      "2016-08-12T19:02:20.249483: step 2115, loss 0.00183108, acc 1\n",
      "2016-08-12T19:02:20.322619: step 2116, loss 0.00908727, acc 1\n",
      "2016-08-12T19:02:20.396098: step 2117, loss 0.0127063, acc 1\n",
      "2016-08-12T19:02:20.468722: step 2118, loss 0.0200764, acc 1\n",
      "2016-08-12T19:02:20.542772: step 2119, loss 0.0645502, acc 0.984375\n",
      "2016-08-12T19:02:20.616129: step 2120, loss 0.0267054, acc 0.984375\n",
      "2016-08-12T19:02:20.689202: step 2121, loss 0.0523453, acc 0.984375\n",
      "2016-08-12T19:02:20.762530: step 2122, loss 0.0133924, acc 1\n",
      "2016-08-12T19:02:20.834675: step 2123, loss 0.0119143, acc 1\n",
      "2016-08-12T19:02:20.908513: step 2124, loss 0.0124924, acc 1\n",
      "2016-08-12T19:02:20.983042: step 2125, loss 0.0145329, acc 1\n",
      "2016-08-12T19:02:21.056377: step 2126, loss 0.0232745, acc 1\n",
      "2016-08-12T19:02:21.129644: step 2127, loss 0.103751, acc 0.984375\n",
      "2016-08-12T19:02:21.202699: step 2128, loss 0.0276903, acc 0.984375\n",
      "2016-08-12T19:02:21.275662: step 2129, loss 0.0128694, acc 1\n",
      "2016-08-12T19:02:21.329199: step 2130, loss 0.109702, acc 0.971429\n",
      "2016-08-12T19:02:21.403662: step 2131, loss 0.0131488, acc 1\n",
      "2016-08-12T19:02:21.476196: step 2132, loss 0.0243449, acc 0.984375\n",
      "2016-08-12T19:02:21.550365: step 2133, loss 0.0161452, acc 1\n",
      "2016-08-12T19:02:21.621991: step 2134, loss 0.00877719, acc 1\n",
      "2016-08-12T19:02:21.694344: step 2135, loss 0.0153501, acc 1\n",
      "2016-08-12T19:02:21.766620: step 2136, loss 0.0299682, acc 0.984375\n",
      "2016-08-12T19:02:21.839577: step 2137, loss 0.0340538, acc 0.96875\n",
      "2016-08-12T19:02:21.913000: step 2138, loss 0.0156491, acc 1\n",
      "2016-08-12T19:02:21.987613: step 2139, loss 0.0217781, acc 1\n",
      "2016-08-12T19:02:22.060882: step 2140, loss 0.00294504, acc 1\n",
      "2016-08-12T19:02:22.133118: step 2141, loss 0.0414647, acc 1\n",
      "2016-08-12T19:02:22.206654: step 2142, loss 0.023517, acc 1\n",
      "2016-08-12T19:02:22.278589: step 2143, loss 0.0290189, acc 1\n",
      "2016-08-12T19:02:22.351866: step 2144, loss 0.0186246, acc 1\n",
      "2016-08-12T19:02:22.402739: step 2145, loss 0.0212687, acc 1\n",
      "2016-08-12T19:02:22.476031: step 2146, loss 0.00826895, acc 1\n",
      "2016-08-12T19:02:22.548562: step 2147, loss 0.0269535, acc 1\n",
      "2016-08-12T19:02:22.621479: step 2148, loss 0.0113113, acc 1\n",
      "2016-08-12T19:02:22.695105: step 2149, loss 0.0133811, acc 1\n",
      "2016-08-12T19:02:22.770108: step 2150, loss 0.00553807, acc 1\n",
      "2016-08-12T19:02:22.842984: step 2151, loss 0.0146955, acc 1\n",
      "2016-08-12T19:02:22.915303: step 2152, loss 0.0127299, acc 1\n",
      "2016-08-12T19:02:22.989892: step 2153, loss 0.0162532, acc 0.984375\n",
      "2016-08-12T19:02:23.064319: step 2154, loss 0.0215156, acc 1\n",
      "2016-08-12T19:02:23.138622: step 2155, loss 0.0181815, acc 1\n",
      "2016-08-12T19:02:23.212386: step 2156, loss 0.0219447, acc 0.984375\n",
      "2016-08-12T19:02:23.285539: step 2157, loss 0.0173961, acc 1\n",
      "2016-08-12T19:02:23.359167: step 2158, loss 0.064355, acc 0.96875\n",
      "2016-08-12T19:02:23.432676: step 2159, loss 0.0227158, acc 1\n",
      "2016-08-12T19:02:23.484630: step 2160, loss 0.02588, acc 1\n",
      "2016-08-12T19:02:23.559567: step 2161, loss 0.0237436, acc 0.984375\n",
      "2016-08-12T19:02:23.630044: step 2162, loss 0.0132668, acc 1\n",
      "2016-08-12T19:02:23.702593: step 2163, loss 0.0176986, acc 1\n",
      "2016-08-12T19:02:23.775254: step 2164, loss 0.0461368, acc 0.984375\n",
      "2016-08-12T19:02:23.848532: step 2165, loss 0.0400621, acc 0.984375\n",
      "2016-08-12T19:02:23.923145: step 2166, loss 0.0176666, acc 1\n",
      "2016-08-12T19:02:23.995327: step 2167, loss 0.034559, acc 0.984375\n",
      "2016-08-12T19:02:24.068752: step 2168, loss 0.0120855, acc 1\n",
      "2016-08-12T19:02:24.142286: step 2169, loss 0.0357414, acc 0.984375\n",
      "2016-08-12T19:02:24.215699: step 2170, loss 0.0191787, acc 1\n",
      "2016-08-12T19:02:24.290571: step 2171, loss 0.149535, acc 0.984375\n",
      "2016-08-12T19:02:24.363531: step 2172, loss 0.0130132, acc 1\n",
      "2016-08-12T19:02:24.437319: step 2173, loss 0.0231961, acc 1\n",
      "2016-08-12T19:02:24.510628: step 2174, loss 0.0122219, acc 1\n",
      "2016-08-12T19:02:24.563462: step 2175, loss 0.0078531, acc 1\n",
      "2016-08-12T19:02:24.637457: step 2176, loss 0.0264687, acc 1\n",
      "2016-08-12T19:02:24.711355: step 2177, loss 0.010565, acc 1\n",
      "2016-08-12T19:02:24.783959: step 2178, loss 0.0128183, acc 1\n",
      "2016-08-12T19:02:24.857460: step 2179, loss 0.0275836, acc 0.984375\n",
      "2016-08-12T19:02:24.929667: step 2180, loss 0.00837715, acc 1\n",
      "2016-08-12T19:02:25.002347: step 2181, loss 0.0298686, acc 0.984375\n",
      "2016-08-12T19:02:25.079171: step 2182, loss 0.011981, acc 1\n",
      "2016-08-12T19:02:25.152640: step 2183, loss 0.0099649, acc 1\n",
      "2016-08-12T19:02:25.225999: step 2184, loss 0.0173135, acc 1\n",
      "2016-08-12T19:02:25.299644: step 2185, loss 0.0123199, acc 1\n",
      "2016-08-12T19:02:25.372875: step 2186, loss 0.0156201, acc 1\n",
      "2016-08-12T19:02:25.444922: step 2187, loss 0.00995222, acc 1\n",
      "2016-08-12T19:02:25.517579: step 2188, loss 0.0120251, acc 1\n",
      "2016-08-12T19:02:25.592080: step 2189, loss 0.00362988, acc 1\n",
      "2016-08-12T19:02:25.642044: step 2190, loss 0.0240089, acc 1\n",
      "2016-08-12T19:02:25.712688: step 2191, loss 0.00549707, acc 1\n",
      "2016-08-12T19:02:25.785058: step 2192, loss 0.00723256, acc 1\n",
      "2016-08-12T19:02:25.858196: step 2193, loss 0.00818106, acc 1\n",
      "2016-08-12T19:02:25.931572: step 2194, loss 0.0178555, acc 1\n",
      "2016-08-12T19:02:26.004753: step 2195, loss 0.0152485, acc 1\n",
      "2016-08-12T19:02:26.078147: step 2196, loss 0.00953861, acc 1\n",
      "2016-08-12T19:02:26.151938: step 2197, loss 0.021654, acc 0.984375\n",
      "2016-08-12T19:02:26.231372: step 2198, loss 0.0156384, acc 1\n",
      "2016-08-12T19:02:26.305647: step 2199, loss 0.0241274, acc 0.984375\n",
      "2016-08-12T19:02:26.381607: step 2200, loss 0.00329686, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:26.692516: step 2200, loss 0.810765, acc 0.753\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2200\n",
      "\n",
      "2016-08-12T19:02:27.379563: step 2201, loss 0.00854876, acc 1\n",
      "2016-08-12T19:02:27.454269: step 2202, loss 0.0257518, acc 0.984375\n",
      "2016-08-12T19:02:27.527312: step 2203, loss 0.024851, acc 1\n",
      "2016-08-12T19:02:27.599736: step 2204, loss 0.0561021, acc 0.984375\n",
      "2016-08-12T19:02:27.649018: step 2205, loss 0.0295269, acc 1\n",
      "2016-08-12T19:02:27.724609: step 2206, loss 0.0115045, acc 1\n",
      "2016-08-12T19:02:27.798745: step 2207, loss 0.0119222, acc 1\n",
      "2016-08-12T19:02:27.873651: step 2208, loss 0.00382559, acc 1\n",
      "2016-08-12T19:02:27.947013: step 2209, loss 0.0114861, acc 1\n",
      "2016-08-12T19:02:28.021642: step 2210, loss 0.0109878, acc 1\n",
      "2016-08-12T19:02:28.096293: step 2211, loss 0.00618246, acc 1\n",
      "2016-08-12T19:02:28.168918: step 2212, loss 0.0190222, acc 1\n",
      "2016-08-12T19:02:28.242330: step 2213, loss 0.0231394, acc 0.984375\n",
      "2016-08-12T19:02:28.316210: step 2214, loss 0.0043482, acc 1\n",
      "2016-08-12T19:02:28.390311: step 2215, loss 0.0358633, acc 0.984375\n",
      "2016-08-12T19:02:28.463243: step 2216, loss 0.0193087, acc 1\n",
      "2016-08-12T19:02:28.536862: step 2217, loss 0.0303006, acc 0.984375\n",
      "2016-08-12T19:02:28.610334: step 2218, loss 0.0227777, acc 1\n",
      "2016-08-12T19:02:28.683899: step 2219, loss 0.025986, acc 0.984375\n",
      "2016-08-12T19:02:28.736218: step 2220, loss 0.0249824, acc 1\n",
      "2016-08-12T19:02:28.810070: step 2221, loss 0.00242857, acc 1\n",
      "2016-08-12T19:02:28.880660: step 2222, loss 0.0564816, acc 0.984375\n",
      "2016-08-12T19:02:28.955286: step 2223, loss 0.0137983, acc 1\n",
      "2016-08-12T19:02:29.028791: step 2224, loss 0.0162263, acc 1\n",
      "2016-08-12T19:02:29.101136: step 2225, loss 0.0173499, acc 1\n",
      "2016-08-12T19:02:29.172228: step 2226, loss 0.0389832, acc 0.984375\n",
      "2016-08-12T19:02:29.245222: step 2227, loss 0.00541083, acc 1\n",
      "2016-08-12T19:02:29.318522: step 2228, loss 0.0292403, acc 0.984375\n",
      "2016-08-12T19:02:29.391895: step 2229, loss 0.0234966, acc 0.984375\n",
      "2016-08-12T19:02:29.465365: step 2230, loss 0.0244093, acc 1\n",
      "2016-08-12T19:02:29.538776: step 2231, loss 0.0185728, acc 1\n",
      "2016-08-12T19:02:29.612067: step 2232, loss 0.0404272, acc 0.984375\n",
      "2016-08-12T19:02:29.686528: step 2233, loss 0.0184004, acc 1\n",
      "2016-08-12T19:02:29.761625: step 2234, loss 0.0445741, acc 0.984375\n",
      "2016-08-12T19:02:29.811678: step 2235, loss 0.0111988, acc 1\n",
      "2016-08-12T19:02:29.883265: step 2236, loss 0.0120271, acc 1\n",
      "2016-08-12T19:02:29.956825: step 2237, loss 0.00986162, acc 1\n",
      "2016-08-12T19:02:30.030592: step 2238, loss 0.0149885, acc 1\n",
      "2016-08-12T19:02:30.104441: step 2239, loss 0.0130218, acc 1\n",
      "2016-08-12T19:02:30.178685: step 2240, loss 0.0369911, acc 0.984375\n",
      "2016-08-12T19:02:30.254088: step 2241, loss 0.0134888, acc 1\n",
      "2016-08-12T19:02:30.325409: step 2242, loss 0.0159958, acc 1\n",
      "2016-08-12T19:02:30.398287: step 2243, loss 0.0145751, acc 1\n",
      "2016-08-12T19:02:30.472335: step 2244, loss 0.0163291, acc 1\n",
      "2016-08-12T19:02:30.546220: step 2245, loss 0.0184887, acc 1\n",
      "2016-08-12T19:02:30.620496: step 2246, loss 0.0547085, acc 0.96875\n",
      "2016-08-12T19:02:30.694328: step 2247, loss 0.0369773, acc 1\n",
      "2016-08-12T19:02:30.767528: step 2248, loss 0.0202185, acc 1\n",
      "2016-08-12T19:02:30.842816: step 2249, loss 0.0103415, acc 1\n",
      "2016-08-12T19:02:30.892941: step 2250, loss 0.00604106, acc 1\n",
      "2016-08-12T19:02:30.966961: step 2251, loss 0.0900129, acc 0.984375\n",
      "2016-08-12T19:02:31.040673: step 2252, loss 0.0158776, acc 1\n",
      "2016-08-12T19:02:31.113677: step 2253, loss 0.0194692, acc 1\n",
      "2016-08-12T19:02:31.189848: step 2254, loss 0.0142772, acc 1\n",
      "2016-08-12T19:02:31.264461: step 2255, loss 0.0385941, acc 0.984375\n",
      "2016-08-12T19:02:31.339150: step 2256, loss 0.0107911, acc 1\n",
      "2016-08-12T19:02:31.415340: step 2257, loss 0.0306425, acc 0.984375\n",
      "2016-08-12T19:02:31.489077: step 2258, loss 0.0414262, acc 0.984375\n",
      "2016-08-12T19:02:31.564473: step 2259, loss 0.0161092, acc 1\n",
      "2016-08-12T19:02:31.637864: step 2260, loss 0.0207835, acc 1\n",
      "2016-08-12T19:02:31.712536: step 2261, loss 0.011406, acc 1\n",
      "2016-08-12T19:02:31.787271: step 2262, loss 0.0250524, acc 0.984375\n",
      "2016-08-12T19:02:31.861670: step 2263, loss 0.0195874, acc 1\n",
      "2016-08-12T19:02:31.936268: step 2264, loss 0.0160718, acc 1\n",
      "2016-08-12T19:02:31.986173: step 2265, loss 0.00445819, acc 1\n",
      "2016-08-12T19:02:32.061428: step 2266, loss 0.0538818, acc 0.984375\n",
      "2016-08-12T19:02:32.135663: step 2267, loss 0.0134858, acc 1\n",
      "2016-08-12T19:02:32.209160: step 2268, loss 0.0788492, acc 0.96875\n",
      "2016-08-12T19:02:32.283071: step 2269, loss 0.00562083, acc 1\n",
      "2016-08-12T19:02:32.357265: step 2270, loss 0.0102067, acc 1\n",
      "2016-08-12T19:02:32.432318: step 2271, loss 0.01772, acc 1\n",
      "2016-08-12T19:02:32.506331: step 2272, loss 0.0188215, acc 1\n",
      "2016-08-12T19:02:32.578985: step 2273, loss 0.0102842, acc 1\n",
      "2016-08-12T19:02:32.649092: step 2274, loss 0.0270481, acc 1\n",
      "2016-08-12T19:02:32.722369: step 2275, loss 0.0205283, acc 1\n",
      "2016-08-12T19:02:32.795077: step 2276, loss 0.0157308, acc 1\n",
      "2016-08-12T19:02:32.868409: step 2277, loss 0.0268596, acc 0.96875\n",
      "2016-08-12T19:02:32.941789: step 2278, loss 0.0115766, acc 1\n",
      "2016-08-12T19:02:33.015291: step 2279, loss 0.0166627, acc 1\n",
      "2016-08-12T19:02:33.064128: step 2280, loss 0.00944458, acc 1\n",
      "2016-08-12T19:02:33.140778: step 2281, loss 0.0235755, acc 0.984375\n",
      "2016-08-12T19:02:33.213759: step 2282, loss 0.0484222, acc 0.984375\n",
      "2016-08-12T19:02:33.288200: step 2283, loss 0.0243608, acc 1\n",
      "2016-08-12T19:02:33.369020: step 2284, loss 0.00514661, acc 1\n",
      "2016-08-12T19:02:33.442076: step 2285, loss 0.0188265, acc 1\n",
      "2016-08-12T19:02:33.514918: step 2286, loss 0.0215125, acc 0.984375\n",
      "2016-08-12T19:02:33.589502: step 2287, loss 0.00636048, acc 1\n",
      "2016-08-12T19:02:33.664043: step 2288, loss 0.0118411, acc 1\n",
      "2016-08-12T19:02:33.738257: step 2289, loss 0.0108686, acc 1\n",
      "2016-08-12T19:02:33.811395: step 2290, loss 0.0190407, acc 1\n",
      "2016-08-12T19:02:33.887449: step 2291, loss 0.00771067, acc 1\n",
      "2016-08-12T19:02:33.960409: step 2292, loss 0.0331172, acc 0.984375\n",
      "2016-08-12T19:02:34.034526: step 2293, loss 0.0054392, acc 1\n",
      "2016-08-12T19:02:34.108735: step 2294, loss 0.0269004, acc 1\n",
      "2016-08-12T19:02:34.157882: step 2295, loss 0.00186461, acc 1\n",
      "2016-08-12T19:02:34.228851: step 2296, loss 0.00720853, acc 1\n",
      "2016-08-12T19:02:34.302498: step 2297, loss 0.0189553, acc 1\n",
      "2016-08-12T19:02:34.375077: step 2298, loss 0.0129303, acc 1\n",
      "2016-08-12T19:02:34.446263: step 2299, loss 0.0131184, acc 1\n",
      "2016-08-12T19:02:34.519776: step 2300, loss 0.00940932, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:34.827859: step 2300, loss 0.820786, acc 0.743\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2300\n",
      "\n",
      "2016-08-12T19:02:35.509970: step 2301, loss 0.011306, acc 1\n",
      "2016-08-12T19:02:35.583020: step 2302, loss 0.0131108, acc 1\n",
      "2016-08-12T19:02:35.658233: step 2303, loss 0.0297237, acc 1\n",
      "2016-08-12T19:02:35.731134: step 2304, loss 0.00469253, acc 1\n",
      "2016-08-12T19:02:35.804622: step 2305, loss 0.0131513, acc 1\n",
      "2016-08-12T19:02:35.874890: step 2306, loss 0.00801078, acc 1\n",
      "2016-08-12T19:02:35.946806: step 2307, loss 0.0143241, acc 1\n",
      "2016-08-12T19:02:36.020362: step 2308, loss 0.029343, acc 1\n",
      "2016-08-12T19:02:36.092989: step 2309, loss 0.0333914, acc 0.984375\n",
      "2016-08-12T19:02:36.143558: step 2310, loss 0.00513025, acc 1\n",
      "2016-08-12T19:02:36.216379: step 2311, loss 0.0109318, acc 1\n",
      "2016-08-12T19:02:36.290478: step 2312, loss 0.018004, acc 1\n",
      "2016-08-12T19:02:36.365204: step 2313, loss 0.0189993, acc 1\n",
      "2016-08-12T19:02:36.437126: step 2314, loss 0.0304047, acc 0.984375\n",
      "2016-08-12T19:02:36.510373: step 2315, loss 0.0217427, acc 1\n",
      "2016-08-12T19:02:36.583555: step 2316, loss 0.017181, acc 1\n",
      "2016-08-12T19:02:36.657147: step 2317, loss 0.00528196, acc 1\n",
      "2016-08-12T19:02:36.729149: step 2318, loss 0.0213998, acc 1\n",
      "2016-08-12T19:02:36.802971: step 2319, loss 0.0224513, acc 1\n",
      "2016-08-12T19:02:36.876900: step 2320, loss 0.100426, acc 0.953125\n",
      "2016-08-12T19:02:36.951235: step 2321, loss 0.0186553, acc 1\n",
      "2016-08-12T19:02:37.023801: step 2322, loss 0.0540213, acc 0.984375\n",
      "2016-08-12T19:02:37.100967: step 2323, loss 0.0122244, acc 1\n",
      "2016-08-12T19:02:37.173851: step 2324, loss 0.00748865, acc 1\n",
      "2016-08-12T19:02:37.221624: step 2325, loss 0.0141864, acc 1\n",
      "2016-08-12T19:02:37.296377: step 2326, loss 0.0219906, acc 1\n",
      "2016-08-12T19:02:37.370174: step 2327, loss 0.0345168, acc 0.984375\n",
      "2016-08-12T19:02:37.444625: step 2328, loss 0.00654851, acc 1\n",
      "2016-08-12T19:02:37.518369: step 2329, loss 0.021356, acc 1\n",
      "2016-08-12T19:02:37.592469: step 2330, loss 0.0115287, acc 1\n",
      "2016-08-12T19:02:37.666168: step 2331, loss 0.00667943, acc 1\n",
      "2016-08-12T19:02:37.737316: step 2332, loss 0.0142198, acc 1\n",
      "2016-08-12T19:02:37.811740: step 2333, loss 0.0358632, acc 0.984375\n",
      "2016-08-12T19:02:37.887246: step 2334, loss 0.0228104, acc 0.984375\n",
      "2016-08-12T19:02:37.960526: step 2335, loss 0.00926519, acc 1\n",
      "2016-08-12T19:02:38.033452: step 2336, loss 0.00808627, acc 1\n",
      "2016-08-12T19:02:38.108386: step 2337, loss 0.00691967, acc 1\n",
      "2016-08-12T19:02:38.183709: step 2338, loss 0.0266329, acc 1\n",
      "2016-08-12T19:02:38.259868: step 2339, loss 0.0183297, acc 1\n",
      "2016-08-12T19:02:38.307538: step 2340, loss 0.0213121, acc 1\n",
      "2016-08-12T19:02:38.380312: step 2341, loss 0.020268, acc 1\n",
      "2016-08-12T19:02:38.452256: step 2342, loss 0.016793, acc 1\n",
      "2016-08-12T19:02:38.526175: step 2343, loss 0.0199064, acc 0.984375\n",
      "2016-08-12T19:02:38.600484: step 2344, loss 0.00962611, acc 1\n",
      "2016-08-12T19:02:38.676550: step 2345, loss 0.00222847, acc 1\n",
      "2016-08-12T19:02:38.747596: step 2346, loss 0.00362138, acc 1\n",
      "2016-08-12T19:02:38.821583: step 2347, loss 0.0202017, acc 1\n",
      "2016-08-12T19:02:38.895836: step 2348, loss 0.0116197, acc 1\n",
      "2016-08-12T19:02:38.968002: step 2349, loss 0.00742782, acc 1\n",
      "2016-08-12T19:02:39.042686: step 2350, loss 0.0209551, acc 1\n",
      "2016-08-12T19:02:39.117783: step 2351, loss 0.0100997, acc 1\n",
      "2016-08-12T19:02:39.191132: step 2352, loss 0.0231944, acc 0.984375\n",
      "2016-08-12T19:02:39.264656: step 2353, loss 0.0131853, acc 1\n",
      "2016-08-12T19:02:39.336248: step 2354, loss 0.0229209, acc 1\n",
      "2016-08-12T19:02:39.385216: step 2355, loss 0.00574919, acc 1\n",
      "2016-08-12T19:02:39.459869: step 2356, loss 0.013946, acc 1\n",
      "2016-08-12T19:02:39.533717: step 2357, loss 0.00621383, acc 1\n",
      "2016-08-12T19:02:39.606859: step 2358, loss 0.0137343, acc 1\n",
      "2016-08-12T19:02:39.678509: step 2359, loss 0.0120362, acc 1\n",
      "2016-08-12T19:02:39.752868: step 2360, loss 0.0109923, acc 1\n",
      "2016-08-12T19:02:39.827078: step 2361, loss 0.0163048, acc 1\n",
      "2016-08-12T19:02:39.898957: step 2362, loss 0.0135424, acc 1\n",
      "2016-08-12T19:02:39.972579: step 2363, loss 0.037991, acc 0.984375\n",
      "2016-08-12T19:02:40.046715: step 2364, loss 0.0264406, acc 0.984375\n",
      "2016-08-12T19:02:40.130108: step 2365, loss 0.00300616, acc 1\n",
      "2016-08-12T19:02:40.203179: step 2366, loss 0.0230881, acc 1\n",
      "2016-08-12T19:02:40.278788: step 2367, loss 0.00849524, acc 1\n",
      "2016-08-12T19:02:40.352614: step 2368, loss 0.00771596, acc 1\n",
      "2016-08-12T19:02:40.424713: step 2369, loss 0.00825895, acc 1\n",
      "2016-08-12T19:02:40.477133: step 2370, loss 0.0133715, acc 1\n",
      "2016-08-12T19:02:40.552344: step 2371, loss 0.00177657, acc 1\n",
      "2016-08-12T19:02:40.625275: step 2372, loss 0.0211864, acc 1\n",
      "2016-08-12T19:02:40.699329: step 2373, loss 0.00602945, acc 1\n",
      "2016-08-12T19:02:40.774318: step 2374, loss 0.0217111, acc 0.984375\n",
      "2016-08-12T19:02:40.845981: step 2375, loss 0.00887821, acc 1\n",
      "2016-08-12T19:02:40.918422: step 2376, loss 0.0169695, acc 1\n",
      "2016-08-12T19:02:40.989677: step 2377, loss 0.0374258, acc 0.984375\n",
      "2016-08-12T19:02:41.064395: step 2378, loss 0.0260818, acc 1\n",
      "2016-08-12T19:02:41.136925: step 2379, loss 0.0064268, acc 1\n",
      "2016-08-12T19:02:41.208802: step 2380, loss 0.00908567, acc 1\n",
      "2016-08-12T19:02:41.280542: step 2381, loss 0.0604504, acc 0.984375\n",
      "2016-08-12T19:02:41.356092: step 2382, loss 0.0167413, acc 1\n",
      "2016-08-12T19:02:41.430609: step 2383, loss 0.0206665, acc 0.984375\n",
      "2016-08-12T19:02:41.504100: step 2384, loss 0.0144885, acc 1\n",
      "2016-08-12T19:02:41.557017: step 2385, loss 0.0181811, acc 1\n",
      "2016-08-12T19:02:41.633146: step 2386, loss 0.0340805, acc 0.96875\n",
      "2016-08-12T19:02:41.706671: step 2387, loss 0.0239539, acc 1\n",
      "2016-08-12T19:02:41.779433: step 2388, loss 0.00888284, acc 1\n",
      "2016-08-12T19:02:41.852076: step 2389, loss 0.0153841, acc 1\n",
      "2016-08-12T19:02:41.928700: step 2390, loss 0.0159655, acc 1\n",
      "2016-08-12T19:02:42.003633: step 2391, loss 0.0121098, acc 1\n",
      "2016-08-12T19:02:42.077863: step 2392, loss 0.0073336, acc 1\n",
      "2016-08-12T19:02:42.151744: step 2393, loss 0.0154853, acc 1\n",
      "2016-08-12T19:02:42.226564: step 2394, loss 0.00679502, acc 1\n",
      "2016-08-12T19:02:42.302036: step 2395, loss 0.0315724, acc 0.984375\n",
      "2016-08-12T19:02:42.374212: step 2396, loss 0.00667599, acc 1\n",
      "2016-08-12T19:02:42.448078: step 2397, loss 0.0223302, acc 1\n",
      "2016-08-12T19:02:42.527651: step 2398, loss 0.0098022, acc 1\n",
      "2016-08-12T19:02:42.603247: step 2399, loss 0.00536036, acc 1\n",
      "2016-08-12T19:02:42.659010: step 2400, loss 0.0370135, acc 0.971429\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:42.968883: step 2400, loss 0.886054, acc 0.739\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2400\n",
      "\n",
      "2016-08-12T19:02:43.646705: step 2401, loss 0.0148013, acc 0.984375\n",
      "2016-08-12T19:02:43.719518: step 2402, loss 0.00656062, acc 1\n",
      "2016-08-12T19:02:43.796987: step 2403, loss 0.013102, acc 1\n",
      "2016-08-12T19:02:43.874219: step 2404, loss 0.0105653, acc 1\n",
      "2016-08-12T19:02:43.953403: step 2405, loss 0.00996369, acc 1\n",
      "2016-08-12T19:02:44.030932: step 2406, loss 0.00944044, acc 1\n",
      "2016-08-12T19:02:44.105311: step 2407, loss 0.0192951, acc 1\n",
      "2016-08-12T19:02:44.178839: step 2408, loss 0.0126152, acc 1\n",
      "2016-08-12T19:02:44.253092: step 2409, loss 0.0291229, acc 1\n",
      "2016-08-12T19:02:44.328286: step 2410, loss 0.0107164, acc 1\n",
      "2016-08-12T19:02:44.400696: step 2411, loss 0.0272179, acc 1\n",
      "2016-08-12T19:02:44.473443: step 2412, loss 0.0124093, acc 1\n",
      "2016-08-12T19:02:44.546442: step 2413, loss 0.00179494, acc 1\n",
      "2016-08-12T19:02:44.619079: step 2414, loss 0.0191241, acc 1\n",
      "2016-08-12T19:02:44.672971: step 2415, loss 0.0502204, acc 0.971429\n",
      "2016-08-12T19:02:44.749004: step 2416, loss 0.00869186, acc 1\n",
      "2016-08-12T19:02:44.823322: step 2417, loss 0.0130883, acc 1\n",
      "2016-08-12T19:02:44.896094: step 2418, loss 0.0157844, acc 1\n",
      "2016-08-12T19:02:44.969080: step 2419, loss 0.0800301, acc 0.984375\n",
      "2016-08-12T19:02:45.045618: step 2420, loss 0.00849904, acc 1\n",
      "2016-08-12T19:02:45.119685: step 2421, loss 0.0303826, acc 1\n",
      "2016-08-12T19:02:45.193657: step 2422, loss 0.01249, acc 1\n",
      "2016-08-12T19:02:45.265703: step 2423, loss 0.00672357, acc 1\n",
      "2016-08-12T19:02:45.339967: step 2424, loss 0.0103817, acc 1\n",
      "2016-08-12T19:02:45.412617: step 2425, loss 0.00886654, acc 1\n",
      "2016-08-12T19:02:45.485387: step 2426, loss 0.0120601, acc 1\n",
      "2016-08-12T19:02:45.558662: step 2427, loss 0.0381457, acc 0.984375\n",
      "2016-08-12T19:02:45.633529: step 2428, loss 0.0125254, acc 1\n",
      "2016-08-12T19:02:45.706104: step 2429, loss 0.0162677, acc 1\n",
      "2016-08-12T19:02:45.756700: step 2430, loss 0.00773427, acc 1\n",
      "2016-08-12T19:02:45.831737: step 2431, loss 0.0189975, acc 1\n",
      "2016-08-12T19:02:45.905404: step 2432, loss 0.0152076, acc 1\n",
      "2016-08-12T19:02:45.980792: step 2433, loss 0.0184704, acc 1\n",
      "2016-08-12T19:02:46.054666: step 2434, loss 0.0091517, acc 1\n",
      "2016-08-12T19:02:46.127707: step 2435, loss 0.0128497, acc 1\n",
      "2016-08-12T19:02:46.198908: step 2436, loss 0.0194938, acc 1\n",
      "2016-08-12T19:02:46.271500: step 2437, loss 0.00571687, acc 1\n",
      "2016-08-12T19:02:46.345295: step 2438, loss 0.0127146, acc 1\n",
      "2016-08-12T19:02:46.420342: step 2439, loss 0.0216111, acc 0.984375\n",
      "2016-08-12T19:02:46.494161: step 2440, loss 0.0138626, acc 1\n",
      "2016-08-12T19:02:46.568431: step 2441, loss 0.013197, acc 1\n",
      "2016-08-12T19:02:46.642545: step 2442, loss 0.00630989, acc 1\n",
      "2016-08-12T19:02:46.716193: step 2443, loss 0.0188787, acc 1\n",
      "2016-08-12T19:02:46.788811: step 2444, loss 0.0195419, acc 1\n",
      "2016-08-12T19:02:46.840405: step 2445, loss 0.014593, acc 1\n",
      "2016-08-12T19:02:46.913323: step 2446, loss 0.0210832, acc 1\n",
      "2016-08-12T19:02:46.987262: step 2447, loss 0.0347885, acc 1\n",
      "2016-08-12T19:02:47.063051: step 2448, loss 0.00646119, acc 1\n",
      "2016-08-12T19:02:47.135490: step 2449, loss 0.033093, acc 0.984375\n",
      "2016-08-12T19:02:47.209667: step 2450, loss 0.00646139, acc 1\n",
      "2016-08-12T19:02:47.283114: step 2451, loss 0.0209597, acc 1\n",
      "2016-08-12T19:02:47.357748: step 2452, loss 0.0335605, acc 0.984375\n",
      "2016-08-12T19:02:47.429593: step 2453, loss 0.00554717, acc 1\n",
      "2016-08-12T19:02:47.503501: step 2454, loss 0.0146592, acc 1\n",
      "2016-08-12T19:02:47.575880: step 2455, loss 0.00422157, acc 1\n",
      "2016-08-12T19:02:47.650137: step 2456, loss 0.00932923, acc 1\n",
      "2016-08-12T19:02:47.723296: step 2457, loss 0.0158709, acc 1\n",
      "2016-08-12T19:02:47.795278: step 2458, loss 0.0184759, acc 1\n",
      "2016-08-12T19:02:47.869006: step 2459, loss 0.00526319, acc 1\n",
      "2016-08-12T19:02:47.918523: step 2460, loss 0.003289, acc 1\n",
      "2016-08-12T19:02:47.990977: step 2461, loss 0.0113267, acc 1\n",
      "2016-08-12T19:02:48.063473: step 2462, loss 0.0104236, acc 1\n",
      "2016-08-12T19:02:48.137153: step 2463, loss 0.0214, acc 1\n",
      "2016-08-12T19:02:48.210787: step 2464, loss 0.0307319, acc 0.984375\n",
      "2016-08-12T19:02:48.286341: step 2465, loss 0.0384313, acc 0.984375\n",
      "2016-08-12T19:02:48.360276: step 2466, loss 0.0100873, acc 1\n",
      "2016-08-12T19:02:48.431370: step 2467, loss 0.0107054, acc 1\n",
      "2016-08-12T19:02:48.504808: step 2468, loss 0.0102121, acc 1\n",
      "2016-08-12T19:02:48.577349: step 2469, loss 0.0253337, acc 0.984375\n",
      "2016-08-12T19:02:48.652332: step 2470, loss 0.0256852, acc 0.984375\n",
      "2016-08-12T19:02:48.726144: step 2471, loss 0.0128315, acc 1\n",
      "2016-08-12T19:02:48.800405: step 2472, loss 0.040942, acc 0.984375\n",
      "2016-08-12T19:02:48.874385: step 2473, loss 0.00436834, acc 1\n",
      "2016-08-12T19:02:48.948449: step 2474, loss 0.00590137, acc 1\n",
      "2016-08-12T19:02:48.999284: step 2475, loss 0.00142017, acc 1\n",
      "2016-08-12T19:02:49.071287: step 2476, loss 0.00238428, acc 1\n",
      "2016-08-12T19:02:49.144112: step 2477, loss 0.0459714, acc 0.984375\n",
      "2016-08-12T19:02:49.217385: step 2478, loss 0.0180392, acc 0.984375\n",
      "2016-08-12T19:02:49.290065: step 2479, loss 0.00830464, acc 1\n",
      "2016-08-12T19:02:49.364548: step 2480, loss 0.00637814, acc 1\n",
      "2016-08-12T19:02:49.437340: step 2481, loss 0.00434694, acc 1\n",
      "2016-08-12T19:02:49.510786: step 2482, loss 0.0123047, acc 1\n",
      "2016-08-12T19:02:49.585381: step 2483, loss 0.0126503, acc 1\n",
      "2016-08-12T19:02:49.657398: step 2484, loss 0.00699017, acc 1\n",
      "2016-08-12T19:02:49.730529: step 2485, loss 0.0223053, acc 1\n",
      "2016-08-12T19:02:49.804593: step 2486, loss 0.0304199, acc 0.984375\n",
      "2016-08-12T19:02:49.878243: step 2487, loss 0.0176803, acc 1\n",
      "2016-08-12T19:02:49.950957: step 2488, loss 0.0109476, acc 1\n",
      "2016-08-12T19:02:50.024747: step 2489, loss 0.0199073, acc 1\n",
      "2016-08-12T19:02:50.074806: step 2490, loss 0.0127484, acc 1\n",
      "2016-08-12T19:02:50.147327: step 2491, loss 0.00362131, acc 1\n",
      "2016-08-12T19:02:50.219566: step 2492, loss 0.0126545, acc 1\n",
      "2016-08-12T19:02:50.293624: step 2493, loss 0.0089732, acc 1\n",
      "2016-08-12T19:02:50.365922: step 2494, loss 0.0096776, acc 1\n",
      "2016-08-12T19:02:50.437947: step 2495, loss 0.00335683, acc 1\n",
      "2016-08-12T19:02:50.510234: step 2496, loss 0.0128385, acc 1\n",
      "2016-08-12T19:02:50.583758: step 2497, loss 0.00453221, acc 1\n",
      "2016-08-12T19:02:50.657625: step 2498, loss 0.00718243, acc 1\n",
      "2016-08-12T19:02:50.730428: step 2499, loss 0.0218894, acc 1\n",
      "2016-08-12T19:02:50.802532: step 2500, loss 0.0422253, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:51.112040: step 2500, loss 0.894725, acc 0.751\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2500\n",
      "\n",
      "2016-08-12T19:02:51.711184: step 2501, loss 0.00887256, acc 1\n",
      "2016-08-12T19:02:51.785998: step 2502, loss 0.00521149, acc 1\n",
      "2016-08-12T19:02:51.859103: step 2503, loss 0.0199132, acc 0.984375\n",
      "2016-08-12T19:02:51.930670: step 2504, loss 0.0206359, acc 0.984375\n",
      "2016-08-12T19:02:51.981712: step 2505, loss 0.0145533, acc 1\n",
      "2016-08-12T19:02:52.056326: step 2506, loss 0.00407145, acc 1\n",
      "2016-08-12T19:02:52.130615: step 2507, loss 0.00481483, acc 1\n",
      "2016-08-12T19:02:52.202859: step 2508, loss 0.00924159, acc 1\n",
      "2016-08-12T19:02:52.275346: step 2509, loss 0.0142841, acc 1\n",
      "2016-08-12T19:02:52.349327: step 2510, loss 0.010936, acc 1\n",
      "2016-08-12T19:02:52.422377: step 2511, loss 0.0211933, acc 1\n",
      "2016-08-12T19:02:52.494419: step 2512, loss 0.0107631, acc 1\n",
      "2016-08-12T19:02:52.568746: step 2513, loss 0.0204936, acc 0.984375\n",
      "2016-08-12T19:02:52.642189: step 2514, loss 0.0114201, acc 1\n",
      "2016-08-12T19:02:52.714124: step 2515, loss 0.0109815, acc 1\n",
      "2016-08-12T19:02:52.785637: step 2516, loss 0.0186668, acc 1\n",
      "2016-08-12T19:02:52.860341: step 2517, loss 0.0289789, acc 0.984375\n",
      "2016-08-12T19:02:52.935642: step 2518, loss 0.0242887, acc 1\n",
      "2016-08-12T19:02:53.008952: step 2519, loss 0.0148357, acc 1\n",
      "2016-08-12T19:02:53.059146: step 2520, loss 0.0185451, acc 1\n",
      "2016-08-12T19:02:53.134163: step 2521, loss 0.0130052, acc 1\n",
      "2016-08-12T19:02:53.208279: step 2522, loss 0.00919974, acc 1\n",
      "2016-08-12T19:02:53.280288: step 2523, loss 0.0222223, acc 1\n",
      "2016-08-12T19:02:53.355420: step 2524, loss 0.00824232, acc 1\n",
      "2016-08-12T19:02:53.428093: step 2525, loss 0.00866303, acc 1\n",
      "2016-08-12T19:02:53.502897: step 2526, loss 0.00798978, acc 1\n",
      "2016-08-12T19:02:53.576380: step 2527, loss 0.00655636, acc 1\n",
      "2016-08-12T19:02:53.650767: step 2528, loss 0.00619652, acc 1\n",
      "2016-08-12T19:02:53.723647: step 2529, loss 0.0211239, acc 0.984375\n",
      "2016-08-12T19:02:53.797745: step 2530, loss 0.00985455, acc 1\n",
      "2016-08-12T19:02:53.870668: step 2531, loss 0.0266769, acc 0.984375\n",
      "2016-08-12T19:02:53.943689: step 2532, loss 0.0113267, acc 1\n",
      "2016-08-12T19:02:54.015866: step 2533, loss 0.0061096, acc 1\n",
      "2016-08-12T19:02:54.088952: step 2534, loss 0.0429337, acc 0.984375\n",
      "2016-08-12T19:02:54.141760: step 2535, loss 0.0137709, acc 1\n",
      "2016-08-12T19:02:54.215583: step 2536, loss 0.00370028, acc 1\n",
      "2016-08-12T19:02:54.290105: step 2537, loss 0.0185515, acc 1\n",
      "2016-08-12T19:02:54.362198: step 2538, loss 0.0269915, acc 1\n",
      "2016-08-12T19:02:54.435312: step 2539, loss 0.00586613, acc 1\n",
      "2016-08-12T19:02:54.508794: step 2540, loss 0.0168826, acc 1\n",
      "2016-08-12T19:02:54.580369: step 2541, loss 0.0102368, acc 1\n",
      "2016-08-12T19:02:54.652557: step 2542, loss 0.028482, acc 0.984375\n",
      "2016-08-12T19:02:54.726067: step 2543, loss 0.00982783, acc 1\n",
      "2016-08-12T19:02:54.800188: step 2544, loss 0.00573333, acc 1\n",
      "2016-08-12T19:02:54.874636: step 2545, loss 0.00511496, acc 1\n",
      "2016-08-12T19:02:54.947435: step 2546, loss 0.00870137, acc 1\n",
      "2016-08-12T19:02:55.020481: step 2547, loss 0.00307098, acc 1\n",
      "2016-08-12T19:02:55.094985: step 2548, loss 0.0167724, acc 1\n",
      "2016-08-12T19:02:55.168931: step 2549, loss 0.00894192, acc 1\n",
      "2016-08-12T19:02:55.218923: step 2550, loss 0.00404021, acc 1\n",
      "2016-08-12T19:02:55.291656: step 2551, loss 0.00695316, acc 1\n",
      "2016-08-12T19:02:55.365916: step 2552, loss 0.00469263, acc 1\n",
      "2016-08-12T19:02:55.439232: step 2553, loss 0.00586185, acc 1\n",
      "2016-08-12T19:02:55.511287: step 2554, loss 0.0174064, acc 1\n",
      "2016-08-12T19:02:55.586622: step 2555, loss 0.00370384, acc 1\n",
      "2016-08-12T19:02:55.660737: step 2556, loss 0.0108468, acc 1\n",
      "2016-08-12T19:02:55.733079: step 2557, loss 0.0092902, acc 1\n",
      "2016-08-12T19:02:55.805803: step 2558, loss 0.0106287, acc 1\n",
      "2016-08-12T19:02:55.878124: step 2559, loss 0.0075977, acc 1\n",
      "2016-08-12T19:02:55.951835: step 2560, loss 0.0144055, acc 1\n",
      "2016-08-12T19:02:56.025279: step 2561, loss 0.00747172, acc 1\n",
      "2016-08-12T19:02:56.099459: step 2562, loss 0.00466969, acc 1\n",
      "2016-08-12T19:02:56.170770: step 2563, loss 0.0110946, acc 1\n",
      "2016-08-12T19:02:56.245445: step 2564, loss 0.00720201, acc 1\n",
      "2016-08-12T19:02:56.295522: step 2565, loss 0.00899166, acc 1\n",
      "2016-08-12T19:02:56.370060: step 2566, loss 0.00484445, acc 1\n",
      "2016-08-12T19:02:56.442906: step 2567, loss 0.0048483, acc 1\n",
      "2016-08-12T19:02:56.515928: step 2568, loss 0.00443705, acc 1\n",
      "2016-08-12T19:02:56.590602: step 2569, loss 0.0225211, acc 0.984375\n",
      "2016-08-12T19:02:56.662072: step 2570, loss 0.0169843, acc 1\n",
      "2016-08-12T19:02:56.737183: step 2571, loss 0.00516906, acc 1\n",
      "2016-08-12T19:02:56.809045: step 2572, loss 0.00657617, acc 1\n",
      "2016-08-12T19:02:56.881235: step 2573, loss 0.0142695, acc 1\n",
      "2016-08-12T19:02:56.954418: step 2574, loss 0.0254836, acc 1\n",
      "2016-08-12T19:02:57.026710: step 2575, loss 0.0104135, acc 1\n",
      "2016-08-12T19:02:57.100986: step 2576, loss 0.0230611, acc 0.984375\n",
      "2016-08-12T19:02:57.174518: step 2577, loss 0.0104243, acc 1\n",
      "2016-08-12T19:02:57.248255: step 2578, loss 0.0122018, acc 1\n",
      "2016-08-12T19:02:57.321628: step 2579, loss 0.0156373, acc 1\n",
      "2016-08-12T19:02:57.373717: step 2580, loss 0.00925086, acc 1\n",
      "2016-08-12T19:02:57.447841: step 2581, loss 0.0166486, acc 1\n",
      "2016-08-12T19:02:57.522165: step 2582, loss 0.0260086, acc 0.984375\n",
      "2016-08-12T19:02:57.598016: step 2583, loss 0.0221063, acc 0.984375\n",
      "2016-08-12T19:02:57.674043: step 2584, loss 0.0191897, acc 1\n",
      "2016-08-12T19:02:57.747890: step 2585, loss 0.0139231, acc 1\n",
      "2016-08-12T19:02:57.822597: step 2586, loss 0.0298794, acc 0.984375\n",
      "2016-08-12T19:02:57.896803: step 2587, loss 0.0037582, acc 1\n",
      "2016-08-12T19:02:57.969067: step 2588, loss 0.00780246, acc 1\n",
      "2016-08-12T19:02:58.042590: step 2589, loss 0.0144187, acc 1\n",
      "2016-08-12T19:02:58.116646: step 2590, loss 0.0054276, acc 1\n",
      "2016-08-12T19:02:58.189582: step 2591, loss 0.00990833, acc 1\n",
      "2016-08-12T19:02:58.262418: step 2592, loss 0.0350771, acc 0.984375\n",
      "2016-08-12T19:02:58.336327: step 2593, loss 0.00412138, acc 1\n",
      "2016-08-12T19:02:58.408019: step 2594, loss 0.00160755, acc 1\n",
      "2016-08-12T19:02:58.461358: step 2595, loss 0.00314121, acc 1\n",
      "2016-08-12T19:02:58.536852: step 2596, loss 0.00276599, acc 1\n",
      "2016-08-12T19:02:58.609732: step 2597, loss 0.0367837, acc 0.984375\n",
      "2016-08-12T19:02:58.684788: step 2598, loss 0.00900338, acc 1\n",
      "2016-08-12T19:02:58.758181: step 2599, loss 0.0157444, acc 1\n",
      "2016-08-12T19:02:58.831759: step 2600, loss 0.0110633, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:02:59.138272: step 2600, loss 0.957917, acc 0.752\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2600\n",
      "\n",
      "2016-08-12T19:02:59.807347: step 2601, loss 0.0565568, acc 0.984375\n",
      "2016-08-12T19:02:59.881977: step 2602, loss 0.0169936, acc 1\n",
      "2016-08-12T19:02:59.957583: step 2603, loss 0.00880031, acc 1\n",
      "2016-08-12T19:03:00.031648: step 2604, loss 0.0126036, acc 1\n",
      "2016-08-12T19:03:00.104517: step 2605, loss 0.0125329, acc 1\n",
      "2016-08-12T19:03:00.177773: step 2606, loss 0.015021, acc 1\n",
      "2016-08-12T19:03:00.252112: step 2607, loss 0.0214017, acc 1\n",
      "2016-08-12T19:03:00.326419: step 2608, loss 0.014136, acc 1\n",
      "2016-08-12T19:03:00.398654: step 2609, loss 0.0282765, acc 0.984375\n",
      "2016-08-12T19:03:00.448516: step 2610, loss 0.00222747, acc 1\n",
      "2016-08-12T19:03:00.521126: step 2611, loss 0.00661599, acc 1\n",
      "2016-08-12T19:03:00.593549: step 2612, loss 0.00312132, acc 1\n",
      "2016-08-12T19:03:00.668308: step 2613, loss 0.051853, acc 0.984375\n",
      "2016-08-12T19:03:00.741006: step 2614, loss 0.00711216, acc 1\n",
      "2016-08-12T19:03:00.814906: step 2615, loss 0.0116077, acc 1\n",
      "2016-08-12T19:03:00.890269: step 2616, loss 0.00797097, acc 1\n",
      "2016-08-12T19:03:00.964482: step 2617, loss 0.00632849, acc 1\n",
      "2016-08-12T19:03:01.038349: step 2618, loss 0.0122617, acc 1\n",
      "2016-08-12T19:03:01.113471: step 2619, loss 0.00795995, acc 1\n",
      "2016-08-12T19:03:01.187597: step 2620, loss 0.0173774, acc 1\n",
      "2016-08-12T19:03:01.261015: step 2621, loss 0.0133461, acc 1\n",
      "2016-08-12T19:03:01.337092: step 2622, loss 0.0172764, acc 1\n",
      "2016-08-12T19:03:01.410067: step 2623, loss 0.00605399, acc 1\n",
      "2016-08-12T19:03:01.481892: step 2624, loss 0.0116482, acc 1\n",
      "2016-08-12T19:03:01.534169: step 2625, loss 0.0122387, acc 1\n",
      "2016-08-12T19:03:01.607929: step 2626, loss 0.00150809, acc 1\n",
      "2016-08-12T19:03:01.681797: step 2627, loss 0.00770195, acc 1\n",
      "2016-08-12T19:03:01.756397: step 2628, loss 0.00423234, acc 1\n",
      "2016-08-12T19:03:01.828507: step 2629, loss 0.0223873, acc 0.984375\n",
      "2016-08-12T19:03:01.902773: step 2630, loss 0.00590879, acc 1\n",
      "2016-08-12T19:03:01.977151: step 2631, loss 0.0576495, acc 0.984375\n",
      "2016-08-12T19:03:02.053825: step 2632, loss 0.0251544, acc 1\n",
      "2016-08-12T19:03:02.126506: step 2633, loss 0.0156551, acc 1\n",
      "2016-08-12T19:03:02.200195: step 2634, loss 0.0119604, acc 1\n",
      "2016-08-12T19:03:02.272826: step 2635, loss 0.0136934, acc 1\n",
      "2016-08-12T19:03:02.349208: step 2636, loss 0.0156147, acc 1\n",
      "2016-08-12T19:03:02.423050: step 2637, loss 0.00779973, acc 1\n",
      "2016-08-12T19:03:02.495968: step 2638, loss 0.0595391, acc 0.96875\n",
      "2016-08-12T19:03:02.571711: step 2639, loss 0.035762, acc 0.984375\n",
      "2016-08-12T19:03:02.622672: step 2640, loss 0.0322215, acc 1\n",
      "2016-08-12T19:03:02.703449: step 2641, loss 0.0038115, acc 1\n",
      "2016-08-12T19:03:02.782015: step 2642, loss 0.00602858, acc 1\n",
      "2016-08-12T19:03:02.856354: step 2643, loss 0.00423965, acc 1\n",
      "2016-08-12T19:03:02.929556: step 2644, loss 0.0147529, acc 1\n",
      "2016-08-12T19:03:03.003476: step 2645, loss 0.0391405, acc 0.984375\n",
      "2016-08-12T19:03:03.077423: step 2646, loss 0.0608021, acc 0.96875\n",
      "2016-08-12T19:03:03.152437: step 2647, loss 0.0164492, acc 1\n",
      "2016-08-12T19:03:03.224773: step 2648, loss 0.00589192, acc 1\n",
      "2016-08-12T19:03:03.298714: step 2649, loss 0.0124588, acc 1\n",
      "2016-08-12T19:03:03.374410: step 2650, loss 0.00270883, acc 1\n",
      "2016-08-12T19:03:03.448668: step 2651, loss 0.0270559, acc 1\n",
      "2016-08-12T19:03:03.523289: step 2652, loss 0.00754219, acc 1\n",
      "2016-08-12T19:03:03.597603: step 2653, loss 0.032634, acc 0.984375\n",
      "2016-08-12T19:03:03.672257: step 2654, loss 0.0691434, acc 0.984375\n",
      "2016-08-12T19:03:03.726020: step 2655, loss 0.00611929, acc 1\n",
      "2016-08-12T19:03:03.801692: step 2656, loss 0.0127478, acc 1\n",
      "2016-08-12T19:03:03.875900: step 2657, loss 0.0101044, acc 1\n",
      "2016-08-12T19:03:03.949489: step 2658, loss 0.00824144, acc 1\n",
      "2016-08-12T19:03:04.024071: step 2659, loss 0.0170721, acc 0.984375\n",
      "2016-08-12T19:03:04.099105: step 2660, loss 0.0121271, acc 1\n",
      "2016-08-12T19:03:04.172282: step 2661, loss 0.00997162, acc 1\n",
      "2016-08-12T19:03:04.246563: step 2662, loss 0.0906202, acc 0.96875\n",
      "2016-08-12T19:03:04.320367: step 2663, loss 0.00294699, acc 1\n",
      "2016-08-12T19:03:04.395935: step 2664, loss 0.0236682, acc 0.984375\n",
      "2016-08-12T19:03:04.469713: step 2665, loss 0.0108659, acc 1\n",
      "2016-08-12T19:03:04.543311: step 2666, loss 0.0195921, acc 1\n",
      "2016-08-12T19:03:04.616412: step 2667, loss 0.0217304, acc 0.984375\n",
      "2016-08-12T19:03:04.689517: step 2668, loss 0.0243322, acc 0.984375\n",
      "2016-08-12T19:03:04.765301: step 2669, loss 0.0112755, acc 1\n",
      "2016-08-12T19:03:04.818864: step 2670, loss 0.0562034, acc 0.971429\n",
      "2016-08-12T19:03:04.893199: step 2671, loss 0.0110793, acc 1\n",
      "2016-08-12T19:03:04.967120: step 2672, loss 0.0262501, acc 0.984375\n",
      "2016-08-12T19:03:05.041485: step 2673, loss 0.0225391, acc 0.984375\n",
      "2016-08-12T19:03:05.116356: step 2674, loss 0.00668065, acc 1\n",
      "2016-08-12T19:03:05.188377: step 2675, loss 0.0130189, acc 1\n",
      "2016-08-12T19:03:05.260973: step 2676, loss 0.00279406, acc 1\n",
      "2016-08-12T19:03:05.334322: step 2677, loss 0.0168981, acc 1\n",
      "2016-08-12T19:03:05.409040: step 2678, loss 0.0143523, acc 1\n",
      "2016-08-12T19:03:05.483050: step 2679, loss 0.0110166, acc 1\n",
      "2016-08-12T19:03:05.555778: step 2680, loss 0.00301856, acc 1\n",
      "2016-08-12T19:03:05.627020: step 2681, loss 0.00588828, acc 1\n",
      "2016-08-12T19:03:05.700689: step 2682, loss 0.0156973, acc 1\n",
      "2016-08-12T19:03:05.776390: step 2683, loss 0.00390867, acc 1\n",
      "2016-08-12T19:03:05.848997: step 2684, loss 0.068169, acc 0.953125\n",
      "2016-08-12T19:03:05.902831: step 2685, loss 0.0100083, acc 1\n",
      "2016-08-12T19:03:05.976506: step 2686, loss 0.0126744, acc 1\n",
      "2016-08-12T19:03:06.050407: step 2687, loss 0.045091, acc 0.984375\n",
      "2016-08-12T19:03:06.123529: step 2688, loss 0.00749874, acc 1\n",
      "2016-08-12T19:03:06.196300: step 2689, loss 0.0328525, acc 0.984375\n",
      "2016-08-12T19:03:06.271111: step 2690, loss 0.0153368, acc 1\n",
      "2016-08-12T19:03:06.344726: step 2691, loss 0.0149033, acc 1\n",
      "2016-08-12T19:03:06.419425: step 2692, loss 0.00567568, acc 1\n",
      "2016-08-12T19:03:06.492504: step 2693, loss 0.0185251, acc 1\n",
      "2016-08-12T19:03:06.565761: step 2694, loss 0.00625364, acc 1\n",
      "2016-08-12T19:03:06.638427: step 2695, loss 0.00439276, acc 1\n",
      "2016-08-12T19:03:06.713521: step 2696, loss 0.0141023, acc 1\n",
      "2016-08-12T19:03:06.789068: step 2697, loss 0.00411842, acc 1\n",
      "2016-08-12T19:03:06.860286: step 2698, loss 0.00756539, acc 1\n",
      "2016-08-12T19:03:06.934071: step 2699, loss 0.145289, acc 0.984375\n",
      "2016-08-12T19:03:06.982097: step 2700, loss 0.00381543, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:03:07.288513: step 2700, loss 0.911273, acc 0.76\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2700\n",
      "\n",
      "2016-08-12T19:03:08.019343: step 2701, loss 0.0286809, acc 0.984375\n",
      "2016-08-12T19:03:08.093644: step 2702, loss 0.077473, acc 0.96875\n",
      "2016-08-12T19:03:08.167048: step 2703, loss 0.0147749, acc 1\n",
      "2016-08-12T19:03:08.239239: step 2704, loss 0.0489754, acc 0.96875\n",
      "2016-08-12T19:03:08.314552: step 2705, loss 0.0337605, acc 0.984375\n",
      "2016-08-12T19:03:08.390372: step 2706, loss 0.0733071, acc 0.96875\n",
      "2016-08-12T19:03:08.465147: step 2707, loss 0.0153559, acc 1\n",
      "2016-08-12T19:03:08.540571: step 2708, loss 0.00664874, acc 1\n",
      "2016-08-12T19:03:08.614774: step 2709, loss 0.00413803, acc 1\n",
      "2016-08-12T19:03:08.688959: step 2710, loss 0.0122524, acc 1\n",
      "2016-08-12T19:03:08.764455: step 2711, loss 0.00902097, acc 1\n",
      "2016-08-12T19:03:08.840009: step 2712, loss 0.0323871, acc 0.984375\n",
      "2016-08-12T19:03:08.914559: step 2713, loss 0.0124668, acc 1\n",
      "2016-08-12T19:03:08.987219: step 2714, loss 0.0193885, acc 1\n",
      "2016-08-12T19:03:09.038531: step 2715, loss 0.054409, acc 0.971429\n",
      "2016-08-12T19:03:09.112196: step 2716, loss 0.021537, acc 1\n",
      "2016-08-12T19:03:09.186621: step 2717, loss 0.0181086, acc 0.984375\n",
      "2016-08-12T19:03:09.260192: step 2718, loss 0.0135694, acc 1\n",
      "2016-08-12T19:03:09.333381: step 2719, loss 0.00695177, acc 1\n",
      "2016-08-12T19:03:09.407785: step 2720, loss 0.0196484, acc 1\n",
      "2016-08-12T19:03:09.481160: step 2721, loss 0.0158715, acc 1\n",
      "2016-08-12T19:03:09.554090: step 2722, loss 0.0273774, acc 0.984375\n",
      "2016-08-12T19:03:09.628003: step 2723, loss 0.011781, acc 1\n",
      "2016-08-12T19:03:09.703512: step 2724, loss 0.0173158, acc 1\n",
      "2016-08-12T19:03:09.778730: step 2725, loss 0.0156329, acc 1\n",
      "2016-08-12T19:03:09.856431: step 2726, loss 0.108925, acc 0.96875\n",
      "2016-08-12T19:03:09.932880: step 2727, loss 0.0400647, acc 0.96875\n",
      "2016-08-12T19:03:10.008699: step 2728, loss 0.00534679, acc 1\n",
      "2016-08-12T19:03:10.082745: step 2729, loss 0.0183871, acc 1\n",
      "2016-08-12T19:03:10.135631: step 2730, loss 0.0913359, acc 0.914286\n",
      "2016-08-12T19:03:10.209424: step 2731, loss 0.0283465, acc 1\n",
      "2016-08-12T19:03:10.283413: step 2732, loss 0.0289957, acc 0.984375\n",
      "2016-08-12T19:03:10.354527: step 2733, loss 0.0142212, acc 1\n",
      "2016-08-12T19:03:10.429539: step 2734, loss 0.00991089, acc 1\n",
      "2016-08-12T19:03:10.506159: step 2735, loss 0.00872129, acc 1\n",
      "2016-08-12T19:03:10.579107: step 2736, loss 0.00917427, acc 1\n",
      "2016-08-12T19:03:10.654911: step 2737, loss 0.00617151, acc 1\n",
      "2016-08-12T19:03:10.729501: step 2738, loss 0.0457329, acc 0.96875\n",
      "2016-08-12T19:03:10.804311: step 2739, loss 0.0234853, acc 1\n",
      "2016-08-12T19:03:10.877609: step 2740, loss 0.0505539, acc 0.984375\n",
      "2016-08-12T19:03:10.951790: step 2741, loss 0.0120729, acc 1\n",
      "2016-08-12T19:03:11.025819: step 2742, loss 0.0467733, acc 0.984375\n",
      "2016-08-12T19:03:11.099501: step 2743, loss 0.00661224, acc 1\n",
      "2016-08-12T19:03:11.172795: step 2744, loss 0.0100014, acc 1\n",
      "2016-08-12T19:03:11.222664: step 2745, loss 0.00879555, acc 1\n",
      "2016-08-12T19:03:11.299293: step 2746, loss 0.0138297, acc 1\n",
      "2016-08-12T19:03:11.374041: step 2747, loss 0.0208664, acc 1\n",
      "2016-08-12T19:03:11.448832: step 2748, loss 0.0921672, acc 0.984375\n",
      "2016-08-12T19:03:11.521209: step 2749, loss 0.0339298, acc 1\n",
      "2016-08-12T19:03:11.592567: step 2750, loss 0.00438387, acc 1\n",
      "2016-08-12T19:03:11.666735: step 2751, loss 0.0391764, acc 0.984375\n",
      "2016-08-12T19:03:11.741673: step 2752, loss 0.00542087, acc 1\n",
      "2016-08-12T19:03:11.815573: step 2753, loss 0.00874975, acc 1\n",
      "2016-08-12T19:03:11.888986: step 2754, loss 0.011394, acc 1\n",
      "2016-08-12T19:03:11.962571: step 2755, loss 0.0331175, acc 0.984375\n",
      "2016-08-12T19:03:12.038925: step 2756, loss 0.0424844, acc 0.984375\n",
      "2016-08-12T19:03:12.113402: step 2757, loss 0.00380546, acc 1\n",
      "2016-08-12T19:03:12.187009: step 2758, loss 0.0132461, acc 1\n",
      "2016-08-12T19:03:12.261480: step 2759, loss 0.00843616, acc 1\n",
      "2016-08-12T19:03:12.312066: step 2760, loss 0.0116398, acc 1\n",
      "2016-08-12T19:03:12.387776: step 2761, loss 0.019593, acc 1\n",
      "2016-08-12T19:03:12.460838: step 2762, loss 0.00953593, acc 1\n",
      "2016-08-12T19:03:12.534556: step 2763, loss 0.0089547, acc 1\n",
      "2016-08-12T19:03:12.610154: step 2764, loss 0.031171, acc 0.984375\n",
      "2016-08-12T19:03:12.683696: step 2765, loss 0.0499663, acc 0.96875\n",
      "2016-08-12T19:03:12.759324: step 2766, loss 0.00907371, acc 1\n",
      "2016-08-12T19:03:12.832443: step 2767, loss 0.0323447, acc 0.984375\n",
      "2016-08-12T19:03:12.906227: step 2768, loss 0.0436999, acc 0.984375\n",
      "2016-08-12T19:03:12.981779: step 2769, loss 0.0314735, acc 0.984375\n",
      "2016-08-12T19:03:13.056253: step 2770, loss 0.033703, acc 0.984375\n",
      "2016-08-12T19:03:13.131344: step 2771, loss 0.0305589, acc 0.984375\n",
      "2016-08-12T19:03:13.207316: step 2772, loss 0.0300566, acc 1\n",
      "2016-08-12T19:03:13.281494: step 2773, loss 0.00742959, acc 1\n",
      "2016-08-12T19:03:13.357545: step 2774, loss 0.0146345, acc 1\n",
      "2016-08-12T19:03:13.407400: step 2775, loss 0.0116032, acc 1\n",
      "2016-08-12T19:03:13.482441: step 2776, loss 0.0204453, acc 1\n",
      "2016-08-12T19:03:13.556482: step 2777, loss 0.0395076, acc 0.984375\n",
      "2016-08-12T19:03:13.629977: step 2778, loss 0.0881111, acc 0.984375\n",
      "2016-08-12T19:03:13.702937: step 2779, loss 0.00480988, acc 1\n",
      "2016-08-12T19:03:13.777407: step 2780, loss 0.00529432, acc 1\n",
      "2016-08-12T19:03:13.848751: step 2781, loss 0.0258489, acc 1\n",
      "2016-08-12T19:03:13.922537: step 2782, loss 0.00486999, acc 1\n",
      "2016-08-12T19:03:13.996197: step 2783, loss 0.0182197, acc 1\n",
      "2016-08-12T19:03:14.068857: step 2784, loss 0.0477329, acc 0.96875\n",
      "2016-08-12T19:03:14.143504: step 2785, loss 0.00774655, acc 1\n",
      "2016-08-12T19:03:14.216707: step 2786, loss 0.0184559, acc 1\n",
      "2016-08-12T19:03:14.291188: step 2787, loss 0.0209773, acc 0.984375\n",
      "2016-08-12T19:03:14.365352: step 2788, loss 0.0116282, acc 1\n",
      "2016-08-12T19:03:14.438727: step 2789, loss 0.0420426, acc 0.96875\n",
      "2016-08-12T19:03:14.490178: step 2790, loss 0.0128798, acc 1\n",
      "2016-08-12T19:03:14.565554: step 2791, loss 0.0117661, acc 1\n",
      "2016-08-12T19:03:14.638686: step 2792, loss 0.0164123, acc 1\n",
      "2016-08-12T19:03:14.711958: step 2793, loss 0.00599913, acc 1\n",
      "2016-08-12T19:03:14.785733: step 2794, loss 0.00925079, acc 1\n",
      "2016-08-12T19:03:14.860295: step 2795, loss 0.00674509, acc 1\n",
      "2016-08-12T19:03:14.935276: step 2796, loss 0.0238976, acc 1\n",
      "2016-08-12T19:03:15.011576: step 2797, loss 0.01986, acc 1\n",
      "2016-08-12T19:03:15.087623: step 2798, loss 0.00224275, acc 1\n",
      "2016-08-12T19:03:15.161736: step 2799, loss 0.0686799, acc 0.984375\n",
      "2016-08-12T19:03:15.236931: step 2800, loss 0.0123333, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:03:15.547393: step 2800, loss 0.953912, acc 0.745\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2800\n",
      "\n",
      "2016-08-12T19:03:16.284752: step 2801, loss 0.0101684, acc 1\n",
      "2016-08-12T19:03:16.362339: step 2802, loss 0.0095538, acc 1\n",
      "2016-08-12T19:03:16.435701: step 2803, loss 0.0141046, acc 1\n",
      "2016-08-12T19:03:16.510655: step 2804, loss 0.0593455, acc 0.984375\n",
      "2016-08-12T19:03:16.564521: step 2805, loss 0.0178244, acc 1\n",
      "2016-08-12T19:03:16.639049: step 2806, loss 0.0187346, acc 0.984375\n",
      "2016-08-12T19:03:16.713573: step 2807, loss 0.0430096, acc 0.984375\n",
      "2016-08-12T19:03:16.788295: step 2808, loss 0.0148286, acc 1\n",
      "2016-08-12T19:03:16.867493: step 2809, loss 0.0091699, acc 1\n",
      "2016-08-12T19:03:16.941537: step 2810, loss 0.0225581, acc 1\n",
      "2016-08-12T19:03:17.015803: step 2811, loss 0.00289585, acc 1\n",
      "2016-08-12T19:03:17.088305: step 2812, loss 0.0107755, acc 1\n",
      "2016-08-12T19:03:17.161296: step 2813, loss 0.0196593, acc 1\n",
      "2016-08-12T19:03:17.235500: step 2814, loss 0.0140989, acc 1\n",
      "2016-08-12T19:03:17.309093: step 2815, loss 0.0128059, acc 1\n",
      "2016-08-12T19:03:17.383902: step 2816, loss 0.0379999, acc 0.984375\n",
      "2016-08-12T19:03:17.459252: step 2817, loss 0.0170918, acc 1\n",
      "2016-08-12T19:03:17.535312: step 2818, loss 0.0218999, acc 1\n",
      "2016-08-12T19:03:17.609827: step 2819, loss 0.027097, acc 0.984375\n",
      "2016-08-12T19:03:17.662860: step 2820, loss 0.00500102, acc 1\n",
      "2016-08-12T19:03:17.737850: step 2821, loss 0.00989379, acc 1\n",
      "2016-08-12T19:03:17.811842: step 2822, loss 0.0101132, acc 1\n",
      "2016-08-12T19:03:17.885094: step 2823, loss 0.0109191, acc 1\n",
      "2016-08-12T19:03:17.959772: step 2824, loss 0.0541985, acc 0.984375\n",
      "2016-08-12T19:03:18.034784: step 2825, loss 0.017508, acc 0.984375\n",
      "2016-08-12T19:03:18.107630: step 2826, loss 0.0181658, acc 1\n",
      "2016-08-12T19:03:18.180667: step 2827, loss 0.00658251, acc 1\n",
      "2016-08-12T19:03:18.252787: step 2828, loss 0.00725052, acc 1\n",
      "2016-08-12T19:03:18.326840: step 2829, loss 0.0264381, acc 0.984375\n",
      "2016-08-12T19:03:18.401390: step 2830, loss 0.0102707, acc 1\n",
      "2016-08-12T19:03:18.476767: step 2831, loss 0.0226518, acc 1\n",
      "2016-08-12T19:03:18.549373: step 2832, loss 0.0504004, acc 0.96875\n",
      "2016-08-12T19:03:18.622198: step 2833, loss 0.0143089, acc 1\n",
      "2016-08-12T19:03:18.695940: step 2834, loss 0.00602134, acc 1\n",
      "2016-08-12T19:03:18.749745: step 2835, loss 0.0274988, acc 1\n",
      "2016-08-12T19:03:18.825412: step 2836, loss 0.0591111, acc 0.984375\n",
      "2016-08-12T19:03:18.899250: step 2837, loss 0.0193542, acc 1\n",
      "2016-08-12T19:03:18.973418: step 2838, loss 0.0370059, acc 0.984375\n",
      "2016-08-12T19:03:19.048826: step 2839, loss 0.0338939, acc 0.96875\n",
      "2016-08-12T19:03:19.125572: step 2840, loss 0.0150279, acc 1\n",
      "2016-08-12T19:03:19.198910: step 2841, loss 0.0137455, acc 1\n",
      "2016-08-12T19:03:19.275413: step 2842, loss 0.00106759, acc 1\n",
      "2016-08-12T19:03:19.349099: step 2843, loss 0.0169264, acc 1\n",
      "2016-08-12T19:03:19.425850: step 2844, loss 0.0259888, acc 0.984375\n",
      "2016-08-12T19:03:19.500081: step 2845, loss 0.00573209, acc 1\n",
      "2016-08-12T19:03:19.574934: step 2846, loss 0.0556334, acc 0.96875\n",
      "2016-08-12T19:03:19.648878: step 2847, loss 0.00834887, acc 1\n",
      "2016-08-12T19:03:19.723976: step 2848, loss 0.0112469, acc 1\n",
      "2016-08-12T19:03:19.796445: step 2849, loss 0.0131462, acc 1\n",
      "2016-08-12T19:03:19.848750: step 2850, loss 0.00245504, acc 1\n",
      "2016-08-12T19:03:19.922829: step 2851, loss 0.00345847, acc 1\n",
      "2016-08-12T19:03:19.995374: step 2852, loss 0.00979815, acc 1\n",
      "2016-08-12T19:03:20.070615: step 2853, loss 0.00440197, acc 1\n",
      "2016-08-12T19:03:20.143942: step 2854, loss 0.00667641, acc 1\n",
      "2016-08-12T19:03:20.217386: step 2855, loss 0.00955908, acc 1\n",
      "2016-08-12T19:03:20.292549: step 2856, loss 0.00278002, acc 1\n",
      "2016-08-12T19:03:20.366376: step 2857, loss 0.0200457, acc 1\n",
      "2016-08-12T19:03:20.439685: step 2858, loss 0.0320981, acc 0.984375\n",
      "2016-08-12T19:03:20.511565: step 2859, loss 0.0322723, acc 0.984375\n",
      "2016-08-12T19:03:20.586517: step 2860, loss 0.0165324, acc 1\n",
      "2016-08-12T19:03:20.661106: step 2861, loss 0.00848841, acc 1\n",
      "2016-08-12T19:03:20.738336: step 2862, loss 0.0134341, acc 1\n",
      "2016-08-12T19:03:20.810901: step 2863, loss 0.0360827, acc 0.984375\n",
      "2016-08-12T19:03:20.884487: step 2864, loss 0.0390205, acc 0.984375\n",
      "2016-08-12T19:03:20.937736: step 2865, loss 0.0426886, acc 0.971429\n",
      "2016-08-12T19:03:21.013394: step 2866, loss 0.0180077, acc 1\n",
      "2016-08-12T19:03:21.085649: step 2867, loss 0.0676232, acc 0.984375\n",
      "2016-08-12T19:03:21.159567: step 2868, loss 0.00282856, acc 1\n",
      "2016-08-12T19:03:21.232580: step 2869, loss 0.037034, acc 1\n",
      "2016-08-12T19:03:21.308006: step 2870, loss 0.0206759, acc 1\n",
      "2016-08-12T19:03:21.382550: step 2871, loss 0.0158784, acc 1\n",
      "2016-08-12T19:03:21.456972: step 2872, loss 0.00500684, acc 1\n",
      "2016-08-12T19:03:21.530331: step 2873, loss 0.0116209, acc 1\n",
      "2016-08-12T19:03:21.604968: step 2874, loss 0.00713011, acc 1\n",
      "2016-08-12T19:03:21.683317: step 2875, loss 0.0393066, acc 0.984375\n",
      "2016-08-12T19:03:21.761079: step 2876, loss 0.0166495, acc 0.984375\n",
      "2016-08-12T19:03:21.837233: step 2877, loss 0.00182437, acc 1\n",
      "2016-08-12T19:03:21.921435: step 2878, loss 0.030298, acc 0.984375\n",
      "2016-08-12T19:03:21.996147: step 2879, loss 0.00911455, acc 1\n",
      "2016-08-12T19:03:22.047044: step 2880, loss 0.0124811, acc 1\n",
      "2016-08-12T19:03:22.123424: step 2881, loss 0.0388097, acc 0.984375\n",
      "2016-08-12T19:03:22.197974: step 2882, loss 0.00846352, acc 1\n",
      "2016-08-12T19:03:22.270889: step 2883, loss 0.0184072, acc 1\n",
      "2016-08-12T19:03:22.347393: step 2884, loss 0.00675053, acc 1\n",
      "2016-08-12T19:03:22.420457: step 2885, loss 0.0147114, acc 1\n",
      "2016-08-12T19:03:22.493317: step 2886, loss 0.0880372, acc 0.984375\n",
      "2016-08-12T19:03:22.568391: step 2887, loss 0.00768301, acc 1\n",
      "2016-08-12T19:03:22.645636: step 2888, loss 0.087193, acc 0.984375\n",
      "2016-08-12T19:03:22.719446: step 2889, loss 0.0142557, acc 1\n",
      "2016-08-12T19:03:22.793364: step 2890, loss 0.0165054, acc 1\n",
      "2016-08-12T19:03:22.866300: step 2891, loss 0.00533972, acc 1\n",
      "2016-08-12T19:03:22.940058: step 2892, loss 0.0153274, acc 1\n",
      "2016-08-12T19:03:23.013738: step 2893, loss 0.021889, acc 1\n",
      "2016-08-12T19:03:23.087165: step 2894, loss 0.0139395, acc 1\n",
      "2016-08-12T19:03:23.138335: step 2895, loss 0.0139259, acc 1\n",
      "2016-08-12T19:03:23.212747: step 2896, loss 0.00373983, acc 1\n",
      "2016-08-12T19:03:23.287385: step 2897, loss 0.0431248, acc 0.984375\n",
      "2016-08-12T19:03:23.362343: step 2898, loss 0.00729405, acc 1\n",
      "2016-08-12T19:03:23.435682: step 2899, loss 0.0196206, acc 1\n",
      "2016-08-12T19:03:23.509758: step 2900, loss 0.0482987, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:03:23.820383: step 2900, loss 0.930301, acc 0.762\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-2900\n",
      "\n",
      "2016-08-12T19:03:24.479805: step 2901, loss 0.00465657, acc 1\n",
      "2016-08-12T19:03:24.553868: step 2902, loss 0.0127549, acc 1\n",
      "2016-08-12T19:03:24.630696: step 2903, loss 0.00496258, acc 1\n",
      "2016-08-12T19:03:24.705380: step 2904, loss 0.00852917, acc 1\n",
      "2016-08-12T19:03:24.783540: step 2905, loss 0.0104255, acc 1\n",
      "2016-08-12T19:03:24.857196: step 2906, loss 0.0106291, acc 1\n",
      "2016-08-12T19:03:24.935207: step 2907, loss 0.00666295, acc 1\n",
      "2016-08-12T19:03:25.009227: step 2908, loss 0.00241386, acc 1\n",
      "2016-08-12T19:03:25.083348: step 2909, loss 0.0215281, acc 1\n",
      "2016-08-12T19:03:25.135144: step 2910, loss 0.0426722, acc 0.971429\n",
      "2016-08-12T19:03:25.209494: step 2911, loss 0.00990301, acc 1\n",
      "2016-08-12T19:03:25.282797: step 2912, loss 0.00294694, acc 1\n",
      "2016-08-12T19:03:25.355564: step 2913, loss 0.0219816, acc 1\n",
      "2016-08-12T19:03:25.430996: step 2914, loss 0.00342724, acc 1\n",
      "2016-08-12T19:03:25.503681: step 2915, loss 0.00991317, acc 1\n",
      "2016-08-12T19:03:25.577392: step 2916, loss 0.0140731, acc 1\n",
      "2016-08-12T19:03:25.652407: step 2917, loss 0.00928618, acc 1\n",
      "2016-08-12T19:03:25.725992: step 2918, loss 0.0190557, acc 1\n",
      "2016-08-12T19:03:25.801123: step 2919, loss 0.0157892, acc 1\n",
      "2016-08-12T19:03:25.875320: step 2920, loss 0.0225503, acc 1\n",
      "2016-08-12T19:03:25.948920: step 2921, loss 0.011625, acc 1\n",
      "2016-08-12T19:03:26.023988: step 2922, loss 0.0186421, acc 1\n",
      "2016-08-12T19:03:26.098889: step 2923, loss 0.021044, acc 1\n",
      "2016-08-12T19:03:26.172728: step 2924, loss 0.0204397, acc 1\n",
      "2016-08-12T19:03:26.223971: step 2925, loss 0.0191928, acc 1\n",
      "2016-08-12T19:03:26.299474: step 2926, loss 0.107284, acc 0.984375\n",
      "2016-08-12T19:03:26.374285: step 2927, loss 0.0108244, acc 1\n",
      "2016-08-12T19:03:26.447038: step 2928, loss 0.00366419, acc 1\n",
      "2016-08-12T19:03:26.521499: step 2929, loss 0.00704161, acc 1\n",
      "2016-08-12T19:03:26.595156: step 2930, loss 0.0171484, acc 1\n",
      "2016-08-12T19:03:26.668575: step 2931, loss 0.00753378, acc 1\n",
      "2016-08-12T19:03:26.742200: step 2932, loss 0.00576748, acc 1\n",
      "2016-08-12T19:03:26.816861: step 2933, loss 0.0125724, acc 1\n",
      "2016-08-12T19:03:26.889729: step 2934, loss 0.0134985, acc 1\n",
      "2016-08-12T19:03:26.967643: step 2935, loss 0.0332682, acc 0.984375\n",
      "2016-08-12T19:03:27.040252: step 2936, loss 0.0228741, acc 1\n",
      "2016-08-12T19:03:27.113512: step 2937, loss 0.0218569, acc 1\n",
      "2016-08-12T19:03:27.187625: step 2938, loss 0.0175148, acc 1\n",
      "2016-08-12T19:03:27.260922: step 2939, loss 0.0328035, acc 0.96875\n",
      "2016-08-12T19:03:27.312507: step 2940, loss 0.0035426, acc 1\n",
      "2016-08-12T19:03:27.389748: step 2941, loss 0.0488972, acc 0.984375\n",
      "2016-08-12T19:03:27.464884: step 2942, loss 0.00537901, acc 1\n",
      "2016-08-12T19:03:27.538522: step 2943, loss 0.012837, acc 1\n",
      "2016-08-12T19:03:27.613079: step 2944, loss 0.00765711, acc 1\n",
      "2016-08-12T19:03:27.685941: step 2945, loss 0.0066476, acc 1\n",
      "2016-08-12T19:03:27.758878: step 2946, loss 0.0162051, acc 1\n",
      "2016-08-12T19:03:27.832306: step 2947, loss 0.0294387, acc 0.984375\n",
      "2016-08-12T19:03:27.905494: step 2948, loss 0.0397712, acc 0.96875\n",
      "2016-08-12T19:03:27.979364: step 2949, loss 0.0152682, acc 1\n",
      "2016-08-12T19:03:28.052481: step 2950, loss 0.0299576, acc 0.984375\n",
      "2016-08-12T19:03:28.126251: step 2951, loss 0.0101516, acc 1\n",
      "2016-08-12T19:03:28.199145: step 2952, loss 0.00729931, acc 1\n",
      "2016-08-12T19:03:28.274763: step 2953, loss 0.013571, acc 1\n",
      "2016-08-12T19:03:28.349200: step 2954, loss 0.00238946, acc 1\n",
      "2016-08-12T19:03:28.395234: step 2955, loss 0.00763455, acc 1\n",
      "2016-08-12T19:03:28.470353: step 2956, loss 0.0200127, acc 1\n",
      "2016-08-12T19:03:28.544221: step 2957, loss 0.00853363, acc 1\n",
      "2016-08-12T19:03:28.616748: step 2958, loss 0.00537243, acc 1\n",
      "2016-08-12T19:03:28.690797: step 2959, loss 0.0153931, acc 1\n",
      "2016-08-12T19:03:28.764732: step 2960, loss 0.0263709, acc 0.984375\n",
      "2016-08-12T19:03:28.838109: step 2961, loss 0.0162439, acc 1\n",
      "2016-08-12T19:03:28.911151: step 2962, loss 0.0443252, acc 0.984375\n",
      "2016-08-12T19:03:28.986205: step 2963, loss 0.00516963, acc 1\n",
      "2016-08-12T19:03:29.060305: step 2964, loss 0.00782145, acc 1\n",
      "2016-08-12T19:03:29.135974: step 2965, loss 0.0171962, acc 1\n",
      "2016-08-12T19:03:29.211399: step 2966, loss 0.0197154, acc 1\n",
      "2016-08-12T19:03:29.284936: step 2967, loss 0.0133606, acc 1\n",
      "2016-08-12T19:03:29.359933: step 2968, loss 0.0158661, acc 1\n",
      "2016-08-12T19:03:29.434398: step 2969, loss 0.00472567, acc 1\n",
      "2016-08-12T19:03:29.485218: step 2970, loss 0.0105873, acc 1\n",
      "2016-08-12T19:03:29.560450: step 2971, loss 0.0267864, acc 0.984375\n",
      "2016-08-12T19:03:29.634202: step 2972, loss 0.0253823, acc 1\n",
      "2016-08-12T19:03:29.709042: step 2973, loss 0.00872525, acc 1\n",
      "2016-08-12T19:03:29.782077: step 2974, loss 0.010314, acc 1\n",
      "2016-08-12T19:03:29.857603: step 2975, loss 0.0271638, acc 1\n",
      "2016-08-12T19:03:29.931354: step 2976, loss 0.00990863, acc 1\n",
      "2016-08-12T19:03:30.005624: step 2977, loss 0.0101962, acc 1\n",
      "2016-08-12T19:03:30.080236: step 2978, loss 0.00327593, acc 1\n",
      "2016-08-12T19:03:30.152347: step 2979, loss 0.0158494, acc 1\n",
      "2016-08-12T19:03:30.225903: step 2980, loss 0.113874, acc 0.96875\n",
      "2016-08-12T19:03:30.299848: step 2981, loss 0.0186302, acc 1\n",
      "2016-08-12T19:03:30.373657: step 2982, loss 0.0222744, acc 0.984375\n",
      "2016-08-12T19:03:30.449163: step 2983, loss 0.0235316, acc 0.984375\n",
      "2016-08-12T19:03:30.524742: step 2984, loss 0.00687348, acc 1\n",
      "2016-08-12T19:03:30.571305: step 2985, loss 0.0193632, acc 1\n",
      "2016-08-12T19:03:30.645383: step 2986, loss 0.0263748, acc 0.984375\n",
      "2016-08-12T19:03:30.719317: step 2987, loss 0.0345726, acc 0.96875\n",
      "2016-08-12T19:03:30.793385: step 2988, loss 0.0316396, acc 1\n",
      "2016-08-12T19:03:30.867091: step 2989, loss 0.0104783, acc 1\n",
      "2016-08-12T19:03:30.942215: step 2990, loss 0.00933706, acc 1\n",
      "2016-08-12T19:03:31.015094: step 2991, loss 0.00523856, acc 1\n",
      "2016-08-12T19:03:31.088305: step 2992, loss 0.0164158, acc 1\n",
      "2016-08-12T19:03:31.161816: step 2993, loss 0.0118411, acc 1\n",
      "2016-08-12T19:03:31.236434: step 2994, loss 0.00502186, acc 1\n",
      "2016-08-12T19:03:31.313012: step 2995, loss 0.00644108, acc 1\n",
      "2016-08-12T19:03:31.386939: step 2996, loss 0.0315195, acc 0.984375\n",
      "2016-08-12T19:03:31.462126: step 2997, loss 0.017346, acc 1\n",
      "2016-08-12T19:03:31.536581: step 2998, loss 0.00801804, acc 1\n",
      "2016-08-12T19:03:31.610974: step 2999, loss 0.0474209, acc 0.96875\n",
      "2016-08-12T19:03:31.662955: step 3000, loss 0.00200633, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2016-08-12T19:03:31.974142: step 3000, loss 0.93865, acc 0.763\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/work/ML-Testing/cnn-text-classification-tf/runs/1470995912/checkpoints/model-3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append(\"/usr/local/lib/python2.7/dist-packages\")\n",
    "!python ./train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n",
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_DIR=./runs/1470995912/checkpoints/\n",
      "EVAL_TRAIN=True\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n",
      "name: GRID K520\n",
      "major: 3 minor: 0 memoryClockRate (GHz) 0.797\n",
      "pciBusID 0000:00:03.0\n",
      "Total memory: 4.00GiB\n",
      "Free memory: 3.95GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\n",
      "Total number of test examples: 1931\n",
      "Accuracy: 0.877266\n"
     ]
    }
   ],
   "source": [
    "!python ./eval.py --eval_train --checkpoint_dir=\"./runs/1470995912/checkpoints/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
